{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Seamus Murray's collection of disparate notes about Linux, Programming, System Administration, and System Engineering. This is a collection of notes that I may need to reference again in the future. The content here is neither written, validated, organized, nor presented in a manner suitable for anyone other than myself to easily use or navigate.","title":"Home"},{"location":"#seamus-murrays-collection-of-disparate-notes-about-linux-programming-system-administration-and-system-engineering","text":"This is a collection of notes that I may need to reference again in the future. The content here is neither written, validated, organized, nor presented in a manner suitable for anyone other than myself to easily use or navigate.","title":"Seamus Murray's collection of disparate notes about Linux, Programming, System Administration, and System Engineering."},{"location":"linux/hot_add_memory_and_cpus/","text":"How to Hot-add Memory in Linux RHEL 5 ? Check if the acpi modules are loaded lsmod | grep acpi You should see both ... acpiphp 43673 0 acpi_memhotplug 42199 0 If not loaded, load the modules... modprobe acpiphp modprobe acpi_memhotplug Display the total number of modules available in the system. cat /sys/devices/system/memory/memory*/state | wc Increase your server memory with either physical Hot-add memory modules or if its a virtual machine simply increase the amount a memory allocated to the VM guest. Display the number of modules that are offline grep offline /sys/devices/system/memory/memory*/state | wc Execute either of the following to set the modules to be online for offline_module in `grep offline /sys/devices/system/memory/memory*/state | sed s/\\:offline//g` do echo \"online\" > $offline_module done Validate that none of the modules are still in an offline state grep offline /sys/devices/system/memory/memory*/state Display the number of online modules and ensure that it is greater than the previous value cat /sys/devices/system/memory/memory*/state | wc For Ubuntu you have to try with another script as below, #!/bin/sh i=0 while [ $i -lt 4500 ] do if [ -f /sys/devices/system/memory/memory$i/state ] then if grep \"offline\" /sys/devices/system/memory/memory$i/state then echo 'online' > /sys/devices/system/memory/memory$i/state fi fi i=`expr $i + 1` done To validate that the memory has been successfully added run either.. cat /proc/meminfo free Hot add a CPU echo 1 > /sys/devices/system/cpu/cpu1/online","title":"Hot add memory and cpus"},{"location":"linux/hot_add_memory_and_cpus/#how-to-hot-add-memory-in-linux-rhel-5","text":"Check if the acpi modules are loaded lsmod | grep acpi You should see both ... acpiphp 43673 0 acpi_memhotplug 42199 0 If not loaded, load the modules... modprobe acpiphp modprobe acpi_memhotplug Display the total number of modules available in the system. cat /sys/devices/system/memory/memory*/state | wc Increase your server memory with either physical Hot-add memory modules or if its a virtual machine simply increase the amount a memory allocated to the VM guest. Display the number of modules that are offline grep offline /sys/devices/system/memory/memory*/state | wc Execute either of the following to set the modules to be online for offline_module in `grep offline /sys/devices/system/memory/memory*/state | sed s/\\:offline//g` do echo \"online\" > $offline_module done Validate that none of the modules are still in an offline state grep offline /sys/devices/system/memory/memory*/state Display the number of online modules and ensure that it is greater than the previous value cat /sys/devices/system/memory/memory*/state | wc For Ubuntu you have to try with another script as below, #!/bin/sh i=0 while [ $i -lt 4500 ] do if [ -f /sys/devices/system/memory/memory$i/state ] then if grep \"offline\" /sys/devices/system/memory/memory$i/state then echo 'online' > /sys/devices/system/memory/memory$i/state fi fi i=`expr $i + 1` done To validate that the memory has been successfully added run either.. cat /proc/meminfo free","title":"How to Hot-add Memory in Linux RHEL 5 ?"},{"location":"linux/hot_add_memory_and_cpus/#hot-add-a-cpu","text":"echo 1 > /sys/devices/system/cpu/cpu1/online","title":"Hot add a CPU"},{"location":"linux/linux_proccess_signaling/","text":"Adventures in the land of Linux signalling If you have a process that is consuming too much CPU or I/O you can pause it by sending it a STOP signal, If it has been called by a shell that you have access too you can simply press CTRL^Z, the job will be paused and put in the back ground. execute fg to start the job But if the process is not a shell command or is owned by another shell you can still pause the job by sending it a SIGSTOP signal using the kill command kill -19 <pid> kill -s SIGSTOP <pid> This will put the process into the background. To un-pause the process send it a SIGCONT (continue) signal kill -18 <pid> kill -s SIGCONT <pid> If a process forks a child but does not call wait() if the child dies first the process will still stick around in the process table until someone picks up the exit code, if no one picks up the exit code init will do it seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26464 4400 0.0 -su 3219 pts/11 S+ 0:08 0 2 43253 356 0.0 ./fork.exe 3220 pts/11 S+ 0:00 0 2 82317 104 0.0 ./fcrk.exe 3221 pts/23 R+ 0:00 0 84 22555 1076 0.0 ps -U seamus -v seamus@ubuntu144:~$ kill 3220 seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26464 4400 0.0 -su 3219 pts/11 S+ 0:08 0 2 43253 356 0.0 ./fork.exe 3220 pts/11 Z+ 0:00 0 0 0 0 0.0 [fork.exe] <defunct> 3222 pts/23 R+ 0:00 0 84 22555 1072 0.0 ps -U seamus -v If I add a wait call after the fork.. #include printf(\"child exited with status of = %d\\n\",wait(&exitstatus)); And send the child a kill signal then all it good, no zombie and the parent receives the signal seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26548 4620 0.0 -su 3334 pts/11 S+ 0:08 0 2 43253 352 0.0 ./fork.exe 3340 pts/11 S+ 0:00 0 2 43253 100 0.0 ./fork.exe 3343 pts/23 R+ 0:00 0 84 22555 1076 0.0 ps -U seamus -v seamus@ubuntu144:~$ kill 3340 seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26548 4620 0.0 -su 3334 pts/11 S+ 0:08 0 2 43253 352 0.0 ./fork.exe 3344 pts/23 R+ 0:00 0 84 22555 1072 0.0 ps -U seamus -v seamus@ubuntu144:~$ ./fork.exe I am the parent process I am the parent of the child proccess ID 3340 child exited with status of = 3340 DD and signals Sending signals to the DD command to force it to display a progress update. while killall -USR1 dd; do sleep 1; done guess@desktop:~$ dd if=/dev/urandom of=file bs=512 253427+0 records in 253426+0 records out 129754112 bytes (130 MB) copied, 11.1867 s, 11.6 MB/s 276324+0 records in 276323+0 records out 141477376 bytes (141 MB) copied, 12.1911 s, 11.6 MB/s 299038+0 records in 299037+0 records out 153106944 bytes (153 MB) copied, 13.1956 s, 11.6 MB/s 321950+0 records in 321949+0 records out 164837888 bytes (165 MB) copied, 14.2001 s, 11.6 MB/s","title":"Linux proccess signaling"},{"location":"linux/linux_proccess_signaling/#adventures-in-the-land-of-linux-signalling","text":"If you have a process that is consuming too much CPU or I/O you can pause it by sending it a STOP signal, If it has been called by a shell that you have access too you can simply press CTRL^Z, the job will be paused and put in the back ground. execute fg to start the job But if the process is not a shell command or is owned by another shell you can still pause the job by sending it a SIGSTOP signal using the kill command kill -19 <pid> kill -s SIGSTOP <pid> This will put the process into the background. To un-pause the process send it a SIGCONT (continue) signal kill -18 <pid> kill -s SIGCONT <pid> If a process forks a child but does not call wait() if the child dies first the process will still stick around in the process table until someone picks up the exit code, if no one picks up the exit code init will do it seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26464 4400 0.0 -su 3219 pts/11 S+ 0:08 0 2 43253 356 0.0 ./fork.exe 3220 pts/11 S+ 0:00 0 2 82317 104 0.0 ./fcrk.exe 3221 pts/23 R+ 0:00 0 84 22555 1076 0.0 ps -U seamus -v seamus@ubuntu144:~$ kill 3220 seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26464 4400 0.0 -su 3219 pts/11 S+ 0:08 0 2 43253 356 0.0 ./fork.exe 3220 pts/11 Z+ 0:00 0 0 0 0 0.0 [fork.exe] <defunct> 3222 pts/23 R+ 0:00 0 84 22555 1072 0.0 ps -U seamus -v If I add a wait call after the fork.. #include printf(\"child exited with status of = %d\\n\",wait(&exitstatus)); And send the child a kill signal then all it good, no zombie and the parent receives the signal seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26548 4620 0.0 -su 3334 pts/11 S+ 0:08 0 2 43253 352 0.0 ./fork.exe 3340 pts/11 S+ 0:00 0 2 43253 100 0.0 ./fork.exe 3343 pts/23 R+ 0:00 0 84 22555 1076 0.0 ps -U seamus -v seamus@ubuntu144:~$ kill 3340 seamus@ubuntu144:~$ ps -U seamus -v PID TTY STAT TIME MAJFL TRS DRS RSS %MEM COMMAND 2772 pts/23 S 0:00 0 955 26460 4380 0.0 -su 3177 pts/11 S 0:00 0 955 26548 4620 0.0 -su 3334 pts/11 S+ 0:08 0 2 43253 352 0.0 ./fork.exe 3344 pts/23 R+ 0:00 0 84 22555 1072 0.0 ps -U seamus -v seamus@ubuntu144:~$ ./fork.exe I am the parent process I am the parent of the child proccess ID 3340 child exited with status of = 3340","title":"Adventures in the land of Linux signalling"},{"location":"linux/linux_proccess_signaling/#dd-and-signals","text":"Sending signals to the DD command to force it to display a progress update. while killall -USR1 dd; do sleep 1; done guess@desktop:~$ dd if=/dev/urandom of=file bs=512 253427+0 records in 253426+0 records out 129754112 bytes (130 MB) copied, 11.1867 s, 11.6 MB/s 276324+0 records in 276323+0 records out 141477376 bytes (141 MB) copied, 12.1911 s, 11.6 MB/s 299038+0 records in 299037+0 records out 153106944 bytes (153 MB) copied, 13.1956 s, 11.6 MB/s 321950+0 records in 321949+0 records out 164837888 bytes (165 MB) copied, 14.2001 s, 11.6 MB/s","title":"DD and signals"},{"location":"linux/Clustering/HA_LVM_automatic_failover/","text":"below is a test of the automatic failover of a 2 node RHEL 5.7 cluster There is 1 service called ServiceName There is a single shared VG/LV called /dev/shared_vg1/shared_lv1 <?xml version=\"1.0\"?> <cluster alias=\"round-and-round\" config_version=\"40\" name=\"round-and-round\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.domain.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.domain.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"224.0.0.001\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/>= </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.domain.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.domain.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> <lvm lv_name=\"shared_lv1\" name=\"shared_lv1_name\" self_fence=\"1\" vg_name=\"shared_vg1\"/> <fs device=\"/dev/shared_vg1/shared_lv1\" force_fsck=\"0\" force_unmount=\"1\" fsid=\"6742\" fstype=\"ext3\" mountpoint=\"/mnt/shared\" name=\"shared-fs\" self_fence=\"1\"/> <ip address=\"192.168.1.3\" monitor_link=\"1\"/> <script file=\"/usr/local/bin/pc.sh\" name=\"start.sh\"/> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"ServiceName\"> <lvm ref=\"shared_lv1_name\"> <fs ref=\"shared-fs\"/> <ip ref=\"192.168.1.3\"> <script ref=\"start.sh\"/> </ip> </lvm> </service> </rm> </cluster> the kernel crash will be triggered by executing echo c > /proc/sysrq-trigger status of cluster before triggering kernel panic on primary node hostname1 [root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:31:00 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Online Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname1.domain.private started status of cluster after triggering kernel panic on primary node hostname1 [root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:31:40 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Offline Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname1.domain.private started status of cluster during fencing and service migration [root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:32:02 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Offline Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname2.domain.private starting status of cluster after fencing and service migration [root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:39:13 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Offline Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname2.domain.private started July 1 16:31:16 hostname2 root: seamus simulating kernel panic echo c on primary node July 1 16:31:31 hostname2 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state. July 1 16:31:31 hostname2 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes). July 1 16:31:31 hostname2 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes). July 1 16:31:31 hostname2 openais[5968]: [TOTEM] entering GATHER state from 2. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering GATHER state from 0. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Creating commit token because I am the rep. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Storing new sequence id for ring 1530 July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering COMMIT state. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering RECOVERY state. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] position [0] member 10.10.90.1: July 1 16:31:33 hostname2 openais[5968]: [TOTEM] previous ring seq 5420 rep 10.10.90.1 July 1 16:31:33 hostname2 openais[5968]: [TOTEM] aru 5c high delivered 5c received flag 1 July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Did not need to originate any messages in recovery. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Sending initial ORF token July 1 16:31:33 hostname2 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 16:31:33 hostname2 openais[5968]: [CLM ] New Configuration: July 1 16:31:33 hostname2 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 16:31:33 hostname2 kernel: dlm: closing connection to node 2 July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Left: July 1 16:31:33 hostname2 fenced[5990]: hostname1.domain.private not a cluster member after 0 sec post_fail_delay July 1 16:31:33 hostname2 openais[5968]: [CLM ] r(0) ip(10.10.90.2) July 1 16:31:33 hostname2 fenced[5990]: fencing node \"hostname1.domain.private\" July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Joined: July 1 16:31:33 hostname2 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 16:31:33 hostname2 openais[5968]: [CLM ] New Configuration: July 1 16:31:33 hostname2 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Left: July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Joined: July 1 16:31:33 hostname2 openais[5968]: [SYNC ] This node is within the primary component and will provide service. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering OPERATIONAL state. July 1 16:31:33 hostname2 openais[5968]: [CLM ] got nodejoin message 10.10.90.1 July 1 16:31:33 hostname2 openais[5968]: [CPG ] got joinlist message from node 1 July 1 16:31:50 hostname2 fenced[5990]: fence \"hostname1.domain.private\" success July 1 16:31:50 hostname2 clurgmgrd[6229]: <notice> Taking over service service:ServiceName from down member hostname1.domain.private July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Owner of shared_vg1/shared_lv1 is not in the cluster July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Stealing shared_vg1/shared_lv1 July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Activating shared_vg1/shared_lv1 July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Making resilient : lvchange -ay shared_vg1/shared_lv1 July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Resilient command: lvchange -ay shared_vg1/shared_lv1 {wrapped} --config devices{filter=[\"a|/dev/mpath/mpshared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]} July 1 16:31:51 hostname2 multipathd: dm-12: devmap not registered, can't remove July 1 16:31:51 hostname2 multipathd: dm-12: add map (uevent) July 1 16:31:52 hostname2 kernel: kjournald starting. Commit interval 5 seconds July 1 16:31:52 hostname2 kernel: EXT3-fs warning: maximal mount count reached, running e2fsck is recommended July 1 16:31:52 hostname2 kernel: EXT3 FS on dm-12, internal journal July 1 16:31:52 hostname2 kernel: EXT3-fs: dm-12: 1 orphan inode deleted July 1 16:31:52 hostname2 kernel: EXT3-fs: recovery complete. July 1 16:31:52 hostname2 kernel: EXT3-fs: mounted filesystem with ordered data mode. July 1 16:31:54 hostname2 avahi-daemon[5564]: Registering new address record for 192.168.1.3 on eth0. July 1 16:33:30 hostname2 clurgmgrd[6229]: <notice> Service service:ServiceName started","title":"HA LVM automatic failover"},{"location":"linux/Clustering/HA_LVM_automatic_failover/#below-is-a-test-of-the-automatic-failover-of-a-2-node-rhel-57-cluster","text":"There is 1 service called ServiceName There is a single shared VG/LV called /dev/shared_vg1/shared_lv1 <?xml version=\"1.0\"?> <cluster alias=\"round-and-round\" config_version=\"40\" name=\"round-and-round\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.domain.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.domain.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"224.0.0.001\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/>= </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.domain.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.domain.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> <lvm lv_name=\"shared_lv1\" name=\"shared_lv1_name\" self_fence=\"1\" vg_name=\"shared_vg1\"/> <fs device=\"/dev/shared_vg1/shared_lv1\" force_fsck=\"0\" force_unmount=\"1\" fsid=\"6742\" fstype=\"ext3\" mountpoint=\"/mnt/shared\" name=\"shared-fs\" self_fence=\"1\"/> <ip address=\"192.168.1.3\" monitor_link=\"1\"/> <script file=\"/usr/local/bin/pc.sh\" name=\"start.sh\"/> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"ServiceName\"> <lvm ref=\"shared_lv1_name\"> <fs ref=\"shared-fs\"/> <ip ref=\"192.168.1.3\"> <script ref=\"start.sh\"/> </ip> </lvm> </service> </rm> </cluster>","title":"below is a test of the automatic failover of a 2 node RHEL 5.7 cluster"},{"location":"linux/Clustering/HA_LVM_automatic_failover/#the-kernel-crash-will-be-triggered-by-executing","text":"echo c > /proc/sysrq-trigger","title":"the kernel crash will be triggered by executing"},{"location":"linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-before-triggering-kernel-panic-on-primary-node-hostname1","text":"[root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:31:00 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Online Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname1.domain.private started","title":"status of cluster before triggering kernel panic on primary node hostname1"},{"location":"linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-after-triggering-kernel-panic-on-primary-node-hostname1","text":"[root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:31:40 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Offline Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname1.domain.private started","title":"status of cluster after triggering kernel panic on primary node hostname1"},{"location":"linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-during-fencing-and-service-migration","text":"[root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:32:02 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Offline Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname2.domain.private starting","title":"status of cluster during fencing and service migration"},{"location":"linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-after-fencing-and-service-migration","text":"[root@hostname2 ~]# clustat Cluster Status for round-and-round @ Fri July 1 16:39:13 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ hostname2.domain.private 1 Online, Local, rgmanager hostname1.domain.private 2 Offline Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName hostname2.domain.private started July 1 16:31:16 hostname2 root: seamus simulating kernel panic echo c on primary node July 1 16:31:31 hostname2 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state. July 1 16:31:31 hostname2 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes). July 1 16:31:31 hostname2 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes). July 1 16:31:31 hostname2 openais[5968]: [TOTEM] entering GATHER state from 2. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering GATHER state from 0. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Creating commit token because I am the rep. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Storing new sequence id for ring 1530 July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering COMMIT state. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering RECOVERY state. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] position [0] member 10.10.90.1: July 1 16:31:33 hostname2 openais[5968]: [TOTEM] previous ring seq 5420 rep 10.10.90.1 July 1 16:31:33 hostname2 openais[5968]: [TOTEM] aru 5c high delivered 5c received flag 1 July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Did not need to originate any messages in recovery. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] Sending initial ORF token July 1 16:31:33 hostname2 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 16:31:33 hostname2 openais[5968]: [CLM ] New Configuration: July 1 16:31:33 hostname2 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 16:31:33 hostname2 kernel: dlm: closing connection to node 2 July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Left: July 1 16:31:33 hostname2 fenced[5990]: hostname1.domain.private not a cluster member after 0 sec post_fail_delay July 1 16:31:33 hostname2 openais[5968]: [CLM ] r(0) ip(10.10.90.2) July 1 16:31:33 hostname2 fenced[5990]: fencing node \"hostname1.domain.private\" July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Joined: July 1 16:31:33 hostname2 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 16:31:33 hostname2 openais[5968]: [CLM ] New Configuration: July 1 16:31:33 hostname2 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Left: July 1 16:31:33 hostname2 openais[5968]: [CLM ] Members Joined: July 1 16:31:33 hostname2 openais[5968]: [SYNC ] This node is within the primary component and will provide service. July 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering OPERATIONAL state. July 1 16:31:33 hostname2 openais[5968]: [CLM ] got nodejoin message 10.10.90.1 July 1 16:31:33 hostname2 openais[5968]: [CPG ] got joinlist message from node 1 July 1 16:31:50 hostname2 fenced[5990]: fence \"hostname1.domain.private\" success July 1 16:31:50 hostname2 clurgmgrd[6229]: <notice> Taking over service service:ServiceName from down member hostname1.domain.private July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Owner of shared_vg1/shared_lv1 is not in the cluster July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Stealing shared_vg1/shared_lv1 July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Activating shared_vg1/shared_lv1 July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Making resilient : lvchange -ay shared_vg1/shared_lv1 July 1 16:31:51 hostname2 clurgmgrd: [6229]: <notice> Resilient command: lvchange -ay shared_vg1/shared_lv1 {wrapped} --config devices{filter=[\"a|/dev/mpath/mpshared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]} July 1 16:31:51 hostname2 multipathd: dm-12: devmap not registered, can't remove July 1 16:31:51 hostname2 multipathd: dm-12: add map (uevent) July 1 16:31:52 hostname2 kernel: kjournald starting. Commit interval 5 seconds July 1 16:31:52 hostname2 kernel: EXT3-fs warning: maximal mount count reached, running e2fsck is recommended July 1 16:31:52 hostname2 kernel: EXT3 FS on dm-12, internal journal July 1 16:31:52 hostname2 kernel: EXT3-fs: dm-12: 1 orphan inode deleted July 1 16:31:52 hostname2 kernel: EXT3-fs: recovery complete. July 1 16:31:52 hostname2 kernel: EXT3-fs: mounted filesystem with ordered data mode. July 1 16:31:54 hostname2 avahi-daemon[5564]: Registering new address record for 192.168.1.3 on eth0. July 1 16:33:30 hostname2 clurgmgrd[6229]: <notice> Service service:ServiceName started","title":"status of cluster after fencing and service migration"},{"location":"linux/Clustering/HA_LVM_automatic_failover_manual_fence_override/","text":"RHEL cluster testing a fencing override using the fence_ack_manual command triggering a kernel panic , trigger a crash [root@hostname2 ~]# echo c > /proc/sysrq-trigger tailing the log on the second node July 1 22:57 hostname1 ccsd[5959]: Update of cluster.conf complete (version 40 -> 41). July 1 22:10 hostname1 clurgmgrd[6229]: <notice> Reconfiguring July 1 22:15 hostname1 clurgmgrd: [6229]: <notice> Getting status July 1 22:01 hostname1 root: seamus simulating fencedevice\\site failure July 1 22:48 hostname1 root: seamus triggred kenel panic on hostname2 July 1 22:40 hostname1 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state. July 1 22:40 hostname1 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes). July 1 22:40 hostname1 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes). July 1 22:40 hostname1 openais[5968]: [TOTEM] entering GATHER state from 2. July 1 22:42 hostname1 openais[5968]: [TOTEM] entering GATHER state from 0. July 1 22:42 hostname1 openais[5968]: [TOTEM] Creating commit token because I am the rep. July 1 22:42 hostname1 openais[5968]: [TOTEM] Storing new sequence id for ring 1538 July 1 22:42 hostname1 openais[5968]: [TOTEM] entering COMMIT state. July 1 22:42 hostname1 openais[5968]: [TOTEM] entering RECOVERY state. July 1 22:42 hostname1 openais[5968]: [TOTEM] position [0] member 10.10.90.1: July 1 22:42 hostname1 openais[5968]: [TOTEM] previous ring seq 5428 rep 10.10.90.1 July 1 22:42 hostname1 openais[5968]: [TOTEM] aru 2f high delivered 2f received flag 1 July 1 22:42 hostname1 openais[5968]: [TOTEM] Did not need to originate any messages in recovery. July 1 22:42 hostname1 openais[5968]: [TOTEM] Sending initial ORF token July 1 22:42 hostname1 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 22:42 hostname1 openais[5968]: [CLM ] New Configuration: July 1 22:42 hostname1 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 22:42 hostname1 openais[5968]: [CLM ] Members Left: July 1 22:42 hostname1 openais[5968]: [CLM ] r(0) ip(10.10.90.2) July 1 22:42 hostname1 kernel: dlm: closing connection to node 2 July 1 22:42 hostname1 openais[5968]: [CLM ] Members Joined: July 1 22:42 hostname1 fenced[5990]: hostname2.domain.private not a cluster member after 0 sec post_fail_delay July 1 22:42 hostname1 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 22:42 hostname1 fenced[5990]: fencing node \"hostname2.domain.private\" July 1 22:42 hostname1 openais[5968]: [CLM ] New Configuration: July 1 22:42 hostname1 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 22:42 hostname1 openais[5968]: [CLM ] Members Left: July 1 22:42 hostname1 openais[5968]: [CLM ] Members Joined: July 1 22:42 hostname1 openais[5968]: [SYNC ] This node is within the primary component and will provide service. July 1 22:42 hostname1 openais[5968]: [TOTEM] entering OPERATIONAL state. July 1 22:42 hostname1 openais[5968]: [CLM ] got nodejoin message 10.10.90.1 July 1 22:42 hostname1 openais[5968]: [CPG ] got joinlist message from node 1 July 1 22:50 hostname1 fenced[5990]: agent \"fence_ipmilan\" reports: Rebooting machine @ IPMI:192.168.214.33...Failed July 1 22:50 hostname1 fenced[5990]: fence \"hostname2.domain.private\" failed July 1 22:18 hostname1 root: seamus fencing failing due to incorrect ipmi password July 1 22:48 hostname1 fenced[5990]: fencing node \"hostname2.domain.private\" July 1 22:56 hostname1 fenced[5990]: agent \"fence_ipmilan\" reports: Rebooting machine @ IPMI:192.168.214.33...Failed July 1 22:56 hostname1 fenced[5990]: fence \"hostname2.domain.private\" failed running the fence_ack_manual command [root@hostname1 ~]# fence_ack_manual -e -n hostname2.domain.private Warning: If the node \"hostname2.domain.private\" has not been manually fenced (i.e. power cycled or disconnected from shared storage devices) the GFS file system may become corrupted and all its data unrecoverable! Please verify that the node shown above has been reset or disconnected from storage. Are you certain you want to continue? [yN] y July 1 22:06 hostname1 root: seamus running fence_ack_manual -e -n hostname2.domain.private July 1 22:23 hostname1 fenced[5990]: fence \"hostname2.domain.private\" overridden by administrator intervention","title":"HA LVM automatic failover manual fence override"},{"location":"linux/Clustering/HA_LVM_automatic_failover_manual_fence_override/#rhel-cluster-testing-a-fencing-override-using-the-fence_ack_manual-command","text":"","title":"RHEL cluster testing a fencing override using the fence_ack_manual command"},{"location":"linux/Clustering/HA_LVM_automatic_failover_manual_fence_override/#triggering-a-kernel-panic-trigger-a-crash","text":"[root@hostname2 ~]# echo c > /proc/sysrq-trigger","title":"triggering a kernel panic , trigger a crash"},{"location":"linux/Clustering/HA_LVM_automatic_failover_manual_fence_override/#tailing-the-log-on-the-second-node","text":"July 1 22:57 hostname1 ccsd[5959]: Update of cluster.conf complete (version 40 -> 41). July 1 22:10 hostname1 clurgmgrd[6229]: <notice> Reconfiguring July 1 22:15 hostname1 clurgmgrd: [6229]: <notice> Getting status July 1 22:01 hostname1 root: seamus simulating fencedevice\\site failure July 1 22:48 hostname1 root: seamus triggred kenel panic on hostname2 July 1 22:40 hostname1 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state. July 1 22:40 hostname1 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes). July 1 22:40 hostname1 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes). July 1 22:40 hostname1 openais[5968]: [TOTEM] entering GATHER state from 2. July 1 22:42 hostname1 openais[5968]: [TOTEM] entering GATHER state from 0. July 1 22:42 hostname1 openais[5968]: [TOTEM] Creating commit token because I am the rep. July 1 22:42 hostname1 openais[5968]: [TOTEM] Storing new sequence id for ring 1538 July 1 22:42 hostname1 openais[5968]: [TOTEM] entering COMMIT state. July 1 22:42 hostname1 openais[5968]: [TOTEM] entering RECOVERY state. July 1 22:42 hostname1 openais[5968]: [TOTEM] position [0] member 10.10.90.1: July 1 22:42 hostname1 openais[5968]: [TOTEM] previous ring seq 5428 rep 10.10.90.1 July 1 22:42 hostname1 openais[5968]: [TOTEM] aru 2f high delivered 2f received flag 1 July 1 22:42 hostname1 openais[5968]: [TOTEM] Did not need to originate any messages in recovery. July 1 22:42 hostname1 openais[5968]: [TOTEM] Sending initial ORF token July 1 22:42 hostname1 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 22:42 hostname1 openais[5968]: [CLM ] New Configuration: July 1 22:42 hostname1 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 22:42 hostname1 openais[5968]: [CLM ] Members Left: July 1 22:42 hostname1 openais[5968]: [CLM ] r(0) ip(10.10.90.2) July 1 22:42 hostname1 kernel: dlm: closing connection to node 2 July 1 22:42 hostname1 openais[5968]: [CLM ] Members Joined: July 1 22:42 hostname1 fenced[5990]: hostname2.domain.private not a cluster member after 0 sec post_fail_delay July 1 22:42 hostname1 openais[5968]: [CLM ] CLM CONFIGURATION CHANGE July 1 22:42 hostname1 fenced[5990]: fencing node \"hostname2.domain.private\" July 1 22:42 hostname1 openais[5968]: [CLM ] New Configuration: July 1 22:42 hostname1 openais[5968]: [CLM ] r(0) ip(10.10.90.1) July 1 22:42 hostname1 openais[5968]: [CLM ] Members Left: July 1 22:42 hostname1 openais[5968]: [CLM ] Members Joined: July 1 22:42 hostname1 openais[5968]: [SYNC ] This node is within the primary component and will provide service. July 1 22:42 hostname1 openais[5968]: [TOTEM] entering OPERATIONAL state. July 1 22:42 hostname1 openais[5968]: [CLM ] got nodejoin message 10.10.90.1 July 1 22:42 hostname1 openais[5968]: [CPG ] got joinlist message from node 1 July 1 22:50 hostname1 fenced[5990]: agent \"fence_ipmilan\" reports: Rebooting machine @ IPMI:192.168.214.33...Failed July 1 22:50 hostname1 fenced[5990]: fence \"hostname2.domain.private\" failed July 1 22:18 hostname1 root: seamus fencing failing due to incorrect ipmi password July 1 22:48 hostname1 fenced[5990]: fencing node \"hostname2.domain.private\" July 1 22:56 hostname1 fenced[5990]: agent \"fence_ipmilan\" reports: Rebooting machine @ IPMI:192.168.214.33...Failed July 1 22:56 hostname1 fenced[5990]: fence \"hostname2.domain.private\" failed","title":"tailing the log on the second node"},{"location":"linux/Clustering/HA_LVM_automatic_failover_manual_fence_override/#running-the-fence_ack_manual-command","text":"[root@hostname1 ~]# fence_ack_manual -e -n hostname2.domain.private Warning: If the node \"hostname2.domain.private\" has not been manually fenced (i.e. power cycled or disconnected from shared storage devices) the GFS file system may become corrupted and all its data unrecoverable! Please verify that the node shown above has been reset or disconnected from storage. Are you certain you want to continue? [yN] y July 1 22:06 hostname1 root: seamus running fence_ack_manual -e -n hostname2.domain.private July 1 22:23 hostname1 fenced[5990]: fence \"hostname2.domain.private\" overridden by administrator intervention","title":"running the fence_ack_manual command"},{"location":"linux/Clustering/HA_LVM_cluster_build/","text":"RHEL 5.7 HA-LVM cluster setup guide version 201600508 created by Seamus Murray After manually building the server via multiple RDP sessions and KVM consoles you sometimes have typos or duplicate key strokes in config files. Before going further, you should ensure that all the hostname's IP address gateways etc are correct.. ensure the host name is correct and a FQDN in /etc/sysconfig/network ensure hostname is not in any of the ifcfg-ethx files ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default restart network services, run hostname command manually or reboot if necessary Ensure NTP is working correctly, You may need to manually sync the time service ntp stop ntpdate 192.168.100.1 ntpdate 192.168.100.2 service ntp start Important , Due to the application vendor support requirements, the cluster servers need to remain on RHEL 5.7. ie, they cannot be updated to RHEL 5.8 or 5.9 To ensure the hosts stay on RHEL 5.7 there has been a dedicated software channel created for this specific project. The hosts have to be registered against this specific channel.... if they subscribe to the main RHEL 5.x channel they will be updated to the latest package versions which will make then unsupportable from the application vendors perspective. rhel-x86_64-server-5_7 Clone Red Hat Network Tools for RHEL Server (v.5.7 64-bit x86_64) rhel-x86_64-server-cluster-5_7 Download and install the certificates for the signed RPM packages wget --no-check-certificate https://satellite1.local/pub/rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm wget --no-check-certificate https://satellite1.local/pub/rpm-gpg-key-1.0-7.noarch.rpm rpm -Uvh rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm rpm-gpg-key-corp-1.0-7.noarch.rpm sed -i 's/^\\(sslCACert=\\).*/\\1\\/usr\\/share\\/rhn\\/RHN-ORG-TRUSTED-SSL-CERT/' /etc/sysconfig/rhn/up2date Ensure the that the RedHat certificate is installed # not installed by default rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release if not you may get errors like... Public key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed Register the server with the RedHat satellite server using one of the following keys rhnreg_ks --activationkey 1-bc5fb717c87abcdefghijklm --serverUrl https://satellite1.local/XMLRPC rhnreg_ks --activationkey 1-5bbf25c4abcdefghijklm123 --serverUrl https://satellite1.local/XMLRPC check if the repo access is working yum clean all yum list Install the addons... yum install java-1.6.0-openjdk yum install samba3x-winbind iptables lsof krb5-workstation pam_passwdqc xauth yum install ipv6-disable Fixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5 cause RHEL 5 multipath bindings file is located in var /var isn't mounted before multipath is configured redhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot solution (work around) Edit the /etc/multipath.conf add the following lines to multipath config to set netapp specific settings defaults { user_friendly_names yes bindings_file /etc/multipath/bindings ##changed flush_on_last_del yes max_fds max pg_prio_calc avg queue_without_daemon no } devices { device { vendor \"NETAPP\" product \"LUN\" path_checker tur path_selector \"round-robin 0\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" # prio_callout \"/sbin/mpath_prio_ontap /dev/%n\" prio_callout \"/sbin/mpath_prio_alua /dev/%n\" # features \"1 queue_if_no_path\" features \"3 queue_if_no_path pg_init_retries 50\" # hardware_handler \"0\" hardware_handler \"1 alua\" path_grouping_policy group_by_prio failback immediate rr_weight uniform rr_min_io 128 } } mkdir /etc/multipath cp /var/lib/multipath/bindings /etc/multipath/bindings cd /boot cp initrd*img initrd*img.multipath-bak mkinitrd -f initrd-`uname -r`.img `uname -r` ls -ltr #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size edit the multipath bindings and ensure the shared luns have the same friendly names across all nodes recreate initial ram disk image reboot all nodes and ensure all nodes see the luns at same friendly name cat /etc/multipath/bindings # Multipath bindings, Version : 1.0 # NOTE: this file is automatically maintained by the multipath program. # You should not need to edit this file in normal circumstances. # # Format: # alias wwid # mpsys 360a9800037542d73442443123456755 mpath1 360a9800037542d73442443123456757 mpath2 360a9800037542d73442443123456759 mpath3 360a9800037542d7344244312345672f mpath4 360a9800037542d73442443123456762 mpath5 360a9800037542d73442443123456764 mpath6 360a9800037542d73442443123456766 Final multipath.conf file should look somthing like defaults { user_friendly_names yes bindings_file /etc/multipath/bindings flush_on_last_del yes max_fds max pg_prio_calc avg queue_without_daemon no } blacklist { devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\" devnode \"^(hd|xvd|vd)[a-z]*\" # wwid \"*\" } # Make sure our multipath devices are enabled. blacklist_exceptions { wwid \"360a9800037542d73442443123456755\" } multipaths { multipath { wwid 360a9800037542d73442443123456755 alias mpsys } } devices { device { vendor \"NETAPP\" product \"LUN\" path_checker tur path_selector \"round-robin 0\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" # prio_callout \"/sbin/mpath_prio_ontap /dev/%n\" prio_callout \"/sbin/mpath_prio_alua /dev/%n\" # features \"1 queue_if_no_path\" features \"3 queue_if_no_path pg_init_retries 50\" # hardware_handler \"0\" hardware_handler \"1 alua\" path_grouping_policy group_by_prio failback immediate rr_weight uniform rr_min_io 128 } } Change fstab #/dev/mapper/mpath0p1 /boot ext3 defaults 1 2 /dev/mapper/mpsysp1 /boot ext3 defaults 1 2 Cluster preconfiguration... Ensure NTP is working correctly, you may need to manually sync the time service ntp stop ntpdate 192.168.0.1 ntpdate 192.168.0.2 service ntp start Install cluster software and fencing tools yum install OpenIPMI-tools.x86_64 cman rgmanager lucci ricci Disable the cluster from starting up... until you have finished the config chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off Edit firewall rules to allow cluster traffic iptables -I INPUT --protocol tcp --dport 22 -j ACCEPT # Cluster iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT turn off firewall for private nic iptables -I INPUT -i eth1 -j ACCEPT save the iptables so that they will be persistant Configure the private/heartbeat nic *add multicast route for private/heartbeat /etc/sysconfig/network-scripts/route-eth1 239.0.0.0/4 dev eth1 Define which multicast address to use for the cluster 239.192.0.1 for App 1 239.192.0.2 for App 2 239.192.0.3 for App 3 Add the private hostnames/domains to /etc/hosts # cluster nodes 192.168.0.1 hostname1.local hostname1 10.10.10.1 hostname1.private hostname1-priv 192.168.0.2 hostname2.local hostname2 10.10.10.2 hostname2.private hostname2-priv 192.168.0.3 hostname-vip.local hostname-vip if you want to use luci warning due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs you have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config You can use ricci/luci to import an existing cluster Therefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci Starting ricci/luci if you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password start up ricci on both servers and luci on one server setup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci start browser window https://127.0.0.1:80 Simple cluster.conf file to get you started <?xml version=\"1.0\"?> <cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"239.192.0.1\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/>= </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"> </service> </rm> </cluster> scp cluster.conf hostname1:/etc/cluster/cluster.conf ccs_tool update cluster.conf Configure fencing Setup ipmi profile on the hardware Test to see if fencing works echo -e \"ipaddr=192.168.1.1 \\nlogin= \\npasswd= \\naction=status\" | fence_ipmilan Add the fencing details to the cluster.conf file service rgmanager start service cman start clustat cman_tool status tail -f /var/log/messages manual fence override fence_ack_manual -e -n hostname1.private manaul service relocation clusvcadm -r ServiceName ifconfig will not display a VIP you have to run ip address show you may want to disable the acpi daemon otherwise your server may not switch off fast enough chkconfig --level 234 5 acpid off chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off setup HA-LVM ...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide edit /etc/lvm/lvm.conf Ensure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'. Edit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file: volume_list = [ \"vgsys\", \"@hostname1.private\" ] Create the PV VG LV and filesystems from one of the nodes pvcreate /dev/mapper/mpath1 pvcreate /dev/mapper/mpath2 pvcreate /dev/mapper/mpath3 pvcreate /dev/mapper/mpath4 pvcreate /dev/mapper/mpath5 pvcreate /dev/mapper/mpath6 vgcreate vg_shared_pc /dev/mapper/mpath1 vgcreate vg_shared_db /dev/mapper/mpath2 vgcreate vg_shared_arch /dev/mapper/mpath3 vgcreate vg_shared_logs /dev/mapper/mpath4 vgcreate vg_shared_data /dev/mapper/mpath5 vgcreate vg_shared_backup /dev/mapper/mpath6 lvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc lvcreate -l 100%FREE -n lv_shared_db vg_shared_db lvcreate -l 100%FREE -n lv_shared_arch vg_shared_arch lvcreate -l 100%FREE -n lv_shared_logs vg_shared_logs lvcreate -l 100%FREE -n lv_shared_data vg_shared_data lvcreate -l 100%FREE -n lv_shared_backup vg_shared_backup mkfs.ext3 /dev/vg_shared_pc/lv_shared_pc mkfs.ext3 /dev/vg_shared_db/lv_shared_db mkfs.ext3 /dev/vg_shared_arch/lv_shared_arch mkfs.ext3 /dev/vg_shared_logs/lv_shared_logs mkfs.ext3 /dev/vg_shared_data/lv_shared_data mkfs.ext3 /dev/vg_shared_backup/lv_shared_backup mkdir /opt/pc mkdir /opt/pc_db mkdir /opt/pc_arch mkdir /opt/pc_logs mkdir /opt/pc_data mkdir /opt/pc_backup","title":"HA LVM cluster build"},{"location":"linux/Clustering/HA_LVM_cluster_build/#rhel-57-ha-lvm-cluster-setup-guide","text":"version 201600508 created by Seamus Murray After manually building the server via multiple RDP sessions and KVM consoles you sometimes have typos or duplicate key strokes in config files. Before going further, you should ensure that all the hostname's IP address gateways etc are correct.. ensure the host name is correct and a FQDN in /etc/sysconfig/network ensure hostname is not in any of the ifcfg-ethx files ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default restart network services, run hostname command manually or reboot if necessary","title":"RHEL 5.7 HA-LVM cluster setup guide"},{"location":"linux/Clustering/HA_LVM_cluster_build/#ensure-ntp-is-working-correctly","text":"","title":"Ensure NTP is working correctly,"},{"location":"linux/Clustering/HA_LVM_cluster_build/#you-may-need-to-manually-sync-the-time","text":"service ntp stop ntpdate 192.168.100.1 ntpdate 192.168.100.2 service ntp start Important , Due to the application vendor support requirements, the cluster servers need to remain on RHEL 5.7. ie, they cannot be updated to RHEL 5.8 or 5.9 To ensure the hosts stay on RHEL 5.7 there has been a dedicated software channel created for this specific project. The hosts have to be registered against this specific channel.... if they subscribe to the main RHEL 5.x channel they will be updated to the latest package versions which will make then unsupportable from the application vendors perspective. rhel-x86_64-server-5_7 Clone Red Hat Network Tools for RHEL Server (v.5.7 64-bit x86_64) rhel-x86_64-server-cluster-5_7","title":"You may need to manually sync the time"},{"location":"linux/Clustering/HA_LVM_cluster_build/#download-and-install-the-certificates-for-the-signed-rpm-packages","text":"wget --no-check-certificate https://satellite1.local/pub/rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm wget --no-check-certificate https://satellite1.local/pub/rpm-gpg-key-1.0-7.noarch.rpm rpm -Uvh rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm rpm-gpg-key-corp-1.0-7.noarch.rpm sed -i 's/^\\(sslCACert=\\).*/\\1\\/usr\\/share\\/rhn\\/RHN-ORG-TRUSTED-SSL-CERT/' /etc/sysconfig/rhn/up2date","title":"Download and install the certificates for the signed RPM packages"},{"location":"linux/Clustering/HA_LVM_cluster_build/#ensure-the-that-the-redhat-certificate-is-installed-not-installed-by-default","text":"rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release","title":"Ensure the that the RedHat certificate is installed # not installed by default"},{"location":"linux/Clustering/HA_LVM_cluster_build/#if-not-you-may-get-errors-like","text":"Public key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed","title":"if not you may get errors like..."},{"location":"linux/Clustering/HA_LVM_cluster_build/#register-the-server-with-the-redhat-satellite-server-using-one-of-the-following-keys","text":"rhnreg_ks --activationkey 1-bc5fb717c87abcdefghijklm --serverUrl https://satellite1.local/XMLRPC rhnreg_ks --activationkey 1-5bbf25c4abcdefghijklm123 --serverUrl https://satellite1.local/XMLRPC","title":"Register the server with the RedHat satellite server using one of the following keys"},{"location":"linux/Clustering/HA_LVM_cluster_build/#check-if-the-repo-access-is-working","text":"yum clean all yum list","title":"check if the repo access is working"},{"location":"linux/Clustering/HA_LVM_cluster_build/#install-the-addons","text":"yum install java-1.6.0-openjdk yum install samba3x-winbind iptables lsof krb5-workstation pam_passwdqc xauth yum install ipv6-disable","title":"Install the addons..."},{"location":"linux/Clustering/HA_LVM_cluster_build/#fixing-the-inconsistent-path-ordering-when-using-friendly_names-and-a-separate-var-partition-on-rhel-5","text":"","title":"Fixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5"},{"location":"linux/Clustering/HA_LVM_cluster_build/#cause","text":"RHEL 5 multipath bindings file is located in var /var isn't mounted before multipath is configured redhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot","title":"cause"},{"location":"linux/Clustering/HA_LVM_cluster_build/#solution-work-around","text":"Edit the /etc/multipath.conf","title":"solution (work around)"},{"location":"linux/Clustering/HA_LVM_cluster_build/#add-the-following-lines-to-multipath-config-to-set-netapp-specific-settings","text":"defaults { user_friendly_names yes bindings_file /etc/multipath/bindings ##changed flush_on_last_del yes max_fds max pg_prio_calc avg queue_without_daemon no } devices { device { vendor \"NETAPP\" product \"LUN\" path_checker tur path_selector \"round-robin 0\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" # prio_callout \"/sbin/mpath_prio_ontap /dev/%n\" prio_callout \"/sbin/mpath_prio_alua /dev/%n\" # features \"1 queue_if_no_path\" features \"3 queue_if_no_path pg_init_retries 50\" # hardware_handler \"0\" hardware_handler \"1 alua\" path_grouping_policy group_by_prio failback immediate rr_weight uniform rr_min_io 128 } } mkdir /etc/multipath cp /var/lib/multipath/bindings /etc/multipath/bindings cd /boot cp initrd*img initrd*img.multipath-bak mkinitrd -f initrd-`uname -r`.img `uname -r` ls -ltr #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size edit the multipath bindings and ensure the shared luns have the same friendly names across all nodes recreate initial ram disk image reboot all nodes and ensure all nodes see the luns at same friendly name cat /etc/multipath/bindings # Multipath bindings, Version : 1.0 # NOTE: this file is automatically maintained by the multipath program. # You should not need to edit this file in normal circumstances. # # Format: # alias wwid # mpsys 360a9800037542d73442443123456755 mpath1 360a9800037542d73442443123456757 mpath2 360a9800037542d73442443123456759 mpath3 360a9800037542d7344244312345672f mpath4 360a9800037542d73442443123456762 mpath5 360a9800037542d73442443123456764 mpath6 360a9800037542d73442443123456766","title":"add the following lines to multipath config to set netapp specific settings"},{"location":"linux/Clustering/HA_LVM_cluster_build/#final-multipathconf-file-should-look-somthing-like","text":"defaults { user_friendly_names yes bindings_file /etc/multipath/bindings flush_on_last_del yes max_fds max pg_prio_calc avg queue_without_daemon no } blacklist { devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\" devnode \"^(hd|xvd|vd)[a-z]*\" # wwid \"*\" } # Make sure our multipath devices are enabled. blacklist_exceptions { wwid \"360a9800037542d73442443123456755\" } multipaths { multipath { wwid 360a9800037542d73442443123456755 alias mpsys } } devices { device { vendor \"NETAPP\" product \"LUN\" path_checker tur path_selector \"round-robin 0\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" # prio_callout \"/sbin/mpath_prio_ontap /dev/%n\" prio_callout \"/sbin/mpath_prio_alua /dev/%n\" # features \"1 queue_if_no_path\" features \"3 queue_if_no_path pg_init_retries 50\" # hardware_handler \"0\" hardware_handler \"1 alua\" path_grouping_policy group_by_prio failback immediate rr_weight uniform rr_min_io 128 } }","title":"Final multipath.conf file should look somthing like"},{"location":"linux/Clustering/HA_LVM_cluster_build/#change-fstab","text":"#/dev/mapper/mpath0p1 /boot ext3 defaults 1 2 /dev/mapper/mpsysp1 /boot ext3 defaults 1 2","title":"Change fstab"},{"location":"linux/Clustering/HA_LVM_cluster_build/#cluster-preconfiguration","text":"","title":"Cluster preconfiguration..."},{"location":"linux/Clustering/HA_LVM_cluster_build/#ensure-ntp-is-working-correctly_1","text":"you may need to manually sync the time service ntp stop ntpdate 192.168.0.1 ntpdate 192.168.0.2 service ntp start","title":"Ensure NTP is working correctly,"},{"location":"linux/Clustering/HA_LVM_cluster_build/#install-cluster-software-and-fencing-tools","text":"yum install OpenIPMI-tools.x86_64 cman rgmanager lucci ricci","title":"Install cluster software and fencing tools"},{"location":"linux/Clustering/HA_LVM_cluster_build/#disable-the-cluster-from-starting-up-until-you-have-finished-the-config","text":"chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off","title":"Disable the cluster from starting up... until you have finished the config"},{"location":"linux/Clustering/HA_LVM_cluster_build/#edit-firewall-rules-to-allow-cluster-traffic","text":"iptables -I INPUT --protocol tcp --dport 22 -j ACCEPT # Cluster iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT turn off firewall for private nic iptables -I INPUT -i eth1 -j ACCEPT save the iptables so that they will be persistant","title":"Edit firewall rules to allow cluster traffic"},{"location":"linux/Clustering/HA_LVM_cluster_build/#configure-the-privateheartbeat-nic","text":"*add multicast route for private/heartbeat /etc/sysconfig/network-scripts/route-eth1 239.0.0.0/4 dev eth1","title":"Configure the private/heartbeat nic"},{"location":"linux/Clustering/HA_LVM_cluster_build/#define-which-multicast-address-to-use-for-the-cluster","text":"239.192.0.1 for App 1 239.192.0.2 for App 2 239.192.0.3 for App 3","title":"Define which multicast address to use for the cluster"},{"location":"linux/Clustering/HA_LVM_cluster_build/#add-the-private-hostnamesdomains-to-etchosts","text":"# cluster nodes 192.168.0.1 hostname1.local hostname1 10.10.10.1 hostname1.private hostname1-priv 192.168.0.2 hostname2.local hostname2 10.10.10.2 hostname2.private hostname2-priv 192.168.0.3 hostname-vip.local hostname-vip if you want to use luci warning due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs you have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config You can use ricci/luci to import an existing cluster Therefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci","title":"Add the private hostnames/domains to /etc/hosts"},{"location":"linux/Clustering/HA_LVM_cluster_build/#starting-ricciluci","text":"if you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password start up ricci on both servers and luci on one server setup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci start browser window https://127.0.0.1:80","title":"Starting ricci/luci"},{"location":"linux/Clustering/HA_LVM_cluster_build/#simple-clusterconf-file-to-get-you-started","text":"<?xml version=\"1.0\"?> <cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"239.192.0.1\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/>= </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"> </service> </rm> </cluster> scp cluster.conf hostname1:/etc/cluster/cluster.conf ccs_tool update cluster.conf","title":"Simple cluster.conf file to get you started"},{"location":"linux/Clustering/HA_LVM_cluster_build/#configure-fencing","text":"Setup ipmi profile on the hardware Test to see if fencing works echo -e \"ipaddr=192.168.1.1 \\nlogin= \\npasswd= \\naction=status\" | fence_ipmilan Add the fencing details to the cluster.conf file service rgmanager start service cman start clustat cman_tool status tail -f /var/log/messages manual fence override fence_ack_manual -e -n hostname1.private manaul service relocation clusvcadm -r ServiceName ifconfig will not display a VIP you have to run ip address show you may want to disable the acpi daemon otherwise your server may not switch off fast enough chkconfig --level 234 5 acpid off chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off","title":"Configure fencing"},{"location":"linux/Clustering/HA_LVM_cluster_build/#setup-ha-lvm","text":"...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide","title":"setup HA-LVM"},{"location":"linux/Clustering/HA_LVM_cluster_build/#edit-etclvmlvmconf","text":"Ensure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'. Edit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file: volume_list = [ \"vgsys\", \"@hostname1.private\" ]","title":"edit /etc/lvm/lvm.conf"},{"location":"linux/Clustering/HA_LVM_cluster_build/#create-the-pv-vg-lv-and-filesystems-from-one-of-the-nodes","text":"pvcreate /dev/mapper/mpath1 pvcreate /dev/mapper/mpath2 pvcreate /dev/mapper/mpath3 pvcreate /dev/mapper/mpath4 pvcreate /dev/mapper/mpath5 pvcreate /dev/mapper/mpath6 vgcreate vg_shared_pc /dev/mapper/mpath1 vgcreate vg_shared_db /dev/mapper/mpath2 vgcreate vg_shared_arch /dev/mapper/mpath3 vgcreate vg_shared_logs /dev/mapper/mpath4 vgcreate vg_shared_data /dev/mapper/mpath5 vgcreate vg_shared_backup /dev/mapper/mpath6 lvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc lvcreate -l 100%FREE -n lv_shared_db vg_shared_db lvcreate -l 100%FREE -n lv_shared_arch vg_shared_arch lvcreate -l 100%FREE -n lv_shared_logs vg_shared_logs lvcreate -l 100%FREE -n lv_shared_data vg_shared_data lvcreate -l 100%FREE -n lv_shared_backup vg_shared_backup mkfs.ext3 /dev/vg_shared_pc/lv_shared_pc mkfs.ext3 /dev/vg_shared_db/lv_shared_db mkfs.ext3 /dev/vg_shared_arch/lv_shared_arch mkfs.ext3 /dev/vg_shared_logs/lv_shared_logs mkfs.ext3 /dev/vg_shared_data/lv_shared_data mkfs.ext3 /dev/vg_shared_backup/lv_shared_backup mkdir /opt/pc mkdir /opt/pc_db mkdir /opt/pc_arch mkdir /opt/pc_logs mkdir /opt/pc_data mkdir /opt/pc_backup","title":"Create the PV VG LV and filesystems from one of the nodes"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/","text":"RHEL 5.9 HA-LVM cluster install and setup guide version 201600508 created by Seamus Murray Prerequisites 2 machines either physical or virtual RHEL OS packages RHEL Cluster packages these can be sourced from Note write guides for each of these methods * RedHat directly RHN * Local YUM Repository * Local Satellite/Spacewalk server * Local media cdrom/dvdrom/usb drive/iso image etc. In this guide I will be using the ISO of the RHEL 5.9 install This ISO was downloaded from RHN and contains the following Repositories Server Cluster ClusterStorage VT for the initial OS install we are just going to use the Server repo Boot from the DVD/ISO Step though the installer note the package choice is dependant on what you want to do with the servers later Eg if you want to run a GUI and web browser you will have to install those Groups personally i always try to install the minimum number of packages to get the specific job done. @base @core @editors @text-internet Once you install has completed and you have booted into your new OS. setup the hosts to connect to a RHEL OS packages RHEL Cluster packages these can be sourced from * RedHat directy RHN * Local YUM Repository * Local Satellite/Spacewalk server * Local media cdrom/dvdrom/usb drive/iso image etc. Install the required cluster utils for this example it is just * cman * rgamanger * openais Note: for a HA-LVM cluster these is no need for any distributed lockmanager GFS etc... if these are installed its best to remove them now Disable the cluster from starting up... until you have finished the config chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off Before we setup the cluster.. * ensure your hostnames IP addresses and interfaces are in order. __in this exaple each node has 2 physical interfaces 1. eth0 the public side, this is where the service that will be clusterd will run from https,nfs,ftp etc 2. eth1 the private interface that is just used for cluster communication broadcast,multicast or unicast * ensure the host name is correct and a FQDN in /etc/sysconfig/network * ensure hostname is not in any of the ifcfg-ethx files * ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default * restart network services, run hostname command manually or reboot if necessary* * ensure the hostnames for all IPs on all the cluster nodes are listed in the /etc/hosts file sample /etc/hosts file 192.168.56.101 rhel59halvmn1.local rhel59halvmn1 10.10.100.101 rhel59halvmn1.private rhel59halvmn1-priv 192.168.56.102 rhel59halvmn2.local rhel59halvmn2 10.10.100.102 rhel59halvmn2.private rhel59halvmn2-priv 192.168.56.103 rhel59halvm-vip.local rhel59halvm-vip Setup ssh keys between the nodes to make things easier # ssh-copy-id -i /root/.ssh/id_rsa.pub rhel59halvmn2 Ensure the time on all cluster nodes are in sync you may need to manually sync the time alias REMOTETIME='ssh rhel59halvmn1 -C date' date -s \"`REMOTETIME`\" service ntp stop ntpdate 192.168.1.1 ntpdate 192.168.1.1 service ntp start Configure iptables to allow the cluster traffic through## iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 16851 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 21064 -j ACCEPT Create a /etc/cluster/cluster.conf file __note personaly I start with an existing complete cluste.conf file, edit it to match the desired config then start the cluster __if you want a better understanding of the config follow the steps below <?xml version=\"1.0\"?> <cluster name=\"rhel59halvm\" config_version=\"1\"> <clusternodes> <clusternode name=\"rhel59halvmn1.private\" nodeid=\"1\" /> <clusternode name=\"rhel59halvmn2.private\" nodeid=\"2\" /> </clusternodes> <fencedevices /> <rm> <failoverdomains /> <resources /> </rm> </cluster> Note: this is a very simple cluster.conf file there is no fencing resources services etc.. Start the cluster to see if everything works [root@rhel59halvmn1 cluster]# service cman start Starting cluster: Loading modules... done Mounting configfs... done Starting ccsd... done Starting cman... done Starting daemons... done Starting fencing... done Tuning DLM... done [ OK ] [root@rhel59halvmn1 cluster]# clustat Cluster Status for rhel59halvm @ Wed Jun 26 17:59:12 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ rhel59halvmn1.private 1 Online, Local rhel59halvmn2.private 2 Online Add your fencing device Note: because we didnt specify a fence method with the original cluster/conf version 1 we will now have to add the fence device and a fence method for each node. Personally I'd rather do it by hand using a text editor but... in this example im using the fence_manual device note this is not a real fence device do not usse this in production see man fence_manual for more info__ If you want you can use the ccs_tool to delete the individual node's config then re-add the node's config with the fence method ccs_tool addfence manual fake-parameter Note: because the fence_manual is a fake agent it doesnt require any parameters but the ccs_tool requires atleast 1 ccs_tool addnode rhel59halvmn1 -n1 -f manual ccs_tool delnode rhel59halvmn2.private ccs_tool addnode rhel59halvmn2 -n2 -f manual cat /etc/cluster/cluster.conf <?xml version=\"1.0\"?> <cluster alias=\"rhel59halvm\" config_version=\"7\" name=\"rhel59halvm\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"rhel59halvmn1\" votes=\"1\" nodeid=\"1\"> <fence> <method name=\"single\"> <device name=\"manual\"/> </method> </fence> </clusternode> <clusternode name=\"rhel59halvmn2\" votes=\"1\" nodeid=\"2\"> <fence> <method name=\"single\"> <device name=\"manual\"/> </method> </fence> </clusternode> </clusternodes> <fencedevices> <fencedevice name=\"manual\" agent=\"fake-parameter\"/> </fencedevices> <rm> <failoverdomains/> <resources/> </rm> </cluster> sync the cluster.conf file using sthe ccs_tool ccs_tool update /etc/cluster/cluster.conf Configure the private/heartbeat nic *add multicast route for private/heartbeat /etc/sysconfig/network-scripts/route-eth1 239.0.0.0/4 dev eth1 Ensure the that the RedHat certificate is installed # not installed by default rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release if not you may get errors like... Public key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed Check if the repo access is working yum clean all yum list Edit firewall rules to allow cluster traffic #/opt/firewall-rules iptables -I INPUT --protocol tcp --dport 22 -j ACCEPT # Cluster iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT # turn off firewall for private nic iptables -I INPUT -i eth1 -j ACCEPT if you want to use luci warning due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs you have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config You can use ricci/luci to import an existing cluster Therefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci Starting ricci/luci if you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password start up ricci on both servers and luci on one server setup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci start browser window https://127.0.0.1:80 Simple cluster.conf file to get you started <?xml version=\"1.0\"?> <cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"239.192.0.1\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/>= </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"> </service> </rm> </cluster> scp cluster.conf hostname1:/etc/cluster/cluster.conf ccs_tool update cluster.conf Configure fencing Setup ipmi profile on the hardware Test to see if fencing works echo -e \"ipaddr=192.168.1.1 \\nlogin= \\npasswd= \\naction=status\" | fence_ipmilan Add the fencing details to the cluster.conf file service rgmanager start service cman start clustat cman_tool status tail -f /var/log/messages manual fence override fence_ack_manual -e -n hostname1.private manaul service relocation clusvcadm -r ServiceName ifconfig will not display a VIP you have to run ip address show you may want to disable the acpi daemon otherwise your server may not switch off fast enough chkconfig --level 234 5 acpid off chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off setup HA-LVM ...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide edit /etc/lvm/lvm.conf Ensure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'. Edit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file: volume_list = [ \"vgsys\", \"@rhel59halvm1.private\" ] Create the PV VG LV and filesystems on one of the nodes pvcreate /dev/mapper/mpath1 vgcreate vg_shared_pc /dev/mapper/mpath1 lvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc mkfs.ext3 /dev/vg_shared_pc/lv_shared_pc mkdir /mnt/pc","title":"HA LVM cluster build guide"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#rhel-59-ha-lvm-cluster-install-and-setup-guide","text":"version 201600508 created by Seamus Murray Prerequisites 2 machines either physical or virtual RHEL OS packages RHEL Cluster packages these can be sourced from Note write guides for each of these methods * RedHat directly RHN * Local YUM Repository * Local Satellite/Spacewalk server * Local media cdrom/dvdrom/usb drive/iso image etc. In this guide I will be using the ISO of the RHEL 5.9 install This ISO was downloaded from RHN and contains the following Repositories Server Cluster ClusterStorage VT for the initial OS install we are just going to use the Server repo Boot from the DVD/ISO Step though the installer note the package choice is dependant on what you want to do with the servers later Eg if you want to run a GUI and web browser you will have to install those Groups personally i always try to install the minimum number of packages to get the specific job done. @base @core @editors @text-internet Once you install has completed and you have booted into your new OS. setup the hosts to connect to a RHEL OS packages RHEL Cluster packages these can be sourced from * RedHat directy RHN * Local YUM Repository * Local Satellite/Spacewalk server * Local media cdrom/dvdrom/usb drive/iso image etc. Install the required cluster utils for this example it is just * cman * rgamanger * openais Note: for a HA-LVM cluster these is no need for any distributed lockmanager GFS etc... if these are installed its best to remove them now","title":"RHEL 5.9 HA-LVM cluster install and setup guide"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#disable-the-cluster-from-starting-up-until-you-have-finished-the-config","text":"chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off Before we setup the cluster.. * ensure your hostnames IP addresses and interfaces are in order. __in this exaple each node has 2 physical interfaces 1. eth0 the public side, this is where the service that will be clusterd will run from https,nfs,ftp etc 2. eth1 the private interface that is just used for cluster communication broadcast,multicast or unicast * ensure the host name is correct and a FQDN in /etc/sysconfig/network * ensure hostname is not in any of the ifcfg-ethx files * ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default * restart network services, run hostname command manually or reboot if necessary* * ensure the hostnames for all IPs on all the cluster nodes are listed in the /etc/hosts file sample /etc/hosts file 192.168.56.101 rhel59halvmn1.local rhel59halvmn1 10.10.100.101 rhel59halvmn1.private rhel59halvmn1-priv 192.168.56.102 rhel59halvmn2.local rhel59halvmn2 10.10.100.102 rhel59halvmn2.private rhel59halvmn2-priv 192.168.56.103 rhel59halvm-vip.local rhel59halvm-vip","title":"Disable the cluster from starting up... until you have finished the config"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#setup-ssh-keys-between-the-nodes-to-make-things-easier","text":"# ssh-copy-id -i /root/.ssh/id_rsa.pub rhel59halvmn2","title":"Setup ssh keys between the nodes to make things easier"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#ensure-the-time-on-all-cluster-nodes-are-in-sync","text":"you may need to manually sync the time alias REMOTETIME='ssh rhel59halvmn1 -C date' date -s \"`REMOTETIME`\" service ntp stop ntpdate 192.168.1.1 ntpdate 192.168.1.1 service ntp start","title":"Ensure the time on all cluster nodes are in sync"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#configure-iptables-to-allow-the-cluster-traffic-through","text":"iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 16851 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 21064 -j ACCEPT","title":"Configure iptables to allow the cluster traffic through##"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#create-a-etcclusterclusterconf-file","text":"__note personaly I start with an existing complete cluste.conf file, edit it to match the desired config then start the cluster __if you want a better understanding of the config follow the steps below <?xml version=\"1.0\"?> <cluster name=\"rhel59halvm\" config_version=\"1\"> <clusternodes> <clusternode name=\"rhel59halvmn1.private\" nodeid=\"1\" /> <clusternode name=\"rhel59halvmn2.private\" nodeid=\"2\" /> </clusternodes> <fencedevices /> <rm> <failoverdomains /> <resources /> </rm> </cluster> Note: this is a very simple cluster.conf file there is no fencing resources services etc..","title":"Create a /etc/cluster/cluster.conf file"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#start-the-cluster-to-see-if-everything-works","text":"[root@rhel59halvmn1 cluster]# service cman start Starting cluster: Loading modules... done Mounting configfs... done Starting ccsd... done Starting cman... done Starting daemons... done Starting fencing... done Tuning DLM... done [ OK ] [root@rhel59halvmn1 cluster]# clustat Cluster Status for rhel59halvm @ Wed Jun 26 17:59:12 2013 Member Status: Quorate Member Name ID Status ------ ---- ---- ------ rhel59halvmn1.private 1 Online, Local rhel59halvmn2.private 2 Online","title":"Start the cluster to see if everything works"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#add-your-fencing-device","text":"Note: because we didnt specify a fence method with the original cluster/conf version 1 we will now have to add the fence device and a fence method for each node. Personally I'd rather do it by hand using a text editor but... in this example im using the fence_manual device note this is not a real fence device do not usse this in production see man fence_manual for more info__ If you want you can use the ccs_tool to delete the individual node's config then re-add the node's config with the fence method ccs_tool addfence manual fake-parameter Note: because the fence_manual is a fake agent it doesnt require any parameters but the ccs_tool requires atleast 1 ccs_tool addnode rhel59halvmn1 -n1 -f manual ccs_tool delnode rhel59halvmn2.private ccs_tool addnode rhel59halvmn2 -n2 -f manual cat /etc/cluster/cluster.conf <?xml version=\"1.0\"?> <cluster alias=\"rhel59halvm\" config_version=\"7\" name=\"rhel59halvm\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"rhel59halvmn1\" votes=\"1\" nodeid=\"1\"> <fence> <method name=\"single\"> <device name=\"manual\"/> </method> </fence> </clusternode> <clusternode name=\"rhel59halvmn2\" votes=\"1\" nodeid=\"2\"> <fence> <method name=\"single\"> <device name=\"manual\"/> </method> </fence> </clusternode> </clusternodes> <fencedevices> <fencedevice name=\"manual\" agent=\"fake-parameter\"/> </fencedevices> <rm> <failoverdomains/> <resources/> </rm> </cluster>","title":"Add your fencing device"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#sync-the-clusterconf-file-using-sthe-ccs_tool","text":"ccs_tool update /etc/cluster/cluster.conf","title":"sync the cluster.conf file using sthe ccs_tool"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#configure-the-privateheartbeat-nic","text":"*add multicast route for private/heartbeat /etc/sysconfig/network-scripts/route-eth1 239.0.0.0/4 dev eth1","title":"Configure the private/heartbeat nic"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#ensure-the-that-the-redhat-certificate-is-installed-not-installed-by-default","text":"rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release","title":"Ensure the that the RedHat certificate is installed # not installed by default"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#if-not-you-may-get-errors-like","text":"Public key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed","title":"if not you may get errors like..."},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#check-if-the-repo-access-is-working","text":"yum clean all yum list","title":"Check if the repo access is working"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#edit-firewall-rules-to-allow-cluster-traffic","text":"#/opt/firewall-rules iptables -I INPUT --protocol tcp --dport 22 -j ACCEPT # Cluster iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT iptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT # turn off firewall for private nic iptables -I INPUT -i eth1 -j ACCEPT if you want to use luci warning due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs you have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config You can use ricci/luci to import an existing cluster Therefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci","title":"Edit firewall rules to allow cluster traffic"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#starting-ricciluci","text":"if you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password start up ricci on both servers and luci on one server setup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci start browser window https://127.0.0.1:80","title":"Starting ricci/luci"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#simple-clusterconf-file-to-get-you-started","text":"<?xml version=\"1.0\"?> <cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"239.192.0.1\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/>= </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"> </service> </rm> </cluster> scp cluster.conf hostname1:/etc/cluster/cluster.conf ccs_tool update cluster.conf","title":"Simple cluster.conf file to get you started"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#configure-fencing","text":"Setup ipmi profile on the hardware Test to see if fencing works echo -e \"ipaddr=192.168.1.1 \\nlogin= \\npasswd= \\naction=status\" | fence_ipmilan Add the fencing details to the cluster.conf file service rgmanager start service cman start clustat cman_tool status tail -f /var/log/messages manual fence override fence_ack_manual -e -n hostname1.private manaul service relocation clusvcadm -r ServiceName ifconfig will not display a VIP you have to run ip address show you may want to disable the acpi daemon otherwise your server may not switch off fast enough chkconfig --level 234 5 acpid off chkconfig --levels 2345 cman off chkconfig --levels 2345 rgmanager off chkconfig --levels 2345 ricci off chkconfig --levels 2345 luci off","title":"Configure fencing"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#setup-ha-lvm","text":"...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide","title":"setup HA-LVM"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#edit-etclvmlvmconf","text":"Ensure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'. Edit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file: volume_list = [ \"vgsys\", \"@rhel59halvm1.private\" ]","title":"edit /etc/lvm/lvm.conf"},{"location":"linux/Clustering/HA_LVM_cluster_build_guide/#create-the-pv-vg-lv-and-filesystems-on-one-of-the-nodes","text":"pvcreate /dev/mapper/mpath1 vgcreate vg_shared_pc /dev/mapper/mpath1 lvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc mkfs.ext3 /dev/vg_shared_pc/lv_shared_pc mkdir /mnt/pc","title":"Create the PV VG LV and filesystems    on one of the nodes"},{"location":"linux/Clustering/HA_LVM_manual_failover.txt/","text":"status of both nodes before failover [root@node2 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:26:36 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, rgmanager node2.private 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node1.private started [root@node2 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node2 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 8G [root@node2 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi--- 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 16.00G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G [root@node1 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:26:39 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, rgmanager node2.private 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node1.private started [root@node2 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node2 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 8G [root@node1 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi-ao 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 14.62G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G syslog of both nodes during failover clusvcadm -r ServiceName May 31 15:40:36 node2 root: seamus running clusvcadm -r ServiceName from node1 May 31 15:43:25 node2 clurgmgrd[6214]: Starting stopped service service:ServiceName May 31 15:43:26 node2 clurgmgrd: [6214]: Activating sharedVG/sharedLV May 31 15:43:26 node2 clurgmgrd: [6214]: Making resilient : lvchange -ay sharedVG/sharedLV May 31 15:43:26 node2 clurgmgrd: [6214]: Resilient command: lvchange -ay sharedVG/sharedLV {wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]} May 31 15:43:26 node2 multipathd: dm-12: devmap not registered, can't remove May 31 15:43:26 node2 multipathd: dm-12: add map (uevent) May 31 15:43:26 node2 kernel: kjournald starting. Commit interval 5 seconds May 31 15:43:26 node2 kernel: EXT3 FS on dm-12, internal journal May 31 15:43:26 node2 kernel: EXT3-fs: mounted filesystem with ordered data mode. May 31 15:43:28 node2 avahi-daemon[5577]: Registering new address record for 192.168.1.3 on eth0. May 31 15:44:57 node2 clurgmgrd[6214]: Service service:ServiceName started May 31 15:40:36 node1 root: seamus running clusvcadm -r ServiceName May 31 15:40:45 node1 clurgmgrd[6229]: Stopping service service:ServiceName May 31 15:43:14 node1 avahi-daemon[5564]: Withdrawing address record for 192.168.1.3 on eth0. May 31 15:43:24 node1 multipathd: dm-12: umount map (uevent) May 31 15:43:24 node1 clurgmgrd: [6229]: Deactivating sharedVG/sharedLV May 31 15:43:24 node1 clurgmgrd: [6229]: Making resilient : lvchange -an sharedVG/sharedLV May 31 15:43:24 node1 clurgmgrd: [6229]: Resilient command: lvchange -an sharedVG/sharedLV {wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]} May 31 15:43:24 node1 multipathd: dm-12: remove map (uevent) May 31 15:43:25 node1 clurgmgrd: [6229]: Removing ownership tag (node1.private) from sharedVG/sharedLV May 31 15:43:25 node1 clurgmgrd[6229]: Service service:ServiceName is stopped May 31 15:44:57 node1 clurgmgrd[6229]: Service service:ServiceName is now running on member 2 status of both nodes after failover [root@node2 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:47:29 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, rgmanager node2.private 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node2.private started [root@node2 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node2 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 8G [root@node2 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi-ao 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 16.00G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G [root@node1 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:48:09 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, Local, rgmanager node2.private 2 Online, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node2.private started [root@node1 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node1 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 10.25G [root@node1 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi--- 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 14.62G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G","title":"HA LVM manual failover.txt"},{"location":"linux/Clustering/HA_LVM_manual_failover.txt/#status-of-both-nodes-before-failover","text":"[root@node2 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:26:36 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, rgmanager node2.private 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node1.private started [root@node2 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node2 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 8G [root@node2 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi--- 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 16.00G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G [root@node1 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:26:39 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, rgmanager node2.private 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node1.private started [root@node2 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node2 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 8G [root@node1 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi-ao 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 14.62G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G","title":"status of both nodes before failover"},{"location":"linux/Clustering/HA_LVM_manual_failover.txt/#syslog-of-both-nodes-during-failover","text":"clusvcadm -r ServiceName May 31 15:40:36 node2 root: seamus running clusvcadm -r ServiceName from node1 May 31 15:43:25 node2 clurgmgrd[6214]: Starting stopped service service:ServiceName May 31 15:43:26 node2 clurgmgrd: [6214]: Activating sharedVG/sharedLV May 31 15:43:26 node2 clurgmgrd: [6214]: Making resilient : lvchange -ay sharedVG/sharedLV May 31 15:43:26 node2 clurgmgrd: [6214]: Resilient command: lvchange -ay sharedVG/sharedLV {wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]} May 31 15:43:26 node2 multipathd: dm-12: devmap not registered, can't remove May 31 15:43:26 node2 multipathd: dm-12: add map (uevent) May 31 15:43:26 node2 kernel: kjournald starting. Commit interval 5 seconds May 31 15:43:26 node2 kernel: EXT3 FS on dm-12, internal journal May 31 15:43:26 node2 kernel: EXT3-fs: mounted filesystem with ordered data mode. May 31 15:43:28 node2 avahi-daemon[5577]: Registering new address record for 192.168.1.3 on eth0. May 31 15:44:57 node2 clurgmgrd[6214]: Service service:ServiceName started May 31 15:40:36 node1 root: seamus running clusvcadm -r ServiceName May 31 15:40:45 node1 clurgmgrd[6229]: Stopping service service:ServiceName May 31 15:43:14 node1 avahi-daemon[5564]: Withdrawing address record for 192.168.1.3 on eth0. May 31 15:43:24 node1 multipathd: dm-12: umount map (uevent) May 31 15:43:24 node1 clurgmgrd: [6229]: Deactivating sharedVG/sharedLV May 31 15:43:24 node1 clurgmgrd: [6229]: Making resilient : lvchange -an sharedVG/sharedLV May 31 15:43:24 node1 clurgmgrd: [6229]: Resilient command: lvchange -an sharedVG/sharedLV {wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]} May 31 15:43:24 node1 multipathd: dm-12: remove map (uevent) May 31 15:43:25 node1 clurgmgrd: [6229]: Removing ownership tag (node1.private) from sharedVG/sharedLV May 31 15:43:25 node1 clurgmgrd[6229]: Service service:ServiceName is stopped May 31 15:44:57 node1 clurgmgrd[6229]: Service service:ServiceName is now running on member 2","title":"syslog of both nodes during failover"},{"location":"linux/Clustering/HA_LVM_manual_failover.txt/#status-of-both-nodes-after-failover","text":"[root@node2 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:47:29 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, rgmanager node2.private 2 Online, Local, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node2.private started [root@node2 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node2 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 8G [root@node2 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi-ao 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 16.00G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G [root@node1 ~]# clustat Cluster Status for round-and-round @ Fri May 31 15:48:09 2013 Member Status: Quorate Member Name ID Status ------ ---- --- ------ node1.private 1 Online, Local, rgmanager node2.private 2 Online, rgmanager Service Name Owner (Last) State ------- ---- ----- ------ ----- service:ServiceName node2.private started [root@node1 ~]# pvs PV VG Fmt Attr PSize /dev/mpath/shared sharedVG lvm2 a- 60G /dev/mpath/mpsysp2 vgsys lvm2 a- 49G [root@node1 ~]# vgs VG #PV #LV #SN Attr VSize VFree sharedVG 1 1 0 wz--n- 60G 0 vgsys 1 7 0 wz--n- 49G 10.25G [root@node1 ~]# lvs LV VG Attr LSize sharedLV sharedVG -wi--- 60G home vgsys -wi-ao 2.00G root vgsys -wi-ao 14.62G swap vgsys -wi-ao 10.00G tmp vgsys -wi-ao 5.00G var vgsys -wi-ao 5.00G var_log vgsys -wi-ao 2.00G var_log_audit vgsys -wi-ao 1.00G","title":"status of both nodes after failover"},{"location":"linux/Clustering/RHEL4_cluster_installation/","text":"Red Hat 4 Cluster Install #!/bin/bash exit # this is here encase you run this as a script # install cluster suite for rhes4u3 last updated 17th April 2007 by Seamus # Below details how install the redhat cluster suite and GFS on any redhat ES4 Update 3 box # the prerequisites are based upon the RHEL4u3 kickstart build prepared by Seamus # fileserver.example.com:/u1/Distros/KickStart_rhes4u3/RHEL4u3_32bit.iso # All of the software below is installed from an NFS server # Simply cut and paste the sections into a root shell, # WARNING there are no error checks in this file so please don't run as a script... # just watch the terminal for any errors such as cant find rpm, cant mount NFS volume etc. # I have divided up the commands based on the source of the software # There are a few comments down the bottom which need to be actioned manually NFS_SERVER=fileserver.example.com NFS_SHARE=/u1/Distros VERSION=rhes4u3 NFS_PATH=$NFS_SHARE/$VERSION TEMP_MOUNT=/tmp/software mkdir $TEMP_MOUNT mount $NFS_SERVER:$NFS_SHARE $TEMP_MOUNT ####################################### # Install cluster suite prerequisites # ####################################### # The following RedHat RPM's are not part of the standard build prepared by Seamus # and therefore will need to be installed prior to the installation of the cluster suite mkdir /tmp/rhcs_install mount -o ro,loop -t iso9660 /tmp/software/rhes4u3/rhel-4-u3-rhcs-i386.iso /tmp/rhcs_install RPM_PATH=/tmp/software/rhes4u3/install/RedHat/RPMS/ rpm -ivh $RPM_PATH/libidn-0.5.6-1.i386.rpm rpm -ivh $RPM_PATH/curl-7.12.1-8.rhel4.i386.rpm rpm -ivh $RPM_PATH/php-pear-4.3.9-3.9.i386.rpm $RPM_PATH/php-4.3.9-3.9.i386.rpm rpm -ivh $RPM_PATH/device-mapper-multipath-0.4.5-12.0.RHEL4.i386.rpm ####################################### # Install cluster suite # ####################################### # not all of the following are required to get the cluster running # I figured its easier to have them here just in-case you need them in the future RHCS_PATH=/tmp/rhcs_install/RedHat/RPMS/ rpm -ivh $RHCS_PATH/ipvsadm-1.24-6.i386.rpm rpm -ivh $RHCS_PATH/piranha-0.8.2-1.i386.rpm rpm -ivh $RHCS_PATH/perl-Net-Telnet-3.03-3.noarch.rpm rpm -ivh $RHCS_PATH/magma-1.0.4-0.i686.rpm rpm -ivh $RHCS_PATH/ccs-1.0.3-0.i686.rpm rpm -ivh $RHCS_PATH/gulm-1.0.6-0.i686.rpm rpm -ivh $RHCS_PATH/cman-kernel-2.6.9-43.8.i686.rpm rpm -ivh $RHCS_PATH/cman-1.0.4-0.i686.rpm rpm -ivh $RHCS_PATH/cman-kernel-smp-2.6.9-43.8.i686.rpm rpm -ivh $RHCS_PATH/fence-1.32.18-0.i686.rpm rpm -ivh $RHCS_PATH/rgmanager-1.9.46-0.i386.rpm rpm -ivh $RHCS_PATH/system-config-cluster-1.0.25-1.0.noarch.rpm rpm -ivh $RHCS_PATH/iddev-2.0.0-3.i686.rpm # add these to the production servers rpm -ivh $RHCS_PATH/dlm-1.0.0-5.i686.rpm rpm -ivh $RHCS_PATH/dlm-kernel-2.6.9-41.7.i686.rpm rpm -ivh $RHCS_PATH/dlm-kernheaders-2.6.9-41.7.i686.rpm rpm -ivh $RHCS_PATH/dlm-kernel-smp-2.6.9-41.7.i686.rpm rpm -ivh $RHCS_PATH/magma-plugins-1.0.6-0.i386.rpm ###################################### # Install the few GFS packages # ###################################### mkdir /tmp/rhgfs_install mount -o ro,loop -t iso9660 /tmp/software/rhes4u3/rhel-4-u3-rhgfs-i386.iso /tmp/rhgfs_install RHGFS_PATH=/tmp/rhgfs_install/RedHat/RPMS/ rpm -ivh $RHGFS_PATH/GFS-6.1.5-0.i386.rpm rpm -ivh $RHGFS_PATH/GFS-kernel-smp-2.6.9-49.1.i686.rpm rpm -ivh $RHGFS_PATH/lvm2-cluster-2.02.01-1.2.RHEL4.i386.rpm ######################################################### # # # This is the end of the cluster software installation # # All further steps should be performed manually # # # ######################################################### # You need to setup the host file on each cluster node # either cut and paste into a shell or paste in a VI session # watch out for tabs vs white space when you cut and paste TIME=`date +%Y_%m_%d_%H%M` cp /etc/hosts /etc/hosts_$TIME.bak cat </etc/hosts # Do not remove the following line, or various programs # that require network functionality will fail. 127.0.0.1 localhost.localdomain localhost # production search engine host file last updated 13th April 2007 #temporary blue IPs 10.10.10.50 example10.example.com example10 10.10.10.51 example11.example.com example11 10.10.10.52 example12.example.com example12 10.10.10.53 example13.example.com example13 10.10.10.54 example14.example.com example14 10.10.10.55 example15.example.com example15 10.10.10.56 example16.example.com example16 10.10.10.57 example17.example.com example17 10.10.10.58 example18.example.com example18 10.10.10.59 example19.example.com example19 # ilo interfaces # warning if you move a blade the ilo ip will change 10.10.10.24 example10-ilo 10.10.10.25 example11-ilo 10.10.10.26 example12-ilo 10.10.10.27 example13-ilo 10.10.10.28 example14-ilo 10.10.10.40 example15-ilo 10.10.10.41 example16-ilo 10.10.10.42 example17-ilo 10.10.10.43 example18-ilo 10.10.10.44 example19-ilo #end of hosts file EOF ################################################ # # # Setting up the fibre channel cards and paths # # # ################################################ # make backup of multipath.conf file TIME=`date +%Y_%m_%d_%H%M` cp /etc/multipath.conf /etc/multipath.conf_$TIME.bak #To enable the mutipathd to scan for luns, #you need to comment out the following 3 lines in /etc/multipath.conf #devnode_blacklist { # devnode \"*\" #} # at this point its easiest to reboot you can lookup the rescan method # for you particular HBA driver but these always change # once the box comes back up run multipath -l #you should see somtheing like mpath1 (360060e80000000000000000000000000000) [size=500 GB][features=\"0\"][hwhandler=\"0\"] \\_ round-robin 0 [active] \\_ 0:0:0:0 sda 8:0 [active][ready] \\_ round-robin 0 [enabled] \\_ 1:0:0:0 sdb 8:16 [active][ready] #edit /etc/ssh/sshd_config and change the permit root login just mkdir /root/scripts touch /root/scripts/cluster_services.sh chmod 700 /root/scripts/cluster_services.sh vi /root/scripts/cluster_services.sh >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> #!/bin/bash # this script makes it easy to enable and disable all the cluster services from automatically starting ACTION=$1 if [ -z \"${ACTION}\" ]; then echo \"Usage: $0 on|off|start|stop|status\" exit 1 fi if [ ${ACTION} = \"on\" ]; then printf \"Setting cluster services to start on runlevels 2345\\n\" chkconfig --level 2345 ccsd on chkconfig --level 2345 cman on chkconfig --level 2345 fenced on chkconfig --level 2345 clvmd on chkconfig --level 2345 gfs on chkconfig --level 2345 rgmanager on elif [ ${ACTION} = \"off\" ]; then printf \"Turning cluster services off for runlevels 2345\\n\" chkconfig --level 2345 ccsd off chkconfig --level 2345 cman off chkconfig --level 2345 fenced off chkconfig --level 2345 clvmd off chkconfig --level 2345 gfs off chkconfig --level 2345 rgmanager off elif [ ${ACTION} = \"status\" ]; then /usr/sbin/clustat elif [ ${ACTION} = \"stop\" ]; then printf \"Run the following commands manually in order\";echo echo \"service rgmanager stop\" echo \"service gfs stop\" echo \"service clvmd stop\" echo \"service fenced stop\" echo \"service cman stop\" echo \"service ccsd stop\" elif [ ${ACTION} = \"start\" ]; then printf \"Run the following commands manually in order\";echo echo \"service ccsd start\" echo \"service cman start\" echo \"service fenced start\" echo \"service clvmd start\" echo \"service gfs start\" echo \"service rgmanager start\" fi #end of file >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> # Below is the initial cluster configuration file that I created, the only deviation from standard is.. # I setup a unique ILO FENCE account on each host node, these account name's are based on the host name. # This step is very important when dealing with blade enclosures # The reason for having unique names is due the cluster fencing mechanism. # The problem stems from the way the ILO's IP addresses are assigned via DHCP, # The IP addresses are assigned based upon the physical location of the blade within the enclosure cabinet. # note they are not permanently assigned to a particular blade. # ie of you move a blade from slot 9 to slot 10, its ILO IP address will also change, and what ever blade is put # back into slot 9 will inherit its old IP. Who cares i hear you say. What if you put someone else's server in the old slot 9 # and the cluster tries to fence the the blade you have just moved to slot 10. # If you failed to update the /etc/hosts file The fencing mechanism will shut down the wrong node (if it could log in). # So when ever a blade is physically moved make sure you update the hosts file on each cluster node and manually test login via ssh # There is a bug in the ILOs ssh daemon that prevents you from logging in, a work around is to create a ssh conf file with # ForwardAgent no then call it when you ssh by ssh -F -u mkdir /root/.ssh echo \"ForwardAgent no\" > .ssh/ilo_bug ssh -F .ssh/ilo_bug xxxxxx_test_fence@10.10.10.29 # to restart server typr Fencing via the HP ILO Field Description Name A name for the server with HP iLO support. Login The login name used to access the device. Password The password used to authenticate the connection to the device. Hostname The hostname assigned to the device. ############################################### # # # sample initial /etc/cluster/cluster.conf # # # ############################################### <?xml version=\"1.0\" ?> removed >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> This change is made by the installation of the vm2-cluster-2.02.01-1.2.RHEL4.i386.rpm diff /etc/lvm/lvm.conf /etc/lvm/lvm.conf.lvmconfold 172,173d171 < library_dir = \"/usr/lib\" < locking_library = \"liblvm2clusterlock.so\" 215c213 < locking_type = 2 --- > locking_type = 1 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> The following directory's need to be shared between all nodes via GFS or NFS /home/search /big/search /var/www/html/search manually replicate this file between the nodes /etc/httpd/conf/funnelback-httpd.conf","title":"RHEL4 cluster installation"},{"location":"linux/Clustering/homemade_cluster/","text":"To mitigate a site failure at the primary site the 4 custom_application applications are failed over to the secondary site. A windows 2008 tie-breaker server located at a third site monitors the primary cluster via SSH every 5 seconds. If it detects a failure it asks the DR custom_application server to confirm that the primary site has failed. If both the tie-breaker and the DR server agree that the primary site is down. Then the tie-breaker server executes the failover scripts located on the DR servers which perform the failover. Operating Systems Application servers are RHEL 5.9 Steward server is windows 2008 The tie-breaker server requires... powershell dotnet user account scheduled task for each monitored cluster powershell script ssh wrapper script (powershell to dotnet) ssh.net library log directory public ssh key of the primary and the DR servers (to enable paswordlSERVER login) Files Windows Steward server id_rsa ## ssh private key to allow loginto linux servers id_rsa.pub ## ssh public key to allow loginto linux servers Renci.SshNet.dll ## SSH.net library from http://sshnet.codeplex.com/ SSH-SSERVERions.psd1 ## powershell wrapper for SSH.net from http://www.powershelladmin.com/wiki/SSH_from_PowerShell_using_the_SSH.NET_library SSH-SSERVERions.psm1 ## licence file ssh-fail.ps1 ## powershell script to login and checl custom_application servers from Seamus Murray schedule.xml ## exported windows shedual task to run the script Application Primary Server PrimaryTest.sh ## bash script locally execute on the Application Primary servers from Seamus Murray Application DR Server ConfirmPriDown.sh ## remotely connects to Application Primary Server and runs PrimaryTest.sh from Seamus Murray MountStart.sh ## Starts the Application application Break-n-Mount.sh ## only on the index servers remotely connects to netapp controller, breaks the mirror, maps the lun and mounts it NetApp controller id_rsa.pub ## ssh public key of SnapMirror-user on the 2 custom_application index DR servers Diagrams Script contents................. ssh-fail.ps1 #ssh-fail.ps1 #powershell script to login and check custom_application servers from Seamus Murray Param( [string]$cluster ) Import-Module SSH-SSERVERions #Start-Transcript -path C:\\ssh-fail2\\logs\\transcript.txt -append ##set dubug level #$debug=1 #if ( $debug -eq 1 ) { # # Debug log file # $deboutfile=$scriptdir+\"\\outputlog.\"+$siteid+\".log\" # start-transcript -path $deboutfile -force # # Shows a trace of each line being run with variables as variables # set-psdebug -trace 1 # #} if ( $cluster -eq 'Indexer' -Or $cluster -eq 'DataBase' -Or $cluster -eq 'FrontEndA' -Or $cluster -eq 'FrontEndB') { } else{ Write-Host \"you must specify the cluster to test using an argument\" break } ## IP Function Hostname Role Switch ($cluster) { FrontEndA { $server_P1='10.10.10.51' #a Application FrontEnd SERVERFW1TS QLD-APP-5 $server_P2='10.10.10.52' #a Application FrontEnd SERVERFW2TS QLD-APP-6 $server_VIP='10.10.10.53'#a Application FrontEnd VIPA $server_DR='10.10.11.32' #a Application FrontEnd SERVERFW3TS SYD-APP-3 } FrontEndB{ $server_P1='10.10.10.55' #b Application FrontEnd SERVERFW3TS QLD-APP-7 $server_P2='10.10.10.56' #b Application FrontEnd SERVERFW4TS QLD-APP-8 $server_VIP='10.10.10.57'#b Application FrontEnd VIPB $server_DR='10.10.11.33' #b Application FrontEnd SERVERFW2TS SYD-APP-4 } Indexer{ $server_P1='10.10.10.77' #c Application Indexer SERVERIX1TS QLD-APP-1 $server_P2='10.10.10.78' #c Application Indexer SERVERIX2TS QLD-APP-2 $server_VIP='10.10.10.76'#c Application Indexer VIPI $server_DR='10.10.11.17' #c Application Indexer SERVERIX3TS SYD-APP-1 } DataBase{ $server_P1='10.10.10.85' #d Application DataBase SERVERHD1TS QLD-APP-3 $server_P2='10.10.10.86' #d Application DataBase SERVERHD2TS QLD-APP-4 $server_VIP='10.10.10.84'#d Application DataBase VIPD $server_DR='10.10.11.18' #d Application DataBase SERVERHD3TS SYD-APP-2 } } ##set log file to local directory eg..2012-01-1_0000_10.10.10.51_custom_applicationfail $Logfile = \"c:\\ssh-fail2\\logs\\$(get-date -uformat %Y-%m-%d_%H%M)\"+\"_$server_P1\"+\"_custom_applicationfail.log\" #Write_Host $Logfile #LogWrite $env:PSModulePath Function LogWrite { Param ([string]$logstring) Add-content $Logfile -value $logstring } $stamped = \"$(Get-Date)\" + \" starting script \" LogWrite $stamped #Remote Scripts executed on the linux servers but called from this script #You must specify the argument \"Up\" case sensitive for this test to succeed #If you want to simulate this test failing just change the argument $ApplicationPrimaryTest='/home/failover-user/custom_applicationfailover0.1/primary-test.sh Up' #Specifiy which server the DR should test by assigning a single argument....P1 P2 or VIP #the Various IPs are stored both locally in this file and in ApplicationConfirmPriDown.sh on the respective DR servers #If you want to simulate this test failing just change the argument to something else $ApplicationConfirmPriDown='/home/failover-user/custom_applicationfailover0.1/primary-confirm-fail.sh VIP' #this command needs to execute the start up script via sudo this either requires a tty which \"SSH-SSERVERions\" doesn't provide or..editing sudo to disable the requiretty in /etc/sudoers #This script varies slightly between the Application FrontEnds and the Application Indexers #On the Application indexers the NetApp mirrored lun's need to be broken and mounted this is handled by the.. #Break-n-Mount.sh script called from within the ApplicationMountStart.sh executed from the DR servers $ApplicationMountStart='/home/failover-user/custom_applicationfailover0.1/initiate-dr.sh Start' #called from within $ApplicationMountStart on the Indexer DR servers #$Break-n-Mount='/home/failover-user/custom_applicationfailover0.1/break-snap-mirror.sh RESYNC' while(\"forever\") { New-SshSSERVERion -ComputerName $server_P1 -Username 'failover-user' -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null New-SshSSERVERion -ComputerName $server_DR -Username 'failover-user' -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null try { #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 1st loop\" $stamped = \"$(Get-Date)\" + \" Testing ApplicationPrimaryTest on $server_P1 1st loop\" LogWrite $stamped $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet } catch [Exception] { $CmdOutput1 = \"SSH_SSERVERION_FAILED\" #Write_Host\"ERROR: $CmdOutput1 during 1st loop\" -foregroundcolor white -backgroundcolor red $stamped = \"$(Get-Date)\" + \" ERROR: $CmdOutput1 during 1st loop\" LogWrite $stamped } #Check Primary Server for status if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { #Write_Host \"ERROR: Primary Failure Detected\" #Write_Host \"Waiting 10 seconds before retrying\" $stamped = \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up Waiting 10 seconds before retrying\" LogWrite $stamped sleep 10 try { #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 2nd loop\" $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet } catch [Exception] { $CmdOutput1 = \"SSH_SSERVERION_FAILED\" #Write_Host\"$CmdOutput1 during 2nd loop\" -foregroundcolor white -backgroundcolor red $stamped = \"$(Get-Date)\" + \" $CmdOutput1 during 2nd loop\" LogWrite $stamped } #Check Primary Server for status after a previous failure if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { #Write_Host \"ERROR: Primary Failure Detected 2 times\" $stamped = \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up after 2 tries\" LogWrite $stamped try { $CmdOutput2 = Invoke-SshCommand -ComputerName $server3 -Command $ApplicationPrimaryTest -Quiet } catch [Exception] { #Write_Host \"ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\" $stamped = \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\" LogWrite $stamped } #If this host fails 2 time to determine if Primary_App_Is_Up, then ask DR server to also run the check if ( $CmdOutput2 -ne 'Primary_App_Is_Up' ) { #Write_Host \"ERROR: DR Server $server3 is also reporting Primary Failure....... Need to Initiate DR\" $stamped = \"$(Get-Date)\" + \" ERROR: DR Server is also reporting Primary Failure....... Need to Initiate DR\" LogWrite $stamped try { $CmdOutput3 = Invoke-SshCommand -ComputerName $server_DR -Command $ApplicationMountStart -Quiet } catch [Exception] { $CmdOutput3 = \"SSH_SSERVERION_FAILED\" #Write_Host \"ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\" $stamped = \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\" LogWrite $stamped } if ( $CmdOutput3 -ne 'App_Started' ) { #Write_Host \"ERROR: DR server $server_DR Failed to start the App\" $stamped = \"$(Get-Date)\" + \" ERROR: DR server $server_DR Failed to start the App\" LogWrite $stamped } else { #Write_Host \"DR server $server_DR has started the App\" $stamped = \"$(Get-Date)\" + \" DR server $server_DR has started the App\" LogWrite $stamped $stamped = \"$(Get-Date)\" + \" Nothing else to do...Failover script self terminating\" LogWrite $stamped break } } else { #Write_Host \"DR server $server_DR is reporting Primary $server_P1 is OK: Nothing To Do\" #Write_Host \"Assuming the link between me and the Primary server has failed\" $stamped = \"$(Get-Date)\" + \" Assuming the link between me and the Primary server has failed\" LogWrite $stamped } } else { #Write_Host \"Primary server $server_P1 is OK: Nothing To Do 2nd test\" $stamped = \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do 2nd test\" LogWrite $stamped } } else { #Write_Host \"Primary server $server_P1 is OK: Nothing To Do 1st test\" $stamped = \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do 1st test\" LogWrite $stamped } sleep 5 } Remove-SshSSERVERion -RemoveAll #Stop-Transcript schedule.xml #schedule.xml #exported windows shedual task to run the script <?xml version=\"1.0\" encoding=\"UTF-16\"?> <Task version=\"1.2\" xmlns=\"http://schemas.microsoft.com/windows/2004/02/mit/task\"> <RegistrationInfo> <Date>2012-01-1T12:30:10</Date> <Author>ABCDEFG123\\seamus</Author> </RegistrationInfo> <Triggers> <RegistrationTrigger> <Repetition> <Interval>PT15M</Interval> <StopAtDurationEnd>false</StopAtDurationEnd> </Repetition> <ExecutionTimeLimit>PT1H</ExecutionTimeLimit> <Enabled>true</Enabled> </RegistrationTrigger> </Triggers> <Principals> <Principal id=\"Author\"> <UserId>ABCDEFG123\\Administrator</UserId> <LogonType>Password</LogonType> <RunLevel>HighestAvailable</RunLevel> </Principal> </Principals> <Settings> <MultipleInstancesPolicy>StopExisting</MultipleInstancesPolicy> <DisallowStartIfOnBatteries>false</DisallowStartIfOnBatteries> <StopIfGoingOnBatteries>true</StopIfGoingOnBatteries> <AllowHardTerminate>true</AllowHardTerminate> <StartWhenAvailable>true</StartWhenAvailable> <RunOnlyIfNetworkAvailable>false</RunOnlyIfNetworkAvailable> <IdleSettings> <StopOnIdleEnd>true</StopOnIdleEnd> <RestartOnIdle>false</RestartOnIdle> </IdleSettings> <AllowStartOnDemand>true</AllowStartOnDemand> <Enabled>true</Enabled> <Hidden>false</Hidden> <RunOnlyIfIdle>false</RunOnlyIfIdle> <WakeToRun>false</WakeToRun> <ExecutionTimeLimit>PT1H</ExecutionTimeLimit> <Priority>7</Priority> </Settings> <Actions Context=\"Author\"> <Exec> <Command>C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe</Command> <Arguments>-File \"C:\\ssh-fail2\\slc-tie-breaker.ps1\" FrontEndA -ExecutionPolicy RemoteSigned -noprofile -noninteractive</Arguments> <WorkingDirectory>C:\\ssh-fail2\\</WorkingDirectory> </Exec> </Actions> </Task> PrimaryTest.sh...bash script locally executed on the Application Primary servers #!/bin/bash if [ \"$1\" = \"Up\" ]; then Is_App_Up=\"Primary_App_Is_Up\" else Is_App_Up=\"Primary_App_Is_Down\" fi echo $Is_App_Up echo `date` >> /tmp/log ConfirmPriDown.sh....bash script remotely connects to Application Primary Server and runs PrimaryTest.sh #!/bin/bash VIP=10.10.10.53 P1=10.10.10.51 P2=10.10.10.52 if [[ \"$1\" = \"VIP\" ]]; then TEST=\"$VIP\" elif [[ \"$1\" = \"P1\" ]]; then TEST=\"$P1\" elif [[ \"$1\" = \"P2\" ]]; then TEST=\"$P2\" else echo \"Usage: You must define which node to test {VIP,P1,P2}\" exit 0 fi if [[ `ssh $TEST -q -C /home/failover-user/custom_applicationfailover0.1/primary-test.sh Up` = \"Primary_App_Is_Up\" ]] ; then echo Primary_App_Is_Up else echo test-failed fi #!/bin/bash echo >> /home/failover-user/custom_applicationfailover0.1/startlog echo `date` start >> /home/failover-user/custom_applicationfailover0.1/startlog if [ \"$1\" = \"Start\" ] ; then # Start_App=\"App_Started\" echo `date` \" \" $0\" \" $1 >> /home/failover-user/custom_applicationfailover0.1/startlog /usr/bin/sudo /etc/init.d/custom_application-heavy restart &> /home/failover-user/custom_applicationfailover0.1/startlog echo \"sudo /etc/init.d/custom_application-heavy restart\" >> /home/failover-user/custom_applicationfailover0.1/startlog if [[ $? = \"0\" ]] then Start_App=\"App_Started\" echo $Start_App else echo $0 Failed to restart app echo $Start_App fi else echo `date` Usage $0 Start >> /home/failover-user/custom_applicationfailover0.1/startlog fi echo `date` finsihed >> /home/failover-user/custom_applicationfailover0.1/startlog #/usr/bin/sudo /etc/init.d/custom_application-heavy restart","title":"Homemade cluster"},{"location":"linux/Clustering/homemade_cluster/#operating-systems","text":"Application servers are RHEL 5.9 Steward server is windows 2008","title":"Operating Systems"},{"location":"linux/Clustering/homemade_cluster/#the-tie-breaker-server-requires","text":"powershell dotnet user account scheduled task for each monitored cluster powershell script ssh wrapper script (powershell to dotnet) ssh.net library log directory public ssh key of the primary and the DR servers (to enable paswordlSERVER login)","title":"The tie-breaker server requires..."},{"location":"linux/Clustering/homemade_cluster/#files","text":"","title":"Files"},{"location":"linux/Clustering/homemade_cluster/#windows-steward-server","text":"id_rsa ## ssh private key to allow loginto linux servers id_rsa.pub ## ssh public key to allow loginto linux servers Renci.SshNet.dll ## SSH.net library from http://sshnet.codeplex.com/ SSH-SSERVERions.psd1 ## powershell wrapper for SSH.net from http://www.powershelladmin.com/wiki/SSH_from_PowerShell_using_the_SSH.NET_library SSH-SSERVERions.psm1 ## licence file ssh-fail.ps1 ## powershell script to login and checl custom_application servers from Seamus Murray schedule.xml ## exported windows shedual task to run the script","title":"Windows Steward server"},{"location":"linux/Clustering/homemade_cluster/#application-primary-server","text":"PrimaryTest.sh ## bash script locally execute on the Application Primary servers from Seamus Murray","title":"Application Primary Server"},{"location":"linux/Clustering/homemade_cluster/#application-dr-server","text":"ConfirmPriDown.sh ## remotely connects to Application Primary Server and runs PrimaryTest.sh from Seamus Murray MountStart.sh ## Starts the Application application Break-n-Mount.sh ## only on the index servers remotely connects to netapp controller, breaks the mirror, maps the lun and mounts it","title":"Application DR Server"},{"location":"linux/Clustering/homemade_cluster/#netapp-controller","text":"id_rsa.pub ## ssh public key of SnapMirror-user on the 2 custom_application index DR servers","title":"NetApp controller"},{"location":"linux/Clustering/homemade_cluster/#diagrams","text":"","title":"Diagrams"},{"location":"linux/Clustering/homemade_cluster/#script-contents","text":"","title":"Script contents................."},{"location":"linux/Clustering/homemade_cluster/#ssh-failps1","text":"#ssh-fail.ps1 #powershell script to login and check custom_application servers from Seamus Murray Param( [string]$cluster ) Import-Module SSH-SSERVERions #Start-Transcript -path C:\\ssh-fail2\\logs\\transcript.txt -append ##set dubug level #$debug=1 #if ( $debug -eq 1 ) { # # Debug log file # $deboutfile=$scriptdir+\"\\outputlog.\"+$siteid+\".log\" # start-transcript -path $deboutfile -force # # Shows a trace of each line being run with variables as variables # set-psdebug -trace 1 # #} if ( $cluster -eq 'Indexer' -Or $cluster -eq 'DataBase' -Or $cluster -eq 'FrontEndA' -Or $cluster -eq 'FrontEndB') { } else{ Write-Host \"you must specify the cluster to test using an argument\" break } ## IP Function Hostname Role Switch ($cluster) { FrontEndA { $server_P1='10.10.10.51' #a Application FrontEnd SERVERFW1TS QLD-APP-5 $server_P2='10.10.10.52' #a Application FrontEnd SERVERFW2TS QLD-APP-6 $server_VIP='10.10.10.53'#a Application FrontEnd VIPA $server_DR='10.10.11.32' #a Application FrontEnd SERVERFW3TS SYD-APP-3 } FrontEndB{ $server_P1='10.10.10.55' #b Application FrontEnd SERVERFW3TS QLD-APP-7 $server_P2='10.10.10.56' #b Application FrontEnd SERVERFW4TS QLD-APP-8 $server_VIP='10.10.10.57'#b Application FrontEnd VIPB $server_DR='10.10.11.33' #b Application FrontEnd SERVERFW2TS SYD-APP-4 } Indexer{ $server_P1='10.10.10.77' #c Application Indexer SERVERIX1TS QLD-APP-1 $server_P2='10.10.10.78' #c Application Indexer SERVERIX2TS QLD-APP-2 $server_VIP='10.10.10.76'#c Application Indexer VIPI $server_DR='10.10.11.17' #c Application Indexer SERVERIX3TS SYD-APP-1 } DataBase{ $server_P1='10.10.10.85' #d Application DataBase SERVERHD1TS QLD-APP-3 $server_P2='10.10.10.86' #d Application DataBase SERVERHD2TS QLD-APP-4 $server_VIP='10.10.10.84'#d Application DataBase VIPD $server_DR='10.10.11.18' #d Application DataBase SERVERHD3TS SYD-APP-2 } } ##set log file to local directory eg..2012-01-1_0000_10.10.10.51_custom_applicationfail $Logfile = \"c:\\ssh-fail2\\logs\\$(get-date -uformat %Y-%m-%d_%H%M)\"+\"_$server_P1\"+\"_custom_applicationfail.log\" #Write_Host $Logfile #LogWrite $env:PSModulePath Function LogWrite { Param ([string]$logstring) Add-content $Logfile -value $logstring } $stamped = \"$(Get-Date)\" + \" starting script \" LogWrite $stamped #Remote Scripts executed on the linux servers but called from this script #You must specify the argument \"Up\" case sensitive for this test to succeed #If you want to simulate this test failing just change the argument $ApplicationPrimaryTest='/home/failover-user/custom_applicationfailover0.1/primary-test.sh Up' #Specifiy which server the DR should test by assigning a single argument....P1 P2 or VIP #the Various IPs are stored both locally in this file and in ApplicationConfirmPriDown.sh on the respective DR servers #If you want to simulate this test failing just change the argument to something else $ApplicationConfirmPriDown='/home/failover-user/custom_applicationfailover0.1/primary-confirm-fail.sh VIP' #this command needs to execute the start up script via sudo this either requires a tty which \"SSH-SSERVERions\" doesn't provide or..editing sudo to disable the requiretty in /etc/sudoers #This script varies slightly between the Application FrontEnds and the Application Indexers #On the Application indexers the NetApp mirrored lun's need to be broken and mounted this is handled by the.. #Break-n-Mount.sh script called from within the ApplicationMountStart.sh executed from the DR servers $ApplicationMountStart='/home/failover-user/custom_applicationfailover0.1/initiate-dr.sh Start' #called from within $ApplicationMountStart on the Indexer DR servers #$Break-n-Mount='/home/failover-user/custom_applicationfailover0.1/break-snap-mirror.sh RESYNC' while(\"forever\") { New-SshSSERVERion -ComputerName $server_P1 -Username 'failover-user' -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null New-SshSSERVERion -ComputerName $server_DR -Username 'failover-user' -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null try { #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 1st loop\" $stamped = \"$(Get-Date)\" + \" Testing ApplicationPrimaryTest on $server_P1 1st loop\" LogWrite $stamped $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet } catch [Exception] { $CmdOutput1 = \"SSH_SSERVERION_FAILED\" #Write_Host\"ERROR: $CmdOutput1 during 1st loop\" -foregroundcolor white -backgroundcolor red $stamped = \"$(Get-Date)\" + \" ERROR: $CmdOutput1 during 1st loop\" LogWrite $stamped } #Check Primary Server for status if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { #Write_Host \"ERROR: Primary Failure Detected\" #Write_Host \"Waiting 10 seconds before retrying\" $stamped = \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up Waiting 10 seconds before retrying\" LogWrite $stamped sleep 10 try { #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 2nd loop\" $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet } catch [Exception] { $CmdOutput1 = \"SSH_SSERVERION_FAILED\" #Write_Host\"$CmdOutput1 during 2nd loop\" -foregroundcolor white -backgroundcolor red $stamped = \"$(Get-Date)\" + \" $CmdOutput1 during 2nd loop\" LogWrite $stamped } #Check Primary Server for status after a previous failure if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { #Write_Host \"ERROR: Primary Failure Detected 2 times\" $stamped = \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up after 2 tries\" LogWrite $stamped try { $CmdOutput2 = Invoke-SshCommand -ComputerName $server3 -Command $ApplicationPrimaryTest -Quiet } catch [Exception] { #Write_Host \"ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\" $stamped = \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\" LogWrite $stamped } #If this host fails 2 time to determine if Primary_App_Is_Up, then ask DR server to also run the check if ( $CmdOutput2 -ne 'Primary_App_Is_Up' ) { #Write_Host \"ERROR: DR Server $server3 is also reporting Primary Failure....... Need to Initiate DR\" $stamped = \"$(Get-Date)\" + \" ERROR: DR Server is also reporting Primary Failure....... Need to Initiate DR\" LogWrite $stamped try { $CmdOutput3 = Invoke-SshCommand -ComputerName $server_DR -Command $ApplicationMountStart -Quiet } catch [Exception] { $CmdOutput3 = \"SSH_SSERVERION_FAILED\" #Write_Host \"ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\" $stamped = \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\" LogWrite $stamped } if ( $CmdOutput3 -ne 'App_Started' ) { #Write_Host \"ERROR: DR server $server_DR Failed to start the App\" $stamped = \"$(Get-Date)\" + \" ERROR: DR server $server_DR Failed to start the App\" LogWrite $stamped } else { #Write_Host \"DR server $server_DR has started the App\" $stamped = \"$(Get-Date)\" + \" DR server $server_DR has started the App\" LogWrite $stamped $stamped = \"$(Get-Date)\" + \" Nothing else to do...Failover script self terminating\" LogWrite $stamped break } } else { #Write_Host \"DR server $server_DR is reporting Primary $server_P1 is OK: Nothing To Do\" #Write_Host \"Assuming the link between me and the Primary server has failed\" $stamped = \"$(Get-Date)\" + \" Assuming the link between me and the Primary server has failed\" LogWrite $stamped } } else { #Write_Host \"Primary server $server_P1 is OK: Nothing To Do 2nd test\" $stamped = \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do 2nd test\" LogWrite $stamped } } else { #Write_Host \"Primary server $server_P1 is OK: Nothing To Do 1st test\" $stamped = \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do 1st test\" LogWrite $stamped } sleep 5 } Remove-SshSSERVERion -RemoveAll #Stop-Transcript","title":"ssh-fail.ps1"},{"location":"linux/Clustering/homemade_cluster/#schedulexml","text":"#schedule.xml #exported windows shedual task to run the script <?xml version=\"1.0\" encoding=\"UTF-16\"?> <Task version=\"1.2\" xmlns=\"http://schemas.microsoft.com/windows/2004/02/mit/task\"> <RegistrationInfo> <Date>2012-01-1T12:30:10</Date> <Author>ABCDEFG123\\seamus</Author> </RegistrationInfo> <Triggers> <RegistrationTrigger> <Repetition> <Interval>PT15M</Interval> <StopAtDurationEnd>false</StopAtDurationEnd> </Repetition> <ExecutionTimeLimit>PT1H</ExecutionTimeLimit> <Enabled>true</Enabled> </RegistrationTrigger> </Triggers> <Principals> <Principal id=\"Author\"> <UserId>ABCDEFG123\\Administrator</UserId> <LogonType>Password</LogonType> <RunLevel>HighestAvailable</RunLevel> </Principal> </Principals> <Settings> <MultipleInstancesPolicy>StopExisting</MultipleInstancesPolicy> <DisallowStartIfOnBatteries>false</DisallowStartIfOnBatteries> <StopIfGoingOnBatteries>true</StopIfGoingOnBatteries> <AllowHardTerminate>true</AllowHardTerminate> <StartWhenAvailable>true</StartWhenAvailable> <RunOnlyIfNetworkAvailable>false</RunOnlyIfNetworkAvailable> <IdleSettings> <StopOnIdleEnd>true</StopOnIdleEnd> <RestartOnIdle>false</RestartOnIdle> </IdleSettings> <AllowStartOnDemand>true</AllowStartOnDemand> <Enabled>true</Enabled> <Hidden>false</Hidden> <RunOnlyIfIdle>false</RunOnlyIfIdle> <WakeToRun>false</WakeToRun> <ExecutionTimeLimit>PT1H</ExecutionTimeLimit> <Priority>7</Priority> </Settings> <Actions Context=\"Author\"> <Exec> <Command>C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe</Command> <Arguments>-File \"C:\\ssh-fail2\\slc-tie-breaker.ps1\" FrontEndA -ExecutionPolicy RemoteSigned -noprofile -noninteractive</Arguments> <WorkingDirectory>C:\\ssh-fail2\\</WorkingDirectory> </Exec> </Actions> </Task>","title":"schedule.xml"},{"location":"linux/Clustering/homemade_cluster/#primarytestshbash-script-locally-executed-on-the-application-primary-servers","text":"#!/bin/bash if [ \"$1\" = \"Up\" ]; then Is_App_Up=\"Primary_App_Is_Up\" else Is_App_Up=\"Primary_App_Is_Down\" fi echo $Is_App_Up echo `date` >> /tmp/log","title":"PrimaryTest.sh...bash script locally executed on the Application Primary servers"},{"location":"linux/Clustering/homemade_cluster/#confirmpridownshbash-script-remotely-connects-to-application-primary-server-and-runs-primarytestsh","text":"#!/bin/bash VIP=10.10.10.53 P1=10.10.10.51 P2=10.10.10.52 if [[ \"$1\" = \"VIP\" ]]; then TEST=\"$VIP\" elif [[ \"$1\" = \"P1\" ]]; then TEST=\"$P1\" elif [[ \"$1\" = \"P2\" ]]; then TEST=\"$P2\" else echo \"Usage: You must define which node to test {VIP,P1,P2}\" exit 0 fi if [[ `ssh $TEST -q -C /home/failover-user/custom_applicationfailover0.1/primary-test.sh Up` = \"Primary_App_Is_Up\" ]] ; then echo Primary_App_Is_Up else echo test-failed fi #!/bin/bash echo >> /home/failover-user/custom_applicationfailover0.1/startlog echo `date` start >> /home/failover-user/custom_applicationfailover0.1/startlog if [ \"$1\" = \"Start\" ] ; then # Start_App=\"App_Started\" echo `date` \" \" $0\" \" $1 >> /home/failover-user/custom_applicationfailover0.1/startlog /usr/bin/sudo /etc/init.d/custom_application-heavy restart &> /home/failover-user/custom_applicationfailover0.1/startlog echo \"sudo /etc/init.d/custom_application-heavy restart\" >> /home/failover-user/custom_applicationfailover0.1/startlog if [[ $? = \"0\" ]] then Start_App=\"App_Started\" echo $Start_App else echo $0 Failed to restart app echo $Start_App fi else echo `date` Usage $0 Start >> /home/failover-user/custom_applicationfailover0.1/startlog fi echo `date` finsihed >> /home/failover-user/custom_applicationfailover0.1/startlog #/usr/bin/sudo /etc/init.d/custom_application-heavy restart","title":"ConfirmPriDown.sh....bash script remotely connects to Application Primary Server and runs PrimaryTest.sh"},{"location":"linux/Clustering/simple.cluster.conf/","text":"<?xml version=\"1.0\"?> <cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"> <fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/> <clusternodes> <clusternode name=\"hostname1.example.private\" nodeid=\"1\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname1\"/> </method> </fence> </clusternode> <clusternode name=\"hostname2.example.private\" nodeid=\"2\" votes=\"1\"> <fence> <method name=\"1\"> <device name=\"ipmi-hostname2\"/> </method> </fence> </clusternode> </clusternodes> <cman expected_votes=\"1\" two_node=\"1\"> <multicast addr=\"239.192.0.1\"/> </cman> <fencedevices> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"ucsipmi_pmpricen\" name=\"ipmi-hostname1\" passwd=\"password\" delay=\"30\"/> <fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"ucsipmi_pmpricen\" name=\"ipmi-hostname2\" passwd=\"password\"/> </fencedevices> <rm> <failoverdomains> <failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"> <failoverdomainnode name=\"hostname1.example.private\" priority=\"1\"/> <failoverdomainnode name=\"hostname2.example.private\" priority=\"2\"/> </failoverdomain> </failoverdomains> <resources> </resources> <service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"> </service> </rm> </cluster>","title":"Simple.cluster.conf"},{"location":"linux/Commands/email_on_linux/","text":"change mail relay for postfix vi /etc/postfix/transport postmap /etc/postfix/transport postfix delete all mail from queue postsuper -d ALL postqueue -p send a text file as inline mailx -s \"test from new server\" seamus@example.com ~r{filename} .","title":"change mail relay for postfix"},{"location":"linux/Commands/email_on_linux/#change-mail-relay-for-postfix","text":"vi /etc/postfix/transport postmap /etc/postfix/transport postfix","title":"change mail relay for postfix"},{"location":"linux/Commands/email_on_linux/#delete-all-mail-from-queue","text":"postsuper -d ALL postqueue -p","title":"delete all mail from queue"},{"location":"linux/Commands/email_on_linux/#send-a-text-file-as-inline","text":"mailx -s \"test from new server\" seamus@example.com ~r{filename} .","title":"send a text file as inline"},{"location":"linux/Commands/hot_add_memory/","text":"How to enable Hot-add Memory in Linux RHEL 5 ? What if you have Hot-added memory in your RHEL 5 server but it's still displaying Last memory configuration ? Step : Check if the acpi modules are loaded lsmod | grep acpi You should see both ... acpiphp 43673 0 acpi_memhotplug 42199 0 If not loaded, modprobe acpiphp modprobe acpi_memhotplug Now increase your server memory with Hot-add memory module. This will be done by Hardware engineer directly on the server, i am not talking about on OS level. to determine which modules are offline grep offline /sys/devices/system/memory/memory*/state execute either of the following to set the modules to be online for offline_module in `grep offline /sys/devices/system/memory/memory*/state | sed s/\\:offline//g` do echo \"online\" > $offline_module done cat /sys/devices/system/memory/memory*/state Save it & exit For Ubuntu you have to try with another script as below, #!/bin/sh i=0 while [ $i -lt 4500 ] do if [ -f /sys/devices/system/memory/memory$i/state ] then if grep \"offline\" /sys/devices/system/memory/memory$i/state then echo 'online' > /sys/devices/system/memory/memory$i/state fi fi i=`expr $i + 1` done Give this script execution permission by, chmod 777 memory_online.sh Next execute it & check memory in either \"/proc/meminfo\" file or by below command #free Done.","title":"Hot add memory"},{"location":"linux/Commands/hot_add_memory/#check-if-the-acpi-modules-are-loaded","text":"lsmod | grep acpi","title":"Check if the acpi modules are loaded"},{"location":"linux/Commands/hot_add_memory/#you-should-see-both","text":"acpiphp 43673 0 acpi_memhotplug 42199 0 If not loaded, modprobe acpiphp modprobe acpi_memhotplug Now increase your server memory with Hot-add memory module. This will be done by Hardware engineer directly on the server, i am not talking about on OS level.","title":"You should see both ..."},{"location":"linux/Commands/hot_add_memory/#to-determine-which-modules-are-offline","text":"grep offline /sys/devices/system/memory/memory*/state","title":"to determine which modules are offline"},{"location":"linux/Commands/hot_add_memory/#execute-either-of-the-following-to-set-the-modules-to-be-online","text":"for offline_module in `grep offline /sys/devices/system/memory/memory*/state | sed s/\\:offline//g` do echo \"online\" > $offline_module done cat /sys/devices/system/memory/memory*/state Save it & exit For Ubuntu you have to try with another script as below, #!/bin/sh i=0 while [ $i -lt 4500 ] do if [ -f /sys/devices/system/memory/memory$i/state ] then if grep \"offline\" /sys/devices/system/memory/memory$i/state then echo 'online' > /sys/devices/system/memory/memory$i/state fi fi i=`expr $i + 1` done Give this script execution permission by,","title":"execute either of the following to set the modules to be online"},{"location":"linux/Commands/hot_add_memory/#chmod-777-memory_onlinesh","text":"Next execute it & check memory in either \"/proc/meminfo\" file or by below command #free Done.","title":"chmod 777 memory_online.sh"},{"location":"linux/Core_Utils/awk/","text":"using awk to append numbers to the start of a list awk '{print NR \" \" $0}' list > list-numbered using awk to print a range of lines from a list awk '$1 == 2 , $1 == 7' list display non blank and non commented lines in file filter blank and comment lines from file cat /etc/ssh/sshd_config | grep -v \\# | sed '/^$/d' cat /etc/ssh/sshd_config | grep -v \\# | awk 'NF' grep -v ^# /etc/samba/smb.conf | grep -v ^\\; | awk 'NF' Use awk to auto generate repeating block of code eg.. creating a really long if else or case statement I was creating a keyboard mapper program and instead of typing loads of else if statements manually... I create a paired list of the input/match and the output/action and used the awk script below to generate the else if statements. A 14 B 2F C 26 D 24 E 22 F 2C G 2D H 35 I 3A J 34 K 3C L 44 M 36 N 37 O 42 P 4A Q 12 R 2A S 1C T 2B U 32 V 2E W 1A X 1E Y 33 Z 16 use awk to generate #the spaces in the following line are required cat inputed_list | awk '{ print \" else if ( c = '\"'\"'\"$1\"'\"'\"' ){\\n key_value = \"$2\";\\n single_upper(c);\\n }\"}' the output c code else if ( c = 'T' ){ key_value = 2B; single_upper(c); } else if ( c = 'U' ){ key_value = 32; single_upper(c); } else if ( c = 'V' ){ key_value = 2E; single_upper(c); } else if ( c = 'W' ){ key_value = 1A; single_upper(c); } else if ( c = 'X' ){ key_value = 1E; single_upper(c); } else if ( c = 'Y' ){ key_value = 33; single_upper(c); } else if ( c = 'Z' ){ key_value = 16; single_upper(c); Simple script using awk to dump a list of the filesystems that are over 70% full to a file in /tmp #!/bin/bash #seamus Dec 2004 out=/tmp/diskusage name=`uname -n` date=`date` echo > $out echo $name $date >> $out df -k | awk '{if( $5 > 70 ) print $0 }' | awk '!/Filesystem/' >> $out cat $out use sed to append spaces to start of each line for markdown input cat {file} | sed s/^/\\ \\ \\ \\ /g script to replace the spaces in filenames with underscrores #!/bin/bash # this script was created to replace the spaces in filenames with underscrores #seamus Dec 2004 #if [ $1 != do ] || [ $1 != check ] #then # echo USAGE: do or check # exit # #fi case \"$1\" in do) for i in * do mv \"$i\" `echo -n \"$i\" | sed 's/\\ /_/g'` done ;; check) for i in * do echo;echo -n \"rename\" echo;echo \"$i\" echo to....... echo -n \"$i\" | sed 's/\\ /_/g' echo done ;; *) echo USAGE: do or check esac","title":"Awk"},{"location":"linux/Core_Utils/awk/#using-awk-to-append-numbers-to-the-start-of-a-list","text":"awk '{print NR \" \" $0}' list > list-numbered","title":"using awk to append numbers to the start of a list"},{"location":"linux/Core_Utils/awk/#using-awk-to-print-a-range-of-lines-from-a-list","text":"awk '$1 == 2 , $1 == 7' list","title":"using awk to print a range of lines from a list"},{"location":"linux/Core_Utils/awk/#display-non-blank-and-non-commented-lines-in-file","text":"","title":"display non blank and non commented lines in file"},{"location":"linux/Core_Utils/awk/#filter-blank-and-comment-lines-from-file","text":"cat /etc/ssh/sshd_config | grep -v \\# | sed '/^$/d' cat /etc/ssh/sshd_config | grep -v \\# | awk 'NF' grep -v ^# /etc/samba/smb.conf | grep -v ^\\; | awk 'NF' Use awk to auto generate repeating block of code eg.. creating a really long if else or case statement","title":"filter blank and comment lines from file"},{"location":"linux/Core_Utils/awk/#i-was-creating-a-keyboard-mapper-program-and-instead-of-typing-loads-of-else-if-statements-manually-i-create-a-paired-list-of-the-inputmatch-and-the-outputaction-and-used-the-awk-script-below-to-generate-the-else-if-statements","text":"A 14 B 2F C 26 D 24 E 22 F 2C G 2D H 35 I 3A J 34 K 3C L 44 M 36 N 37 O 42 P 4A Q 12 R 2A S 1C T 2B U 32 V 2E W 1A X 1E Y 33 Z 16","title":"I was creating a keyboard mapper program and instead of typing loads of else if statements manually... I create a paired list of the input/match and the output/action and used the awk script below to generate the else if statements."},{"location":"linux/Core_Utils/awk/#use-awk-to-generate","text":"#the spaces in the following line are required cat inputed_list | awk '{ print \" else if ( c = '\"'\"'\"$1\"'\"'\"' ){\\n key_value = \"$2\";\\n single_upper(c);\\n }\"}'","title":"use awk to generate"},{"location":"linux/Core_Utils/awk/#the-output-c-code","text":"else if ( c = 'T' ){ key_value = 2B; single_upper(c); } else if ( c = 'U' ){ key_value = 32; single_upper(c); } else if ( c = 'V' ){ key_value = 2E; single_upper(c); } else if ( c = 'W' ){ key_value = 1A; single_upper(c); } else if ( c = 'X' ){ key_value = 1E; single_upper(c); } else if ( c = 'Y' ){ key_value = 33; single_upper(c); } else if ( c = 'Z' ){ key_value = 16; single_upper(c);","title":"the output c code"},{"location":"linux/Core_Utils/awk/#simple-script-using-awk-to-dump-a-list-of-the-filesystems-that-are-over-70-full-to-a-file-in-tmp","text":"#!/bin/bash #seamus Dec 2004 out=/tmp/diskusage name=`uname -n` date=`date` echo > $out echo $name $date >> $out df -k | awk '{if( $5 > 70 ) print $0 }' | awk '!/Filesystem/' >> $out cat $out","title":"Simple script using awk to dump a list of the filesystems that are over 70% full to a file in /tmp"},{"location":"linux/Core_Utils/awk/#use-sed-to-append-spaces-to-start-of-each-line-for-markdown-input","text":"cat {file} | sed s/^/\\ \\ \\ \\ /g","title":"use sed to append spaces to start of each line for markdown input"},{"location":"linux/Core_Utils/awk/#script-to-replace-the-spaces-in-filenames-with-underscrores","text":"#!/bin/bash # this script was created to replace the spaces in filenames with underscrores #seamus Dec 2004 #if [ $1 != do ] || [ $1 != check ] #then # echo USAGE: do or check # exit # #fi case \"$1\" in do) for i in * do mv \"$i\" `echo -n \"$i\" | sed 's/\\ /_/g'` done ;; check) for i in * do echo;echo -n \"rename\" echo;echo \"$i\" echo to....... echo -n \"$i\" | sed 's/\\ /_/g' echo done ;; *) echo USAGE: do or check esac","title":"script to replace the spaces in filenames with underscrores"},{"location":"linux/Core_Utils/bash_shortcuts/","text":"Handy shortcut when you mistype a command name, in the example below I mistype \"rm\" as \"em\" em /var/log/path/really-long-file-name.log em: command not found $ rm !:1 rm /var/log/path/really-long-file-name.log Repeating the last command, most of the time you would just use the arrow up key however not all keyboards have arrow keys $ echo \"Hello World!\" Hello World! $ !! echo \"Hello World!\" Hello World! Handy shortcut for when you forget to prepend a command with sudo $ apt-get install sshd E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied) E: Unable to lock the administration directory (/var/lib/dpkg/), are you root? $ sudo !! sudo apt-get install sshd [sudo] password for user: Reading package lists... Done Building dependency tree Reading state information... Done If you want to reuse the last argument from the previous command $ cat /etc/hosts 127.0.0.1 localhost 127.0.1.1 desktop $ vi !$ vi /etc/hosts Cleaning up the terminal Clear the screen $clear Ctrl + l Stop the standard out of a program from displaying in your terminal but keep the program executing. Ctrl + s Ctrl + q Bash tab completion works with wild cards $ rm *.txt <TAB>","title":"Bash shortcuts"},{"location":"linux/Core_Utils/rename/","text":"rename files using the perl rename script written by Larry Wall $ rename -nv 's/\\.sh//' * bash_argument_shift_until_loop.sh.md renamed as bash_argument_shift_until_loop.md","title":"Rename"},{"location":"linux/Core_Utils/spacecmd_breaking_up_large_groups_of_hosts_with_awk/","text":"using awk to append numbers to the start of a list and then use those numbers as a key to divide up list into smaller lists I know you dont need to add the numbers to the list for this task, however in my case I needed the list for other purposes therefore the numbers were handy. add line numbers to the file awk '{print NR \"\" $0}' dc1-dev-list-chomped > dc1-dev-list-chomped-numbered print lines between the 2 values awk '$1 == \"288\", $1 == \"299\"' dc1-dev-list-chomped-numbered spacecmd group_create package-upgrade-dc1-0599-0699 spacecmd group_addsystems package-upgrade-dc1-0299-0599 spacecmd group_listsystems package-upgrade-dc1-0299-0599 STARTNUM=900 LASTNUM=1218 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered > dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM spacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM package-upgrade-dc1-STARTNUM-LASTNUM for i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM | awk '{print $2}' ; do spacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM $i ; sleep 1; done STARTNUM=0000 LASTNUM=0099 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered > dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM.example.local spacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM.example.local package-upgrade-dc1-STARTNUM-LASTNUM.example.local for i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM | awk '{print $2}' ; do spacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local $i.example.local ; done spacecmd group_listsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local package-upgrade-dc1-1-299.example.local package_listinstalledsystems packageClient-6.1.101-87.x86_64","title":"Spacecmd breaking up large groups of hosts with awk"},{"location":"linux/Core_Utils/spacecmd_breaking_up_large_groups_of_hosts_with_awk/#using-awk-to-append-numbers-to-the-start-of-a-list-and-then-use-those-numbers-as-a-key-to-divide-up-list-into-smaller-lists","text":"I know you dont need to add the numbers to the list for this task, however in my case I needed the list for other purposes therefore the numbers were handy.","title":"using awk to append numbers to the start of a list  and then use those numbers as a key to divide up list into smaller lists"},{"location":"linux/Core_Utils/spacecmd_breaking_up_large_groups_of_hosts_with_awk/#add-line-numbers-to-the-file","text":"awk '{print NR \"\" $0}' dc1-dev-list-chomped > dc1-dev-list-chomped-numbered print lines between the 2 values awk '$1 == \"288\", $1 == \"299\"' dc1-dev-list-chomped-numbered spacecmd group_create package-upgrade-dc1-0599-0699 spacecmd group_addsystems package-upgrade-dc1-0299-0599 spacecmd group_listsystems package-upgrade-dc1-0299-0599 STARTNUM=900 LASTNUM=1218 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered > dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM spacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM package-upgrade-dc1-STARTNUM-LASTNUM for i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM | awk '{print $2}' ; do spacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM $i ; sleep 1; done STARTNUM=0000 LASTNUM=0099 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered > dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM.example.local spacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM.example.local package-upgrade-dc1-STARTNUM-LASTNUM.example.local for i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM | awk '{print $2}' ; do spacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local $i.example.local ; done spacecmd group_listsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local package-upgrade-dc1-1-299.example.local package_listinstalledsystems packageClient-6.1.101-87.x86_64","title":"add line numbers to the file"},{"location":"linux/Core_Utils/user_creation/","text":"groupadd -g 50001 group-name useradd -g 50001 -c \"user created by seamus\" -p'$6$8$%^FGRGH*GH' -s /bin/bash -u 50001 user-name ssh user-name@`hostname`","title":"User creation"},{"location":"linux/Core_Utils/vim_sudo/","text":"Handy trick for when you open a file in vim and do not have the permissions to save the file :w !sudo tee %","title":"Vim sudo"},{"location":"linux/Core_Utils/watch_and_timeout/","text":"Both the watch and timeout GNU utilities are very hands for monitoring and putting limits on scripts and programs using the watch command to repeatedly run a command #Run the df command every 5 seconds watch -n 5 df #run the df command every 5 seconds and highlight the differences watch -n 5 -d df Using the timeout command from coreutils to limit the length of time a process can run timeout 5 top The timeout command accepts as integer followed by an option unit of either s m h d seconds minutes hours days Using the at atd atrm commands to schedule once off jobs If you just want to run some simple command line tools you can enter the commands interactively. at now + 1 min warning: commands will be executed using /bin/sh at> echo seamus > test.txt at> ^d If you have a shell script with your commands you can simply use the \"-f\" option at -f ./job.sh now + 3 min Using echo to schedule a job, this is handy to call from another script echo \"echo seamus > test.txt\" | at now + 1 min List the jobs in the queue. atq Show the contents of a particular job. at -c <job number> Removing a job. atq 13 Sun Aug 16 22:12:00 2015 a guess 16 Sun Aug 16 22:12:00 2015 a guess 14 Sun Aug 16 22:12:00 2015 a guess 15 Sun Aug 16 22:12:00 2015 a guess atrm 14 atq 13 Sun Aug 16 22:12:00 2015 a guess 16 Sun Aug 16 22:12:00 2015 a guess 15 Sun Aug 16 22:12:00 2015 a guess Crontab layout # +---------------- minute (0 - 59) # | +------------- hour (0 - 23) # | | +---------- day of month (1 - 31) # | | | +------- month (1 - 12) # | | | | +---- day of week (0 - 6) (Sunday=0 or 7) # | | | | | * * * * * command to be executed A little script I wrote back in 2004 #!/bin/bash if [ $# -le 0 ] then echo \"USSAGE = count (number) (delay) (command)\" exit fi count=$1 while [ $count -ne 0 ] do \"$3\" && sleep $2 count=$(( $count -1 )) done","title":"Watch and timeout"},{"location":"linux/Core_Utils/watch_and_timeout/#using-the-watch-command-to-repeatedly-run-a-command","text":"#Run the df command every 5 seconds watch -n 5 df #run the df command every 5 seconds and highlight the differences watch -n 5 -d df","title":"using the watch command to repeatedly run a command"},{"location":"linux/Core_Utils/watch_and_timeout/#using-the-timeout-command-from-coreutils-to-limit-the-length-of-time-a-process-can-run","text":"timeout 5 top The timeout command accepts as integer followed by an option unit of either s m h d seconds minutes hours days","title":"Using the timeout command from coreutils to limit the length of time a process can run"},{"location":"linux/Core_Utils/watch_and_timeout/#using-the-at-atd-atrm-commands-to-schedule-once-off-jobs","text":"If you just want to run some simple command line tools you can enter the commands interactively. at now + 1 min warning: commands will be executed using /bin/sh at> echo seamus > test.txt at> ^d If you have a shell script with your commands you can simply use the \"-f\" option at -f ./job.sh now + 3 min Using echo to schedule a job, this is handy to call from another script echo \"echo seamus > test.txt\" | at now + 1 min List the jobs in the queue. atq Show the contents of a particular job. at -c <job number> Removing a job. atq 13 Sun Aug 16 22:12:00 2015 a guess 16 Sun Aug 16 22:12:00 2015 a guess 14 Sun Aug 16 22:12:00 2015 a guess 15 Sun Aug 16 22:12:00 2015 a guess atrm 14 atq 13 Sun Aug 16 22:12:00 2015 a guess 16 Sun Aug 16 22:12:00 2015 a guess 15 Sun Aug 16 22:12:00 2015 a guess","title":"Using the at atd atrm commands to schedule once off jobs"},{"location":"linux/Core_Utils/watch_and_timeout/#crontab-layout","text":"# +---------------- minute (0 - 59) # | +------------- hour (0 - 23) # | | +---------- day of month (1 - 31) # | | | +------- month (1 - 12) # | | | | +---- day of week (0 - 6) (Sunday=0 or 7) # | | | | | * * * * * command to be executed","title":"Crontab layout"},{"location":"linux/Core_Utils/watch_and_timeout/#a-little-script-i-wrote-back-in-2004","text":"#!/bin/bash if [ $# -le 0 ] then echo \"USSAGE = count (number) (delay) (command)\" exit fi count=$1 while [ $count -ne 0 ] do \"$3\" && sleep $2 count=$(( $count -1 )) done","title":"A little script I wrote back in 2004"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/","text":"LVM tips and Hints Resizing (extending) Logical Volumes and filesystems Live LogicalVolume and Filesystem extension in a single step (-r) Determine the free space/extents within the Volume Group vgdisplay {vgname} | grep Free Decide if you want to declare the size in relative or absolute terms lvextend -r /dev/vgroot/lvtmp -L 30G # set size to 30GB or lvextend -r /dev/vgroot/lvtmp -L +5G # increase size by 5GB or lvextend -r /dev/vgroot/lvtmp -l +100%FREE # increase the LV to use up all free extents within the VG Live LogicalVolume extension with a separate step to extend the filesystem Determine the free space/extents within the Volume Group vgdisplay | grep Free Extend the Logical Volume lvextend -L +100MB /dev/vgsys/tmp # Increase size by 100MB Extend the Filesystem to fill the logical Volume resize2fs /dev/vgsys/tmp # optional size parameter resize2fs 1.41.12 (17-May-2010) Filesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required old desc_blocks = 1, new_desc_blocks = 1 Performing an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks. The filesystem on /dev/vgsys/tmp is now 557056 blocks long. Reconfigure swap expanding swap swapoff -a # turn swap off lvextend /dev/vgroot/lvswap -L 30G # set size to 30GB or lvextend /dev/vgroot/lvswap -L +5G # increase size by 5GB or lvextend /dev/vgroot/lvswap -l +100%FREE # increase the LV to use up all free extents within the VG mkswap /dev/vgroot/lvswap # recreate the swap filesystem swapon -a # turn swap on removing swap (to freee up disk for filesystem expansion) swapoff -a Edit /etc/fstab && remove the swap line, to stop it from activating upon next boot /dev/vgsys/swap swap swap defaults 0 0 Remove the swap logical volume lvremove /dev/vgroot/lvswap now you can use the swap space for something else turn the swap back on swapon -a show the swap swapon -s free -g","title":"Extend lvm filesystem"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#lvm-tips-and-hints","text":"","title":"LVM tips and Hints"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#resizing-extending-logical-volumes-and-filesystems","text":"","title":"Resizing (extending) Logical Volumes and filesystems"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#live-logicalvolume-and-filesystem-extension-in-a-single-step-r","text":"Determine the free space/extents within the Volume Group vgdisplay {vgname} | grep Free Decide if you want to declare the size in relative or absolute terms lvextend -r /dev/vgroot/lvtmp -L 30G # set size to 30GB or lvextend -r /dev/vgroot/lvtmp -L +5G # increase size by 5GB or lvextend -r /dev/vgroot/lvtmp -l +100%FREE # increase the LV to use up all free extents within the VG","title":"Live LogicalVolume and Filesystem  extension in a single step (-r)"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#live-logicalvolume-extension-with-a-separate-step-to-extend-the-filesystem","text":"Determine the free space/extents within the Volume Group vgdisplay | grep Free Extend the Logical Volume lvextend -L +100MB /dev/vgsys/tmp # Increase size by 100MB Extend the Filesystem to fill the logical Volume resize2fs /dev/vgsys/tmp # optional size parameter resize2fs 1.41.12 (17-May-2010) Filesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required old desc_blocks = 1, new_desc_blocks = 1 Performing an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks. The filesystem on /dev/vgsys/tmp is now 557056 blocks long.","title":"Live LogicalVolume extension with a separate step to extend the filesystem"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#reconfigure-swap","text":"","title":"Reconfigure swap"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#expanding-swap","text":"swapoff -a # turn swap off lvextend /dev/vgroot/lvswap -L 30G # set size to 30GB or lvextend /dev/vgroot/lvswap -L +5G # increase size by 5GB or lvextend /dev/vgroot/lvswap -l +100%FREE # increase the LV to use up all free extents within the VG mkswap /dev/vgroot/lvswap # recreate the swap filesystem swapon -a # turn swap on","title":"expanding swap"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#removing-swap-to-freee-up-disk-for-filesystem-expansion","text":"swapoff -a Edit /etc/fstab && remove the swap line, to stop it from activating upon next boot /dev/vgsys/swap swap swap defaults 0 0 Remove the swap logical volume lvremove /dev/vgroot/lvswap now you can use the swap space for something else","title":"removing swap (to freee up disk for filesystem expansion)"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#turn-the-swap-back-on","text":"swapon -a","title":"turn the swap back on"},{"location":"linux/Disks_and_File_Systems/Extend_lvm_filesystem/#show-the-swap","text":"swapon -s free -g","title":"show the swap"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/","text":"Resizing (extending) Logical Volumes and filesystems Live LogicalVolume and Filesystem extension in a single step (-r) Determine the free space/extents withing the Volume Group vgdisplay {vgname} | grep Free Decide if you want to declare the size in relative or absolute terms lvextend -r /dev/vgroot/lvtmp -L 30G # set size to 30GB or lvextend -r /dev/vgroot/lvtmp -L +5G # increase size by 5GB or lvextend -r /dev/vgroot/lvtmp -l +100%FREE # increase the LV to use up all free extents within the VG Live LogicalVolume extension with a separate step to extend the filesystem Determine the free space/extents within the Volume Group vgdisplay | grep Free Extend the Logical Volume lvextend -L +100MB /dev/vgsys/tmp # Increase size by 100MB Extend the Filesystem to fill the logical Volume resize2fs /dev/vgsys/tmp # optional size parameter resize2fs 1.41.12 (17-May-2010) Filesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required old desc_blocks = 1, new_desc_blocks = 1 Performing an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks. The filesystem on /dev/vgsys/tmp is now 557056 blocks long. Reconfigure swap expanding swap swapoff -a # turn swap off lvextend /dev/vgroot/lvswap -L 30G # set size to 30GB or lvextend /dev/vgroot/lvswap -L +5G # increase size by 5GB or lvextend /dev/vgroot/lvswap -l +100%FREE # increase the LV to use up all free extents within the VG mkswap /dev/vgroot/lvswap # recreate the swap filesystem swapon -a # turn swap on removing swap (to free up disk for filesystem expansion) swapoff -a Edit /etc/fstab && remove the swap line, to stop it from activating upon next boot /dev/vgsys/swap swap swap defaults 0 0 Remove the swap logical volume lvremove /dev/vgroot/lvswap now you can use the swap space for something else turn the swap back on swapon -a show the swap swapon -s free -g Shrinking a linux filesystem on LVM Online reduce/shrink 1. Unmount filesystem if you cant unmount the filesystem on the running system see the offline section below umount /dev/vg01/lvs-02 2. Perform a check of the filesystem... e2fsck -f /dev/vg01/lvs-02 3. Resize the filesystem resize2fs /dev/vg01/lvs-02 15G 2. Reduce the size of the logical volume. lvreduce -L -15G /dev/vg01/lvs-02 # Optional: you can also shrink down the volume group if desired. vgreduce vg01 /dev/sdxy Offline reduce/shrink root filesystem 1. boot from rescue media if san connected append \"linux mpath\" to boot options Do not mount filesystems when booting from rescue media select the menu option that skips the mounting of your filesyetsms 2. Identify the Volume group /usr/sbin/lvm vgscan /usr/sbin/lvm vgdisplay note: you have to run lvm {operand}... instead of the aliases lvscan 3. Activate the the volume group /usr/sbin/lvm vgchange {VG-name} -a y 4. Perform a check of the filesystem... /usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02 5. Resize the filesystem resize2fs /dev/vg01/lvs-02 15G 6. Reduce the size of the logical volume. /usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02 # Optional: you can also shrink down the volume group if desired. /usr/sbin/lvm vgreduce vg01 /dev/sdxy Example: reducing the size of a swap volume [root@server ~]# swapoff -a [root@server ~]# lvreduce /dev/vgsys/swap -L 20G Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda New size given (640 extents) not less than existing size (625 extents) Run `lvreduce --help' for more information. [root@server ~]# lvreduce /dev/vgsys/swap -L 10G Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda WARNING: Reducing active logical volume to 10.00 GB THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce swap? [y/n]: y Reducing logical volume swap to 10.00 GB Logical volume swap successfully resized [root@server ~]# mkswap /dev/vgsys/swap Setting up swapspace version 1, size = 10737414 kB [root@server ~]# swapon -a [root@server ~]# free total used free shared buffers cached Mem: 65931888 527968 65403920 0 32036 343660 -/+ buffers/cache: 152272 65779616 Swap: 10485752 0 10485752","title":"Linux LVM Logical Volume Manager"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#resizing-extending-logical-volumes-and-filesystems","text":"","title":"Resizing (extending) Logical Volumes and filesystems"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#live-logicalvolume-and-filesystem-extension-in-a-single-step-r","text":"Determine the free space/extents withing the Volume Group vgdisplay {vgname} | grep Free Decide if you want to declare the size in relative or absolute terms lvextend -r /dev/vgroot/lvtmp -L 30G # set size to 30GB or lvextend -r /dev/vgroot/lvtmp -L +5G # increase size by 5GB or lvextend -r /dev/vgroot/lvtmp -l +100%FREE # increase the LV to use up all free extents within the VG","title":"Live LogicalVolume and Filesystem  extension in a single step (-r)"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#live-logicalvolume-extension-with-a-separate-step-to-extend-the-filesystem","text":"Determine the free space/extents within the Volume Group vgdisplay | grep Free Extend the Logical Volume lvextend -L +100MB /dev/vgsys/tmp # Increase size by 100MB Extend the Filesystem to fill the logical Volume resize2fs /dev/vgsys/tmp # optional size parameter resize2fs 1.41.12 (17-May-2010) Filesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required old desc_blocks = 1, new_desc_blocks = 1 Performing an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks. The filesystem on /dev/vgsys/tmp is now 557056 blocks long.","title":"Live LogicalVolume extension with a separate step to extend the filesystem"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#reconfigure-swap","text":"","title":"Reconfigure swap"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#expanding-swap","text":"swapoff -a # turn swap off lvextend /dev/vgroot/lvswap -L 30G # set size to 30GB or lvextend /dev/vgroot/lvswap -L +5G # increase size by 5GB or lvextend /dev/vgroot/lvswap -l +100%FREE # increase the LV to use up all free extents within the VG mkswap /dev/vgroot/lvswap # recreate the swap filesystem swapon -a # turn swap on","title":"expanding swap"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#removing-swap-to-free-up-disk-for-filesystem-expansion","text":"swapoff -a Edit /etc/fstab && remove the swap line, to stop it from activating upon next boot /dev/vgsys/swap swap swap defaults 0 0 Remove the swap logical volume lvremove /dev/vgroot/lvswap now you can use the swap space for something else","title":"removing swap (to free up disk for filesystem expansion)"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#turn-the-swap-back-on","text":"swapon -a","title":"turn the swap back on"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#show-the-swap","text":"swapon -s free -g","title":"show the swap"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#shrinking-a-linux-filesystem-on-lvm","text":"","title":"Shrinking a linux filesystem on LVM"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#online-reduceshrink","text":"","title":"Online reduce/shrink"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#1-unmount-filesystem","text":"if you cant unmount the filesystem on the running system see the offline section below umount /dev/vg01/lvs-02","title":"1. Unmount filesystem"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#2-perform-a-check-of-the-filesystem","text":"e2fsck -f /dev/vg01/lvs-02","title":"2. Perform a check of the filesystem..."},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#3-resize-the-filesystem","text":"resize2fs /dev/vg01/lvs-02 15G","title":"3. Resize the filesystem"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#2-reduce-the-size-of-the-logical-volume","text":"lvreduce -L -15G /dev/vg01/lvs-02","title":"2. Reduce the size of the logical volume."},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#optional-you-can-also-shrink-down-the-volume-group-if-desired","text":"vgreduce vg01 /dev/sdxy","title":"# Optional: you can also shrink down the volume group if desired."},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#offline-reduceshrink-root-filesystem","text":"","title":"Offline reduce/shrink root filesystem"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#1-boot-from-rescue-media","text":"if san connected append \"linux mpath\" to boot options","title":"1. boot from rescue media"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#do-not-mount-filesystems-when-booting-from-rescue-media","text":"select the menu option that skips the mounting of your filesyetsms","title":"Do not mount filesystems when booting from rescue media"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#2-identify-the-volume-group","text":"/usr/sbin/lvm vgscan /usr/sbin/lvm vgdisplay note: you have to run lvm {operand}... instead of the aliases lvscan","title":"2. Identify the Volume group"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#3-activate-the-the-volume-group","text":"/usr/sbin/lvm vgchange {VG-name} -a y","title":"3. Activate the the volume group"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#4-perform-a-check-of-the-filesystem","text":"/usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02","title":"4. Perform a check of the filesystem..."},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#5-resize-the-filesystem","text":"resize2fs /dev/vg01/lvs-02 15G","title":"5. Resize the filesystem"},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#6-reduce-the-size-of-the-logical-volume","text":"/usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02","title":"6. Reduce the size of the logical volume."},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#optional-you-can-also-shrink-down-the-volume-group-if-desired_1","text":"/usr/sbin/lvm vgreduce vg01 /dev/sdxy","title":"# Optional: you can also shrink down the volume group if desired."},{"location":"linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#example-reducing-the-size-of-a-swap-volume","text":"[root@server ~]# swapoff -a [root@server ~]# lvreduce /dev/vgsys/swap -L 20G Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda New size given (640 extents) not less than existing size (625 extents) Run `lvreduce --help' for more information. [root@server ~]# lvreduce /dev/vgsys/swap -L 10G Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda WARNING: Reducing active logical volume to 10.00 GB THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce swap? [y/n]: y Reducing logical volume swap to 10.00 GB Logical volume swap successfully resized [root@server ~]# mkswap /dev/vgsys/swap Setting up swapspace version 1, size = 10737414 kB [root@server ~]# swapon -a [root@server ~]# free total used free shared buffers cached Mem: 65931888 527968 65403920 0 32036 343660 -/+ buffers/cache: 152272 65779616 Swap: 10485752 0 10485752","title":"Example: reducing the size of a swap volume"},{"location":"linux/Disks_and_File_Systems/Listing_Oracle_ASM_disks/","text":"listing disks in oracle grid cluster [root@server ~]# ls -l /dev/oracleasm/disks/ total 0 brw-rw---- 1 grid oinstall 253, 31 Jun 19 18:25 VOLNP09 brw-rw---- 1 grid oinstall 253, 18 Jun 19 18:25 VOLNP10 brw-rw---- 1 grid oinstall 253, 19 Jun 19 18:25 VOLNP11 brw-rw---- 1 grid oinstall 253, 22 Jun 19 18:25 VOLNP12 [root@server ~]# oracleasm listdisks VOLNP09 VOLNP10 VOLNP11 VOLNP12","title":"Listing Oracle ASM disks"},{"location":"linux/Disks_and_File_Systems/Listing_Oracle_ASM_disks/#listing-disks-in-oracle-grid-cluster","text":"[root@server ~]# ls -l /dev/oracleasm/disks/ total 0 brw-rw---- 1 grid oinstall 253, 31 Jun 19 18:25 VOLNP09 brw-rw---- 1 grid oinstall 253, 18 Jun 19 18:25 VOLNP10 brw-rw---- 1 grid oinstall 253, 19 Jun 19 18:25 VOLNP11 brw-rw---- 1 grid oinstall 253, 22 Jun 19 18:25 VOLNP12 [root@server ~]# oracleasm listdisks VOLNP09 VOLNP10 VOLNP11 VOLNP12","title":"listing disks in oracle grid cluster"},{"location":"linux/Disks_and_File_Systems/Multipath_and_NetApp_fixes/","text":"Fixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5 cause RHEL 5 multipath bindings file is located in var /var isn't mounted before multipath is configured redhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot solution (work around) dont forget to comment out vi /etc/multipath.conf add the following lines to multipath config to set netapp specific settings defaults { user_friendly_names yes bindings_file /etc/multipath/bindings ##changed flush_on_last_del yes max_fds max pg_prio_calc avg queue_without_daemon no } devices { device { vendor \"NETAPP\" product \"LUN\" path_checker tur path_selector \"round-robin 0\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" # prio_callout \"/sbin/mpath_prio_ontap /dev/%n\" prio_callout \"/sbin/mpath_prio_alua /dev/%n\" # features \"1 queue_if_no_path\" features \"3 queue_if_no_path pg_init_retries 50\" # hardware_handler \"0\" hardware_handler \"1 alua\" path_grouping_policy group_by_prio failback immediate rr_weight uniform rr_min_io 128 } } mkdir /etc/multipath cp /var/lib/multipath/bindings /etc/multipath/bindings cd /boot cp initrd*img initrd*img.multipath-bak mkinitrd -f initrd-`uname -r`.img `uname -r` ls -ltr #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size","title":"Multipath and NetApp fixes"},{"location":"linux/Disks_and_File_Systems/Multipath_and_NetApp_fixes/#fixing-the-inconsistent-path-ordering-when-using-friendly_names-and-a-separate-var-partition-on-rhel-5","text":"","title":"Fixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5"},{"location":"linux/Disks_and_File_Systems/Multipath_and_NetApp_fixes/#cause","text":"RHEL 5 multipath bindings file is located in var /var isn't mounted before multipath is configured redhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot","title":"cause"},{"location":"linux/Disks_and_File_Systems/Multipath_and_NetApp_fixes/#solution-work-around","text":"dont forget to comment out vi /etc/multipath.conf","title":"solution (work around)"},{"location":"linux/Disks_and_File_Systems/Multipath_and_NetApp_fixes/#add-the-following-lines-to-multipath-config-to-set-netapp-specific-settings","text":"defaults { user_friendly_names yes bindings_file /etc/multipath/bindings ##changed flush_on_last_del yes max_fds max pg_prio_calc avg queue_without_daemon no } devices { device { vendor \"NETAPP\" product \"LUN\" path_checker tur path_selector \"round-robin 0\" getuid_callout \"/sbin/scsi_id -g -u -s /block/%n\" # prio_callout \"/sbin/mpath_prio_ontap /dev/%n\" prio_callout \"/sbin/mpath_prio_alua /dev/%n\" # features \"1 queue_if_no_path\" features \"3 queue_if_no_path pg_init_retries 50\" # hardware_handler \"0\" hardware_handler \"1 alua\" path_grouping_policy group_by_prio failback immediate rr_weight uniform rr_min_io 128 } } mkdir /etc/multipath cp /var/lib/multipath/bindings /etc/multipath/bindings cd /boot cp initrd*img initrd*img.multipath-bak mkinitrd -f initrd-`uname -r`.img `uname -r` ls -ltr #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size","title":"add the following lines to multipath config to set netapp specific settings"},{"location":"linux/Disks_and_File_Systems/Online_lun_resize/","text":"Increase the Lun size on the SAN or VM Hypervisor dmesg | tail sdb: Write Protect is off sdb: Mode Sense: 61 00 00 00 sdb: cache data unavailable sdb: assuming drive cache: write through sdb: detected capacity change from 214748364800 to 429496729600 [root@server ~]# pvscan PV /dev/sdb VG vgdata lvm2 [199.97 GB / 0 free] PV /dev/sda2 VG vgsys lvm2 [49.75 GB / 5.81 GB free] Total: 2 [249.72 GB] / in use: 2 [249.72 GB] / in no VG: 0 [0 ] [root@server ~]# pvresize /dev/sdb Physical volume \"/dev/sdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized [root@server ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 vgsys lvm2 a-- 49.75G 5.81G /dev/sdb vgdata0 lvm2 a-- 399.97G 200.00G [root@server ~]# lvextend -r -l +100%FREE /dev/vgdata/lv-data Extending logical volume opt_splunk_var_lib_splunk to 399.97 GB Logical volume opt_splunk_var_lib_splunk successfully resized resize4fs 1.41.12 (17-May-2010) Filesystem at /dev/mapper/vgdata-lvdata is mounted on /mnt/data on-line resizing required old desc_blocks = 13, new_desc_blocks = 25 Performing an on-line resize of /dev/mapper/vgdata-lvdata to 104849408 (4k) blocks. The filesystem on /dev/mapper/vgdata-lvdata is now 104849408 blocks long. [root@server ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/vgsys-root 20G 3.7G 15G 20% / /dev/sda1 244M 49M 183M 21% /boot tmpfs 1.9G 0 1.9G 0% /dev/shm /dev/mapper/vgdata/lvdata 394G 143G 232G 39% /mnt/data","title":"Online lun resize"},{"location":"linux/Disks_and_File_Systems/RHEL_floppy_kickstart/","text":"Working with Floppy disk images Create the image:- dd if=/dev/zero of=floppy.img bs=1K count=1440 mkdosfs -F 12 floppy.img # or /sbin/mkfs.msdos -C floppy.img 1440 Check the image:- mount -o loop -t msdos dosfloppy /mnt/floppy df -h /mnt/floppy umount /mnt/floppy Copy files to the image:- mount -o loop -t msdos dosfloppy /mnt/floppy cp file.txt /mnt/floppy umount /mnt/floppy Keep in mind file names are limited to 8.3 case insensitive chars http://en.wikipedia.org/wiki/8.3_filename The Floppy image contains","title":"RHEL floppy kickstart"},{"location":"linux/Disks_and_File_Systems/boot_cdrom_dvdrom/","text":"#!/bin/bash # This script will create a bootable ISO9660 image # This script has to be run one directory below the cdrom source directory # #you can modify the kickstart config file to suit your preferences # # this is the name of the outputted ISO file OUT=\"RHEL4u3\" #TARGET=\"/home/seamus/$VER\" DATE=`date +%Y%m%d\"_\"%H%M` #-V \"$VER\" \\ mkisofs -o $OUT.iso \\ -b isolinux/isolinux.bin \\ -c isolinux/boot.cat \\ -no-emul-boot -boot-load-size 4 -boot-info-table \\ -R -J -v -T ./bootdisk/","title":"Boot cdrom dvdrom"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/","text":"disk/partition/filesystem imaging with dd, squashfs, gzip, losetup and avfs --To increase the compression ratio of the image, zero the free space in the filesystems before creating the initial image.-- Prepare the source filesystems....... Windows delete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/ defrag to reorder the bits... I just use the microsoft built in tool zero the free space...I use sdelete -z from sysinternals http://download.sysinternals.com/files/SDelete.zip Linux delete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/ defrag if you think it is necessary... e4defrag zero the free space... $ dd if=/dev/zero of=/{mounted filesystem}/tempfile bs=1M remove the temporary zero file Make block copies of the filesystems using dd Individual Partition image $ dd if=/dev/sdb1 of=/backups/sdb1.dd.img Whole disk image $ dd if=/dev/sdb of=/backups/sdb.dd.img Compressing the image to save space -- note if you never plan on mounting the image to retrieve individual files you can pipe the dd through gzip $ dd if=/dev/sdb1 bs=1M | gzip > /backups/sdb1.dd.img.gz or you can gzip later $ dd if=/dev/sdb1 bs=1M of=/backups/sdb1.dd.img && gzip /backups/sdb1.dd.img dd does not give you any progress indication. you can however send the dd process a kill -USR1 signal and it will output its progress to std out. Note: you will have to send this signal from another terminal ps | grep dd && kill -USR1 {pid} or kill -USR1 `ps -aef | grep \"dd\\ if\" | awk '{print $2}' you should see output similar to the following in the terminal that is running the dd command 12917407744 bytes (13 GB) copied, 511.9 s, 25MB/s Make file system copy using squashfs. the advantages of squashfs over dd are.. the compression is multithreaded so its faster on multicore machines easy to mount without having to decompress the whole image. dd if=/dev/sdb1 | gzip > /backups/sdb1.dd.img The disadvantage of squashfs over dd is you cant use it to archive a whole disk including the partition table and boot block etc..x Mounting and restoring from the disk/filesytems images.. mounting the mksquashfs image $ mount /backups/sdb1.dd.img.sqfs /mnt/sdb1-squashfs mounting the Partition image $ mount /backups/sdb1.dd.img.sqfs/sdb1.dd.img sdb1-img mounting a whole disk image you will have to use kpartx to create the the device nodes $ kpartx -a /backup/sdb1.dd.img.sqfs/sdb.dd.img --then mount the individual file systems, the device node will be in /dev/mapper/loop mount /dev/mapper/loop1p1 /mnt/sdb1 mounting a single partition from the disk image --use parted to display the partition table in Bytes-- $ parted /backups/sdb.dd.img unit B print Model: (file) Disk /backups/sdb.dd.img: 268435456B Sector size (logical/physical): 512B/512B Partition Table: msdos Number Start End Size Type File system Flags 1 1024B 99980287B 99979264B primary ext4 boot 2 99980288B 268435455B 168455168B extended 5 99981312B 268435455B 168454144B logical ext4 --specify and offset to the mount command-- $ mount -o loop,offset=99981312 /backups/sdb.dd.img /media/sdb5 --if there are file systems issues you may need to mount with the options -o ro,noload-- using avf and losetup to mount a gzipped dd partition image install http://sourceforge.net/projects/avf/ create a virtual filesystem $ mountavfs Mounting AVFS on /root/.avfs... there is now a pseudo-fs replica of the real file system in /root/.avfs $ ls /root/.avfs/ bin/ etc/ lib64/ mnt/ root/ selinux/ tmp/ boot/ home/ lost+found/ opt/ run/ srv/ usr/ dev/ lib/ media/ proc/ sbin/ sys/ var/ /backups/ $ cd /root/.avfs/backups/ 256/ 256sqash/ guess/ hdb1/ loop1p2/ loop2/ sdb5/ 256MBimg/ avfs/ gziped/ loop1p1/ loop1p5/ sdb1/ sqfs/ $ ls 256 gziped loop2 sdb1.tgz sdb5.sqfs sdb.dd.img 256MBimg hdb1 sdb1 sdb5 sdb5.tgz sdb.dd.img.sqfs 256sqash loop1p1 sdb1.dd.img sdb5.dd.img sdb5.zero-filled.dd.img sdb.zero-filled.dd.img avfs loop1p2 sdb1.dd.img.sqfs sdb5.dd.img.gz sdb5.zero-filled.dd.img.gz sdb.zero-filled.dd.img.sqfs guess loop1p5 sdb1.sqfs sdb5.dd.img.sqfs sdb5.zero-filled.dd.img.sqfs sqfs $ losetup /dev/loop2 sdb5.gz# --the hash at the end is needed, it is short hand for losetup /dev/loop2 sdb5.gz#gunzip $ mount /dev/loop2 gziped mount: block device /dev/loop2 is write-protected, mounting read-only mount: wrong fs type, bad option, bad superblock on /dev/loop2, missing codepage or helper program, or other error In some cases useful info is found in syslog - try dmesg | tail or so $ mount -o ro,noload /dev/loop2 gziped $ df Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 14317616 6159480 7424184 46% / none 4 0 4 0% /sys/fs/cgroup udev 1008392 4 1008388 1% /dev tmpfs 205120 936 204184 1% /run none 5120 0 5120 0% /run/lock none 1025600 76 1025524 1% /run/shm none 102400 36 102364 1% /run/user /dev/loop2 159304 5646 145433 4% /root/.avfs/media/gziped","title":"Disk filesystem partition imaging using squashfs and dd"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#diskpartitionfilesystem-imaging-with-dd-squashfs-gzip-losetup-and-avfs","text":"--To increase the compression ratio of the image, zero the free space in the filesystems before creating the initial image.--","title":"disk/partition/filesystem imaging with dd, squashfs, gzip, losetup and avfs"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#prepare-the-source-filesystems","text":"","title":"Prepare the source filesystems......."},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#windows","text":"delete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/ defrag to reorder the bits... I just use the microsoft built in tool zero the free space...I use sdelete -z from sysinternals http://download.sysinternals.com/files/SDelete.zip","title":"Windows"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#linux","text":"delete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/ defrag if you think it is necessary... e4defrag zero the free space... $ dd if=/dev/zero of=/{mounted filesystem}/tempfile bs=1M remove the temporary zero file","title":"Linux"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#make-block-copies-of-the-filesystems-using-dd","text":"","title":"Make block copies of the filesystems using dd"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#individual-partition-image","text":"$ dd if=/dev/sdb1 of=/backups/sdb1.dd.img","title":"Individual Partition image"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#whole-disk-image","text":"$ dd if=/dev/sdb of=/backups/sdb.dd.img Compressing the image to save space -- note if you never plan on mounting the image to retrieve individual files you can pipe the dd through gzip $ dd if=/dev/sdb1 bs=1M | gzip > /backups/sdb1.dd.img.gz or you can gzip later $ dd if=/dev/sdb1 bs=1M of=/backups/sdb1.dd.img && gzip /backups/sdb1.dd.img","title":"Whole disk image"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#dd-does-not-give-you-any-progress-indication","text":"you can however send the dd process a kill -USR1 signal and it will output its progress to std out. Note: you will have to send this signal from another terminal ps | grep dd && kill -USR1 {pid} or kill -USR1 `ps -aef | grep \"dd\\ if\" | awk '{print $2}'","title":"dd does not give you any progress indication."},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#you-should-see-output-similar-to-the-following-in-the-terminal-that-is-running-the-dd-command","text":"12917407744 bytes (13 GB) copied, 511.9 s, 25MB/s","title":"you should see output similar to the following in the terminal that is running the dd command"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#make-file-system-copy-using-squashfs","text":"the advantages of squashfs over dd are.. the compression is multithreaded so its faster on multicore machines easy to mount without having to decompress the whole image. dd if=/dev/sdb1 | gzip > /backups/sdb1.dd.img The disadvantage of squashfs over dd is you cant use it to archive a whole disk including the partition table and boot block etc..x","title":"Make file system copy using squashfs."},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#mounting-and-restoring-from-the-diskfilesytems-images","text":"","title":"Mounting and restoring from the disk/filesytems images.."},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#mounting-the-mksquashfs-image","text":"$ mount /backups/sdb1.dd.img.sqfs /mnt/sdb1-squashfs","title":"mounting the mksquashfs image"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#mounting-the-partition-image","text":"$ mount /backups/sdb1.dd.img.sqfs/sdb1.dd.img sdb1-img","title":"mounting the Partition image"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#mounting-a-whole-disk-image-you-will-have-to-use-kpartx-to-create-the-the-device-nodes","text":"$ kpartx -a /backup/sdb1.dd.img.sqfs/sdb.dd.img --then mount the individual file systems, the device node will be in /dev/mapper/loop mount /dev/mapper/loop1p1 /mnt/sdb1","title":"mounting a whole disk image you will have to use kpartx to create the the device nodes"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#mounting-a-single-partition-from-the-disk-image","text":"--use parted to display the partition table in Bytes-- $ parted /backups/sdb.dd.img unit B print Model: (file) Disk /backups/sdb.dd.img: 268435456B Sector size (logical/physical): 512B/512B Partition Table: msdos Number Start End Size Type File system Flags 1 1024B 99980287B 99979264B primary ext4 boot 2 99980288B 268435455B 168455168B extended 5 99981312B 268435455B 168454144B logical ext4 --specify and offset to the mount command-- $ mount -o loop,offset=99981312 /backups/sdb.dd.img /media/sdb5 --if there are file systems issues you may need to mount with the options -o ro,noload--","title":"mounting a single partition from the disk image"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#using-avf-and-losetup-to-mount-a-gzipped-dd-partition-image","text":"install http://sourceforge.net/projects/avf/","title":"using avf and losetup to mount a gzipped dd partition image"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#create-a-virtual-filesystem","text":"$ mountavfs Mounting AVFS on /root/.avfs...","title":"create a virtual filesystem"},{"location":"linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#there-is-now-a-pseudo-fs-replica-of-the-real-file-system-in-rootavfs","text":"$ ls /root/.avfs/ bin/ etc/ lib64/ mnt/ root/ selinux/ tmp/ boot/ home/ lost+found/ opt/ run/ srv/ usr/ dev/ lib/ media/ proc/ sbin/ sys/ var/ /backups/ $ cd /root/.avfs/backups/ 256/ 256sqash/ guess/ hdb1/ loop1p2/ loop2/ sdb5/ 256MBimg/ avfs/ gziped/ loop1p1/ loop1p5/ sdb1/ sqfs/ $ ls 256 gziped loop2 sdb1.tgz sdb5.sqfs sdb.dd.img 256MBimg hdb1 sdb1 sdb5 sdb5.tgz sdb.dd.img.sqfs 256sqash loop1p1 sdb1.dd.img sdb5.dd.img sdb5.zero-filled.dd.img sdb.zero-filled.dd.img avfs loop1p2 sdb1.dd.img.sqfs sdb5.dd.img.gz sdb5.zero-filled.dd.img.gz sdb.zero-filled.dd.img.sqfs guess loop1p5 sdb1.sqfs sdb5.dd.img.sqfs sdb5.zero-filled.dd.img.sqfs sqfs $ losetup /dev/loop2 sdb5.gz# --the hash at the end is needed, it is short hand for losetup /dev/loop2 sdb5.gz#gunzip $ mount /dev/loop2 gziped mount: block device /dev/loop2 is write-protected, mounting read-only mount: wrong fs type, bad option, bad superblock on /dev/loop2, missing codepage or helper program, or other error In some cases useful info is found in syslog - try dmesg | tail or so $ mount -o ro,noload /dev/loop2 gziped $ df Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 14317616 6159480 7424184 46% / none 4 0 4 0% /sys/fs/cgroup udev 1008392 4 1008388 1% /dev tmpfs 205120 936 204184 1% /run none 5120 0 5120 0% /run/lock none 1025600 76 1025524 1% /run/shm none 102400 36 102364 1% /run/user /dev/loop2 159304 5646 145433 4% /root/.avfs/media/gziped","title":"there is now a pseudo-fs replica of the real file system in /root/.avfs"},{"location":"linux/Disks_and_File_Systems/filesystem_shuffle/","text":"save the script below to a file on the root filesystem reboot into bash init=/bin/bash execute the script The script will exit if any of the steps fail #!/bin/bash fail_notify() { echo \"step \"$@\" on line $BASH_LINENO failed <><><><><><><><><><><><><><><\" exit } grep 'vgsys-var_log[[:blank:]]' /etc/fstab if [ $? == 0 ] then echo \"/var/log filesystem already exists\" else start_udev || fail_notify \"start_udev\" mount -o rw,remount / || fail_notify \"remount of root\" vgchange -ay vgsys || fail_notify \"activate vgsys\" lvchange -ay /dev/vgsys/var || fail_notify \" activate lv var\" vgscan --mknodes -v || fail_notify \"creating device nodes\" lvcreate vgsys -L 2G -n var_log || fail_notify \"lvcreate\" mkfs.ext4 /dev/vgsys/var_log || fail_notify \"mkfs\" mkdir /tmp/var_log # no test as the directory may already exist if rerunning script mount /dev/vgsys/var_log /tmp/var_log || fail_notify \"mount\" mount /var || fail_notify \"mount var\" cd /var/log/ || fail_notify \"cd /var/log \" echo \"copying files from /var/log/ to new file system /tmp/var_log/\" find . -mount -print0 | cpio -0dump /tmp/var_log/ || fail_notofy \"find\" # need to put this line before /var/log/audit mount point otherwise the the file systems will overlap sed -i '/vgsys-var_log_audit/i \\/dev\\/mapper\\/vgsys\\-var\\_log\\ \\/var\\/log\\ \\ ext4 \\ nodev\\,nosuid\\,noexec \\ 1\\ 2' /etc/fstab || fail notify \" fstab update\" df # display a df to the user so they can check what has happened echo \"remounting filesystems as read only\" mount -o ro,remount /tmp/var_log || fail_notify \"remount of var_log\" mount -o ro,remount /var || fail_notify \"remount of var\" mount -o ro,remount / || fail_notify \"remount of /\" echo \"+--------------------------+\" echo \"| power cycle vm to finish |\" echo \"| or execute \"exec init 6\" |\" echo \"+--------------------------+\" fi","title":"Filesystem shuffle"},{"location":"linux/Disks_and_File_Systems/rsync/","text":"rsync coping files from a source directory that doesn't exits in the destination directory, note this does not perform checksums and does not compare date statmps etc. Its handy for copying original data like photos imported from a camera rsync -zhr --ignore-existing --progress --dry-run /source/ /dest/ | more","title":"Rsync"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/","text":"Shrinking a Linux filesystem on LVM Online reduce/shrink 1. Unmount filesystem if you cant unmount the filesystem on the running system see the offline section below umount /dev/vg01/lvs-02 2. Perform a check of the filesystem... e2fsck -f /dev/vg01/lvs-02 3. Resize the filesystem resize2fs /dev/vg01/lvs-02 15G 2. Reduce the size of the logical volume. lvreduce -L -15G /dev/vg01/lvs-02 # Optional: you can also shrink down the volume group if desired. vgreduce vg01 /dev/sdxy Offline reduce/shrink root filesystem 1. boot from rescue media if san connected append \"linux mpath\" to boot options Do not mount filesystems when booting from rescue media select the menu option that skips the mounting of your filesyetsms 2. Identify the Volume group /usr/sbin/lvm vgscan /usr/sbin/lvm vgdisplay note: you have to run lvm {operand}... instead of the aliases lvscan 3. Activate the the volume group /usr/sbin/lvm vgchange {VG-name} -a y 4. Perform a check of the filesystem... /usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02 5. Resize the filesystem resize2fs /dev/vg01/lvs-02 15G 6. Reduce the size of the logical volume. /usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02 # Optional: you can also shrink down the volume group if desired. /usr/sbin/lvm vgreduce vg01 /dev/sdxy Example: reducing the size of a swap volume [root@server ~]# swapoff -a [root@server ~]# lvreduce /dev/vgsys/swap -L 20G Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda New size given (640 extents) not less than existing size (625 extents) Run `lvreduce --help' for more information. [root@server ~]# lvreduce /dev/vgsys/swap -L 10G Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda WARNING: Reducing active logical volume to 10.00 GB THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce swap? [y/n]: y Reducing logical volume swap to 10.00 GB Logical volume swap successfully resized [root@server ~]# mkswap /dev/vgsys/swap Setting up swapspace version 1, size = 10737414 kB [root@server ~]# swapon -a [root@server ~]# free total used free shared buffers cached Mem: 65931888 527968 65403920 0 32036 343660 -/+ buffers/cache: 152272 65779616 Swap: 10485752 0 10485752","title":"Shrinking a Linux filesystem on LVM"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#shrinking-a-linux-filesystem-on-lvm","text":"","title":"Shrinking a Linux filesystem on LVM"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#online-reduceshrink","text":"","title":"Online reduce/shrink"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#1-unmount-filesystem","text":"if you cant unmount the filesystem on the running system see the offline section below umount /dev/vg01/lvs-02","title":"1. Unmount filesystem"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#2-perform-a-check-of-the-filesystem","text":"e2fsck -f /dev/vg01/lvs-02","title":"2. Perform a check of the filesystem..."},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#3-resize-the-filesystem","text":"resize2fs /dev/vg01/lvs-02 15G","title":"3. Resize the filesystem"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#2-reduce-the-size-of-the-logical-volume","text":"lvreduce -L -15G /dev/vg01/lvs-02","title":"2. Reduce the size of the logical volume."},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#optional-you-can-also-shrink-down-the-volume-group-if-desired","text":"vgreduce vg01 /dev/sdxy","title":"# Optional: you can also shrink down the volume group if desired."},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#offline-reduceshrink-root-filesystem","text":"","title":"Offline reduce/shrink root filesystem"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#1-boot-from-rescue-media","text":"if san connected append \"linux mpath\" to boot options","title":"1. boot from rescue media"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#do-not-mount-filesystems-when-booting-from-rescue-media","text":"select the menu option that skips the mounting of your filesyetsms","title":"Do not mount filesystems when booting from rescue media"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#2-identify-the-volume-group","text":"/usr/sbin/lvm vgscan /usr/sbin/lvm vgdisplay note: you have to run lvm {operand}... instead of the aliases lvscan","title":"2. Identify the Volume group"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#3-activate-the-the-volume-group","text":"/usr/sbin/lvm vgchange {VG-name} -a y","title":"3. Activate the the volume group"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#4-perform-a-check-of-the-filesystem","text":"/usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02","title":"4. Perform a check of the filesystem..."},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#5-resize-the-filesystem","text":"resize2fs /dev/vg01/lvs-02 15G","title":"5. Resize the filesystem"},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#6-reduce-the-size-of-the-logical-volume","text":"/usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02","title":"6. Reduce the size of the logical volume."},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#optional-you-can-also-shrink-down-the-volume-group-if-desired_1","text":"/usr/sbin/lvm vgreduce vg01 /dev/sdxy","title":"# Optional: you can also shrink down the volume group if desired."},{"location":"linux/Disks_and_File_Systems/shrink_lvm_filesystem/#example-reducing-the-size-of-a-swap-volume","text":"[root@server ~]# swapoff -a [root@server ~]# lvreduce /dev/vgsys/swap -L 20G Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda New size given (640 extents) not less than existing size (625 extents) Run `lvreduce --help' for more information. [root@server ~]# lvreduce /dev/vgsys/swap -L 10G Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda WARNING: Reducing active logical volume to 10.00 GB THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce swap? [y/n]: y Reducing logical volume swap to 10.00 GB Logical volume swap successfully resized [root@server ~]# mkswap /dev/vgsys/swap Setting up swapspace version 1, size = 10737414 kB [root@server ~]# swapon -a [root@server ~]# free total used free shared buffers cached Mem: 65931888 527968 65403920 0 32036 343660 -/+ buffers/cache: 152272 65779616 Swap: 10485752 0 10485752","title":"Example: reducing the size of a swap volume"},{"location":"linux/Firewalls_and_Security/auditd_rules/","text":"Using auuditd and aureport on RHEL to track whats going on. Generate list of SUID programs for SUID_BIN in `find / -xdev \\( -perm -4000 \\) -type f -print` do AUDIT_CMD=\"auditctl -A exit,always -F path=\" AUDIT_PERM=\"-pxwra\" KEY=`echo $SUID_BIN | awk -F\\/ '{ print \"-k key-suid-\"$NF }'` echo $AUDIT_CMD$SUID_BIN\" \"$AUDIT_PERM $KEY done Backup the existing audit rules cd /etc/audit/ cp audit.rules audit.rules.orig && vi audit.rules Sample audit rules #Attempts to perform unauthorised functions #-a exit,always -F arch=b64 -F success!=0 -S open -k key-fopen-failure #Additions, deletions and modifications to security/audit log parameters #-a exit,always -F dir=/var/log -F perm=wa -k key-writes-access-var-log #Critical file changes #Activity performed by privileged accounts Modifications to system settings (parameters) -w /etc/ -p w -k key-file-change-etc #The authority and access to use advanced operating system utilities and commands that bypass system access controls must be monitored, logged, reviewed and restricted to those individuals who require access to perform their job functions. auditctl -A exit,always -F path=/opt/google/chrome/chrome-sandbox -pxwra -k key-suid-chrome-sandbox auditctl -A exit,always -F path=/usr/sbin/pppd -pxwra -k key-suid-pppd auditctl -A exit,always -F path=/usr/sbin/uuidd -pxwra -k key-suid-uuidd auditctl -A exit,always -F path=/usr/lib/dbus-1.0/dbus-daemon-launch-helper -pxwra -k key-suid-dbus-daemon-launch-helper auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxSDL -pxwra -k key-suid-VBoxSDL auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxNetAdpCtl -pxwra -k key-suid-VBoxNetAdpCtl auditctl -A exit,always -F path=/usr/lib/virtualbox/VirtualBox -pxwra -k key-suid-VirtualBox auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxHeadless -pxwra -k key-suid-VBoxHeadless auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxNetDHCP -pxwra -k key-suid-VBoxNetDHCP auditctl -A exit,always -F path=/usr/lib/x86_64-linux-gnu/oxide-qt/chrome-sandbox -pxwra -k key-suid-chrome-sandbox auditctl -A exit,always -F path=/usr/lib/policykit-1/polkit-agent-helper-1 -pxwra -k key-suid-polkit-agent-helper-1 auditctl -A exit,always -F path=/usr/lib/eject/dmcrypt-get-device -pxwra -k key-suid-dmcrypt-get-device auditctl -A exit,always -F path=/usr/lib/pt_chown -pxwra -k key-suid-pt_chown auditctl -A exit,always -F path=/usr/lib/openssh/ssh-keysign -pxwra -k key-suid-ssh-keysign auditctl -A exit,always -F path=/usr/bin/traceroute6.iputils -pxwra -k key-suid-traceroute6.iputils auditctl -A exit,always -F path=/usr/bin/chfn -pxwra -k key-suid-chfn auditctl -A exit,always -F path=/usr/bin/pkexec -pxwra -k key-suid-pkexec auditctl -A exit,always -F path=/usr/bin/passwd -pxwra -k key-suid-passwd auditctl -A exit,always -F path=/usr/bin/X -pxwra -k key-suid-X auditctl -A exit,always -F path=/usr/bin/fping -pxwra -k key-suid-fping auditctl -A exit,always -F path=/usr/bin/chsh -pxwra -k key-suid-chsh auditctl -A exit,always -F path=/usr/bin/fping6 -pxwra -k key-suid-fping6 auditctl -A exit,always -F path=/usr/bin/mtr -pxwra -k key-suid-mtr auditctl -A exit,always -F path=/usr/bin/gpasswd -pxwra -k key-suid-gpasswd auditctl -A exit,always -F path=/usr/bin/sudo -pxwra -k key-suid-sudo auditctl -A exit,always -F path=/usr/bin/lppasswd -pxwra -k key-suid-lppasswd auditctl -A exit,always -F path=/usr/bin/newgrp -pxwra -k key-suid-newgrp auditctl -A exit,always -F path=/home/guess/.pia_manager/openvpn_launcher.64 -pxwra -k key-suid-openvpn_launcher.64 auditctl -A exit,always -F path=/home/guess/.pia_manager/openvpn_launcher.32 -pxwra -k key-suid-openvpn_launcher.32 auditctl -A exit,always -F path=/bin/mount -pxwra -k key-suid-mount auditctl -A exit,always -F path=/bin/ping6 -pxwra -k key-suid-ping6 auditctl -A exit,always -F path=/bin/fusermount -pxwra -k key-suid-fusermount auditctl -A exit,always -F path=/bin/umount -pxwra -k key-suid-umount auditctl -A exit,always -F path=/bin/su -pxwra -k key-suid-su auditctl -A exit,always -F path=/bin/ping -pxwra -k key-suid-ping restart the auditd daemon service auditd restart tail -f /var/log/audit/audit.log some extra audit rules to monitor changes to the file system. Warning these will be very noisy on a busy system -a always,exit -F arch=b32 -S chmod -F auid>=500 -F auid!=4294967295 -k perm_mod -a always,exit -F path=/bin/chmod -F auid>=500 -F auid!=4294967295 -k perm_mod -a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete -a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access -a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access (host=example1np OR host=example2np) source=\"/var/log/audit/audit.log\" index=\"os_secure\" host=example1np source=\"/var/log/audit/audit.log\" index=\"os_secure\" retrieving info from the audit system aureport Summary Report ====================== Range of time in logs: 5/08/2015 15:52:22 - 5/08/2015 18:43:12 Selected time for report: 5/08/2015 15:52:22 - 5/08/2015 18:43:12 Number of changes in configuration: 0 Number of changes to accounts, groups, or roles: 0 Number of logins: 0 Number of failed logins: 0 Number of authentications: 17 Number of failed authentications: 0 Number of users: 2 Number of terminals: 2 Number of host names: 2 Number of executables: 25 Number of files: 28649 Number of AVC's: 0 Number of MAC events: 0 Number of failed syscalls: 512 Number of anomaly events: 0 Number of responses to anomaly events: 0 Number of crypto events: 39 Number of keys: 5 Number of process IDs: 1234 Number of events: 23455 defining a time range aureport -ts 12/06/2013 09:00:09.082 -te 12/06/2013 10:21:27.308","title":"Auditd rules"},{"location":"linux/Firewalls_and_Security/auditd_rules/#using-auuditd-and-aureport-on-rhel-to-track-whats-going-on","text":"Generate list of SUID programs for SUID_BIN in `find / -xdev \\( -perm -4000 \\) -type f -print` do AUDIT_CMD=\"auditctl -A exit,always -F path=\" AUDIT_PERM=\"-pxwra\" KEY=`echo $SUID_BIN | awk -F\\/ '{ print \"-k key-suid-\"$NF }'` echo $AUDIT_CMD$SUID_BIN\" \"$AUDIT_PERM $KEY done Backup the existing audit rules cd /etc/audit/ cp audit.rules audit.rules.orig && vi audit.rules Sample audit rules #Attempts to perform unauthorised functions #-a exit,always -F arch=b64 -F success!=0 -S open -k key-fopen-failure #Additions, deletions and modifications to security/audit log parameters #-a exit,always -F dir=/var/log -F perm=wa -k key-writes-access-var-log #Critical file changes #Activity performed by privileged accounts Modifications to system settings (parameters) -w /etc/ -p w -k key-file-change-etc #The authority and access to use advanced operating system utilities and commands that bypass system access controls must be monitored, logged, reviewed and restricted to those individuals who require access to perform their job functions. auditctl -A exit,always -F path=/opt/google/chrome/chrome-sandbox -pxwra -k key-suid-chrome-sandbox auditctl -A exit,always -F path=/usr/sbin/pppd -pxwra -k key-suid-pppd auditctl -A exit,always -F path=/usr/sbin/uuidd -pxwra -k key-suid-uuidd auditctl -A exit,always -F path=/usr/lib/dbus-1.0/dbus-daemon-launch-helper -pxwra -k key-suid-dbus-daemon-launch-helper auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxSDL -pxwra -k key-suid-VBoxSDL auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxNetAdpCtl -pxwra -k key-suid-VBoxNetAdpCtl auditctl -A exit,always -F path=/usr/lib/virtualbox/VirtualBox -pxwra -k key-suid-VirtualBox auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxHeadless -pxwra -k key-suid-VBoxHeadless auditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxNetDHCP -pxwra -k key-suid-VBoxNetDHCP auditctl -A exit,always -F path=/usr/lib/x86_64-linux-gnu/oxide-qt/chrome-sandbox -pxwra -k key-suid-chrome-sandbox auditctl -A exit,always -F path=/usr/lib/policykit-1/polkit-agent-helper-1 -pxwra -k key-suid-polkit-agent-helper-1 auditctl -A exit,always -F path=/usr/lib/eject/dmcrypt-get-device -pxwra -k key-suid-dmcrypt-get-device auditctl -A exit,always -F path=/usr/lib/pt_chown -pxwra -k key-suid-pt_chown auditctl -A exit,always -F path=/usr/lib/openssh/ssh-keysign -pxwra -k key-suid-ssh-keysign auditctl -A exit,always -F path=/usr/bin/traceroute6.iputils -pxwra -k key-suid-traceroute6.iputils auditctl -A exit,always -F path=/usr/bin/chfn -pxwra -k key-suid-chfn auditctl -A exit,always -F path=/usr/bin/pkexec -pxwra -k key-suid-pkexec auditctl -A exit,always -F path=/usr/bin/passwd -pxwra -k key-suid-passwd auditctl -A exit,always -F path=/usr/bin/X -pxwra -k key-suid-X auditctl -A exit,always -F path=/usr/bin/fping -pxwra -k key-suid-fping auditctl -A exit,always -F path=/usr/bin/chsh -pxwra -k key-suid-chsh auditctl -A exit,always -F path=/usr/bin/fping6 -pxwra -k key-suid-fping6 auditctl -A exit,always -F path=/usr/bin/mtr -pxwra -k key-suid-mtr auditctl -A exit,always -F path=/usr/bin/gpasswd -pxwra -k key-suid-gpasswd auditctl -A exit,always -F path=/usr/bin/sudo -pxwra -k key-suid-sudo auditctl -A exit,always -F path=/usr/bin/lppasswd -pxwra -k key-suid-lppasswd auditctl -A exit,always -F path=/usr/bin/newgrp -pxwra -k key-suid-newgrp auditctl -A exit,always -F path=/home/guess/.pia_manager/openvpn_launcher.64 -pxwra -k key-suid-openvpn_launcher.64 auditctl -A exit,always -F path=/home/guess/.pia_manager/openvpn_launcher.32 -pxwra -k key-suid-openvpn_launcher.32 auditctl -A exit,always -F path=/bin/mount -pxwra -k key-suid-mount auditctl -A exit,always -F path=/bin/ping6 -pxwra -k key-suid-ping6 auditctl -A exit,always -F path=/bin/fusermount -pxwra -k key-suid-fusermount auditctl -A exit,always -F path=/bin/umount -pxwra -k key-suid-umount auditctl -A exit,always -F path=/bin/su -pxwra -k key-suid-su auditctl -A exit,always -F path=/bin/ping -pxwra -k key-suid-ping","title":"Using auuditd and aureport on RHEL to track whats going on."},{"location":"linux/Firewalls_and_Security/auditd_rules/#restart-the-auditd-daemon","text":"service auditd restart tail -f /var/log/audit/audit.log","title":"restart the auditd daemon"},{"location":"linux/Firewalls_and_Security/auditd_rules/#some-extra-audit-rules-to-monitor-changes-to-the-file-system-warning-these-will-be-very-noisy-on-a-busy-system","text":"-a always,exit -F arch=b32 -S chmod -F auid>=500 -F auid!=4294967295 -k perm_mod -a always,exit -F path=/bin/chmod -F auid>=500 -F auid!=4294967295 -k perm_mod -a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete -a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access -a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access (host=example1np OR host=example2np) source=\"/var/log/audit/audit.log\" index=\"os_secure\" host=example1np source=\"/var/log/audit/audit.log\" index=\"os_secure\"","title":"some extra audit rules to monitor changes to the file system. Warning these will be very noisy on a busy system"},{"location":"linux/Firewalls_and_Security/auditd_rules/#retrieving-info-from-the-audit-system","text":"aureport Summary Report ====================== Range of time in logs: 5/08/2015 15:52:22 - 5/08/2015 18:43:12 Selected time for report: 5/08/2015 15:52:22 - 5/08/2015 18:43:12 Number of changes in configuration: 0 Number of changes to accounts, groups, or roles: 0 Number of logins: 0 Number of failed logins: 0 Number of authentications: 17 Number of failed authentications: 0 Number of users: 2 Number of terminals: 2 Number of host names: 2 Number of executables: 25 Number of files: 28649 Number of AVC's: 0 Number of MAC events: 0 Number of failed syscalls: 512 Number of anomaly events: 0 Number of responses to anomaly events: 0 Number of crypto events: 39 Number of keys: 5 Number of process IDs: 1234 Number of events: 23455 defining a time range aureport -ts 12/06/2013 09:00:09.082 -te 12/06/2013 10:21:27.308","title":"retrieving info from the audit system"},{"location":"linux/Firewalls_and_Security/iptables_notes/","text":"My notes on IPtables I cant remember where I copied and pasted this from.. The packet processing is called net filter the user space commend is called iptables hook points from the kernel FORWARD INPUT OUTPUT POSTROUTING PREROUTING built in tables RAW FILTER NAT MANGLE If you don't specify a table then the FILTER table is assumed ipchains by default each table has chains which are initially empty both the match and target portion of the rule are optional if there is no match criteria then all packets match if the is no target then all packets pass through TARGETS are used to specify what action to take when a packet matches a rule the are 4 default TARGETs built in * ACCEPT * DROP * QUEUE * RETURN IP filtering terms and expressions To fully understand the upcoming chapters there are a few general terms and expressions that one must understand, including a lot of details regarding the TCP/IP chapter. This is a listing of the most common terms used in IP filtering. Drop/Deny - When a packet is dropped or denied, it is simply deleted, and no further actions are taken. No reply to tell the host it was dropped, nor is the receiving host of the packet notified in any way. The packet simply disappears. Reject - This is basically the same as a drop or deny target or policy, except that we also send a reply to the host sending the packet that was dropped. The reply may be specified, or automatically calculated to some value. (To this date, there is unfortunately no iptables functionality to also send a packet notifying the receiving host of the rejected packet what happened (ie, doing the reverse of the Reject target). This would be very good in certain circumstances, since the receiving host has no ability to stop Denial of Service attacks from happening.) State - A specific state of a packet in comparison to a whole stream of packets. For example, if the packet is the first that the firewall sees or knows about, it is considered new (the SYN packet in a TCP connection), or if it is part of an already established connection that the firewall knows about, it is considered to be established. States are known through the connection tracking system, which keeps track of all the sessions. Chain - A chain contains a ruleset of rules that are applied on packets that traverses the chain. Each chain has a specific purpose (e.g., which table it is connected to, which specifies what this chain is able to do), as well as a specific application area (e.g., only forwarded packets, or only packets destined for this host). In iptables, there are several different chains, which will be discussed in depth in later chapters. Table - Each table has a specific purpose, and in iptables there are 4 tables. The raw, nat, mangle and filter tables. For example, the filter table is specifically designed to filter packets, while the nat table is specifically designed to NAT (Network Address Translation) packets. Match - This word can have two different meanings when it comes to IP filtering. The first meaning would be a single match that tells a rule that this header must contain this and this information. For example, the --source match tells us that the source address must be a specific network range or host address. The second meaning is if a whole rule is a match. If the packet matches the whole rule, the jump or target instructions will be carried out (e.g., the packet will be dropped.) Target - There is generally a target set for each rule in a ruleset. If the rule has matched fully, the target specification tells us what to do with the packet. For example, if we should drop or accept it, or NAT it, etc. There is also something called a jump specification, for more information see the jump description in this list. As a last note, there might not be a target or jump for each rule, but there may be. Rule - A rule is a set of a match or several matches together with a single target in most implementations of IP filters, including the iptables implementation. There are some implementations which let you use several targets/actions per rule. Ruleset - A ruleset is the complete set of rules that are put into a whole IP filter implementation. In the case of iptables, this includes all of the rules set in the filter, nat, raw and mangle tables, and in all of the subsequent chains. Most of the time, they are written down in a configuration file of some sort. Jump - The jump instruction is closely related to a target. A jump instruction is written exactly the same as a target in iptables, with the exception that instead of writing a target name, you write the name of another chain. If the rule matches, the packet will hence be sent to this second chain and be processed as usual in that chain. Connection tracking - A firewall which implements connection tracking is able to track connections/streams simply put. The ability to do so is often done at the impact of lots of processor and memory usage. This is unfortunately true in iptables as well, but much work has been done to work on this. However, the good side is that the firewall will be much more secure with connection tracking properly used by the implementer of the firewall policies. Accept - To accept a packet and to let it through the firewall rules. This is the opposite of the drop or deny targets, as well as the reject target. Policy - There are two kinds of policies that we speak about most of the time when implementing a firewall. First we have the chain policies, which tells the firewall implementation the default behaviour to take on a packet if there was no rule that matched it. This is the main usage of the word that we will use in this book. The second type of policy is the security policy that we may have written documentation on, for example for the whole company or for this specific network segment. Security policies are very good documents to have thought through properly and to study properly before starting to actually implement the firewall.","title":"My notes on IPtables"},{"location":"linux/Firewalls_and_Security/iptables_notes/#my-notes-on-iptables","text":"I cant remember where I copied and pasted this from.. The packet processing is called net filter the user space commend is called iptables hook points from the kernel FORWARD INPUT OUTPUT POSTROUTING PREROUTING built in tables RAW FILTER NAT MANGLE If you don't specify a table then the FILTER table is assumed ipchains by default each table has chains which are initially empty both the match and target portion of the rule are optional if there is no match criteria then all packets match if the is no target then all packets pass through TARGETS are used to specify what action to take when a packet matches a rule the are 4 default TARGETs built in * ACCEPT * DROP * QUEUE * RETURN IP filtering terms and expressions To fully understand the upcoming chapters there are a few general terms and expressions that one must understand, including a lot of details regarding the TCP/IP chapter. This is a listing of the most common terms used in IP filtering. Drop/Deny - When a packet is dropped or denied, it is simply deleted, and no further actions are taken. No reply to tell the host it was dropped, nor is the receiving host of the packet notified in any way. The packet simply disappears. Reject - This is basically the same as a drop or deny target or policy, except that we also send a reply to the host sending the packet that was dropped. The reply may be specified, or automatically calculated to some value. (To this date, there is unfortunately no iptables functionality to also send a packet notifying the receiving host of the rejected packet what happened (ie, doing the reverse of the Reject target). This would be very good in certain circumstances, since the receiving host has no ability to stop Denial of Service attacks from happening.) State - A specific state of a packet in comparison to a whole stream of packets. For example, if the packet is the first that the firewall sees or knows about, it is considered new (the SYN packet in a TCP connection), or if it is part of an already established connection that the firewall knows about, it is considered to be established. States are known through the connection tracking system, which keeps track of all the sessions. Chain - A chain contains a ruleset of rules that are applied on packets that traverses the chain. Each chain has a specific purpose (e.g., which table it is connected to, which specifies what this chain is able to do), as well as a specific application area (e.g., only forwarded packets, or only packets destined for this host). In iptables, there are several different chains, which will be discussed in depth in later chapters. Table - Each table has a specific purpose, and in iptables there are 4 tables. The raw, nat, mangle and filter tables. For example, the filter table is specifically designed to filter packets, while the nat table is specifically designed to NAT (Network Address Translation) packets. Match - This word can have two different meanings when it comes to IP filtering. The first meaning would be a single match that tells a rule that this header must contain this and this information. For example, the --source match tells us that the source address must be a specific network range or host address. The second meaning is if a whole rule is a match. If the packet matches the whole rule, the jump or target instructions will be carried out (e.g., the packet will be dropped.) Target - There is generally a target set for each rule in a ruleset. If the rule has matched fully, the target specification tells us what to do with the packet. For example, if we should drop or accept it, or NAT it, etc. There is also something called a jump specification, for more information see the jump description in this list. As a last note, there might not be a target or jump for each rule, but there may be. Rule - A rule is a set of a match or several matches together with a single target in most implementations of IP filters, including the iptables implementation. There are some implementations which let you use several targets/actions per rule. Ruleset - A ruleset is the complete set of rules that are put into a whole IP filter implementation. In the case of iptables, this includes all of the rules set in the filter, nat, raw and mangle tables, and in all of the subsequent chains. Most of the time, they are written down in a configuration file of some sort. Jump - The jump instruction is closely related to a target. A jump instruction is written exactly the same as a target in iptables, with the exception that instead of writing a target name, you write the name of another chain. If the rule matches, the packet will hence be sent to this second chain and be processed as usual in that chain. Connection tracking - A firewall which implements connection tracking is able to track connections/streams simply put. The ability to do so is often done at the impact of lots of processor and memory usage. This is unfortunately true in iptables as well, but much work has been done to work on this. However, the good side is that the firewall will be much more secure with connection tracking properly used by the implementer of the firewall policies. Accept - To accept a packet and to let it through the firewall rules. This is the opposite of the drop or deny targets, as well as the reject target. Policy - There are two kinds of policies that we speak about most of the time when implementing a firewall. First we have the chain policies, which tells the firewall implementation the default behaviour to take on a packet if there was no rule that matched it. This is the main usage of the word that we will use in this book. The second type of policy is the security policy that we may have written documentation on, for example for the whole company or for this specific network segment. Security policies are very good documents to have thought through properly and to study properly before starting to actually implement the firewall.","title":"My notes on IPtables"},{"location":"linux/Monitoring/nagios_2.6_install/","text":"Seamus's notes on an Install Nagios 2.6 for Suse 10 Solaris 9 and HPUX Nagios is a scheduling Engine and nothing more! 1) Description of Nagios This is the way it was designed from the out set. The idea is to keep the core of Nagios as simple as possible, and therefore as flexible as possible. Nagios it self cannot probe, test, email or SMS by itself it relies on other programs/plugins for all of its functions. The 3 main category's of plugins are listed below. NRPE allows you to remotely execute Nagios plugins on other Linux/Unix machines. NSCA allows you to integrate passive alerts and checks from remote machines and applications with Nagios. *Nagios plugins, there are literally thousands of 3rd party Nagios plugins that cover everything from simple ping's to logging into oracle and performing database queries. The Nagios install usually comes with a default set of plugins. 2) Overview of Nagios' configuration Nagios's keeps its configuration in flat text files. It is left up to you (the Admin) to manage these file's by hand. You can have all the config data in a single file or you can split it up with files per host, per service etc.. After you have a few hosts configured. It becomes tedious and time consuming to manually manage all the interdependencies of the various configuration values. eg if you remove a host from one file you then must go through all the service and alert config files and remove it from there. To see how complicated the configuration interdependencies are click the following link. http://nagios.sourceforge.net/download/contrib/documentation/misc/config_diagrams/nagios-config.png 3) Overview of NagiosQL Due to the complexity of the config files. I chose to use a php/MySQL frontend called NagiosQL to manage all the config files on my behalf. There are a number of current and orphaned Nagios front end projects I chose NagiosQL because it seemed to concentrate on just getting the job done. With NagiosQL all the configuration information is stored in a MySQL database. All updates are performed via web forms. Java scripting is used to perform some form validation. Once you have made all your changes you click the write config files link and all the local config files are over written with the new config from the database. You will have to restart Nagios for the new config to take effect. You can use either of the 3 following methods.... command line..... /etc/init.d/nagios reload nagios CGI..... lefthand panel --> process info --> Restart the Nagios process ++ nagiosQL..... lefthand panel --> Tools --> Nagios control --> Restart Nagios ++ END of Nagios and NagiosQL Intro Installation of Nagios and Nagios QL List of packages I installed on the nagios server example.com. nagios-nsca-2.6-7.6 nagios-plugins-1.4.5-16.6 nagios-2.6-13.7 nagios-nsca-client-2.6-7.6 nagios-plugins-extras-1.4.5-16.6 nagios-www-2.6-13.7 nagios-nrpe-2.12-1 nagiosQL-2.0.2-1.1 N.B. the reason why i chose nagios 2.x over nagios 3.x is because there are no Mysql frontends for 3.x yet. Steps I took to install Nagios server on SUSE 10.......... Install Nagios and associated dependencies via YAST Install nrpe daemon SUSE doesn't come with the nrpe daemon. So you can either compile it yourself or find a package provided by a a 3rd party. I chose to use an rpm provided by..... http://blog.barfoo.org/2008/07/14/latest-sles10-rpm-additions rpm -ivh nagios-nrpe-2.12-1.i586.rpm Once nagios nrpe and all the plugins have been installed you have to setup the permissions for the web interface. create the /usr/lib/nagios/cgi/.htaccess file with the following AuthName \"Nagios Access\" AuthType Basic AuthUserFile /etc/nagios/.htpasswd.users require valid-user create the password file with /usr/sbin/htpasswd2 -c /etc/nagios/.htpasswd.users {user name} restart Apache and login to the web page to test http://hostname/nagios OK Nagios has now been installed. Install mysql from YAST start mysql and setup a password for the root account... /usr/bin/mysqladmin -u root password 'new-password' /usr/bin/mysqladmin -u root -h example.com password 'new-password' Before you install NagiosQL, make you you have meet the following requirements..... Webserver (Apache 1.3.x/2.0.x) PHP Version 4.1 or higher / 5.0 or higher MySQL Version 4.1 or higher / 5.0 or higher Pear Module HTML_Template_IT Version 1.1 (http://pear.php.net) Nagios Version 2.x (1.x is not supported) Javascript supported by your browser Cookies accepted by your browser download the nagiosql front end and php-pear-HTML_Template_IT from wget http://download.opensuse.org/repositories/home:/rhomann/SLE_10/noarch/nagiosQL-2.0.2-1.1.noarch.rpm wget http://download.opensuse.org/repositories/home:/rhomann/SLE_10/noarch/php5-pear-HTML_Template_IT-1.2.1-4.1.noarch.rpm Install the rpms rpm -ivh php5-pear-HTML_Template_IT-1.2.1-4.1.noarch.rpm rpm -ivh nagiosQL-2.0.2-1.1.noarch.rpm Start the mysql database engine. /etc/init.d/mysqld start populate MySQL with data for nagiosQL : mysql -u root -p < /usr/share/nagiosQL/config/nagiosQL_v2_db_mysql.sql For some reason the SQL script above didn't create the user account so I had to perform this step manually... mysql -u root -p mysql> use mysql Database changed mysql> GRANT USAGE ON *.* TO 'nagiosQLusr'@'localhost' IDENTIFIED BY 'nagiosQLpwd'; Query OK, 0 rows affected (0.01 sec) mysql> GRANT SELECT,INSERT,UPDATE,DELETE ON `nagiosQL`.* TO'nagiosQLusr'@'localhost'; Query OK, 0 rows affected (0.00 sec) By default Nagios stores all its configuration information is just 2 files, NagiosQL prefers that you spread the configuration info across multiple files.. You have to manually create directories so that NagiosQL can create the config files in the specified directories. You will also have to make changes to the /etc/nagios/nagios.cfg file to reflect these changes. ~ In my case apache ran as user wwwrun and group daemon ~ mkdir /etc/nagios/hosts mkdir /etc/nagios/services mkdir /etc/nagios/backup mkdir /etc/nagios/backup/hosts mkdir /etc/nagios/backup/services chmod 6755 /etc/nagios chown wwwrun.daemon /etc/nagios chmod 6755 /etc/nagios/hosts chown wwwrun.daemon /etc/nagios/hosts chmod 6755 /etc/nagios/services chown wwwrun.daemon /etc/nagios/services chmod 6755 /etc/nagios/backup chown wwwrun.daemon /etc/nagios/backup chmod 6755 /etc/nagios/backup/hosts chown wwwrun.daemon /etc/nagios/backup/hosts chmod 6755 /etc/nagios/backup/services chown wwwrun.daemon /etc/nagios/backup/services chmod 644 /etc/nagios/*.cfg chown wwwrun.daemon /etc/nagios/*.cfg communication from the web interface to the nagios daemon is performed via a named pipe this allows to to trigger nagios to reread its config files or disable alerts etc.. create command file and set permissions, note the location of this file is specified in the /etc/nagios.cfg touch /var/spool/nagios/nagios.cmd chown nagios.www /var/spool/nagios/nagios.cmd chmod 660 /var/spool/nagios/nagios.cmd edit the /etc/php5/apache2/php.ini config file to allow magicquotes magic_quotes_gpc = On set language to english .... lang = lang_en run http://example.com/nagiosQL/testQL.php to test the configuration Once you have all this working nagios will be able to perform external probes of its clients check_alive # ping check_http # check if a web server is up If you only wish to perform external probes there is no need to install any extra software on the (monitored hosts) . If you wish to check things such as disk usage number of processes users etc you will need to install an agent on the (monitored hosts) below details the steps i took to get nrpe working on the (monitored hosts). End of Installation of Nagios and Nagios QL Start of Client Installation section CLIENTS SUSE RPM dependency order for the nrpe daemon and plugins. rpm -ivh perl-Crypt-DES-2.05-3.2.el4.rf.i386.rpm rpm -ivh perl-Socket6-0.20-1.el4.rf.i386.rpm rpm -ivh perl-Net-SNMP-5.2.0-1.2.el4.rf.noarch.rpm rpm -ivh fping-2.4-1.b2.2.el4.rf.i386.rpm rpm -ivh nagios-plugins-1.4.9-1.el4.rf.i386.rpm rpm -ivh nagios-nrpe-2.5.2-1.el4.rf.i386.rpm HP-UX Neither HP or the Nagios team provide runtime binary's. Getting the nrpe agent to work on HPUX requires you to either compile from source or use 3rd party binaries. Unfortunately I never managed to get ssl to work under HPUX so from the nagios server I have to use the -n (noSSL) option with nrpe. eg nrpe_check -n -H mailhost.example.com -c check_load We had the nrpe daemon for hpux-Itanium on a cd, i don't know where it came from i think it was part of \"HP's Internet Express repository\" it may have come from here http://mayoxide.com/naghpux/ The PA_RISC nrep daemon can be downloaded from here http://www.bennyvision.com/projects/nagios/index.php Solaris 9 Neither SUN or the Nagios team provide runtime binary's. Instead of compiling from source I chose to go the lazy route of using binary packages provided by www.blastwave.org/packages.php/nrpe . To install this package required a vast number of dependant packages. I used the automated dependency system to do the hard work for me. Fist this required the installation of.. pkg-get I first downloaded a self contained binary of wget http://blastwave.network.com/csw/wget-sparc.bin I then used this binary to (w)get the pkg-get wget http://blastwave.network.com/csw/pkg_get.pkg pkgadd -d pkg_get.pkg #install the package vi /opt/csw/etc/pkg-get.conf #edit the http proxy value and enable the export proxy directive pkg-get -U #download the latest list of packages and their interdependencies pkg-get -i wget #install the full version of wget Now for the big install... pkg-get -i nrpe # this will require many dependant packages, just say yes to all . once this a has finished you will have an installation of nrpe and all the plugins. you can test the plugins by running them directly /opt/csw/libexec/nagios-plugins/check_load -w 15,10,5 -c 30,25,20 /opt/csw/libexec/nagios-plugins/check_dummy 0 If this works fine, you can then configure the nrpe daemon about the only changes you will have to /opt/csw/etc/nrpe.cfg are to allow the nrpe daemon to accept connections from the nagios server allowed_hosts=127.0.0.1,192.168.125.42 and to allow command argument processing dont_blame_nrpe=0 # set to 1 if you want to enable you can now start the daemon manually by running /opt/csw/bin/nrpe -d -c /opt/csw/etc/nrpe.cfg then from a remote host check the daemon by running one of the command that is enabled in the target hosts nrpe.cfg file eg.... /usr/lib/nagios/plugins/check_nrpe -H {target host} -c check_load -a -w 1,1,1,2,2,2 Once this is working kill the daemon and set it up via inet.d add the following line to /etc/inetd.conf nrpe stream tcp nowait nagios /opt/csw/bin/nrpe /opt/csw/bin/nrpe -i -c /opt/csw/etc/nrpe.cfg add the following line to /etc/services nrpe 5666/tcp force the inet daemon to reread the config files pkill -HUP inetd run the remote check again End of Client install section","title":"Nagios 2.6 install"},{"location":"linux/Monitoring/nagios_2.6_install/#nagios-is-a-scheduling-engine-and-nothing-more","text":"1) Description of Nagios This is the way it was designed from the out set. The idea is to keep the core of Nagios as simple as possible, and therefore as flexible as possible. Nagios it self cannot probe, test, email or SMS by itself it relies on other programs/plugins for all of its functions. The 3 main category's of plugins are listed below. NRPE allows you to remotely execute Nagios plugins on other Linux/Unix machines. NSCA allows you to integrate passive alerts and checks from remote machines and applications with Nagios. *Nagios plugins, there are literally thousands of 3rd party Nagios plugins that cover everything from simple ping's to logging into oracle and performing database queries. The Nagios install usually comes with a default set of plugins. 2) Overview of Nagios' configuration Nagios's keeps its configuration in flat text files. It is left up to you (the Admin) to manage these file's by hand. You can have all the config data in a single file or you can split it up with files per host, per service etc.. After you have a few hosts configured. It becomes tedious and time consuming to manually manage all the interdependencies of the various configuration values. eg if you remove a host from one file you then must go through all the service and alert config files and remove it from there. To see how complicated the configuration interdependencies are click the following link. http://nagios.sourceforge.net/download/contrib/documentation/misc/config_diagrams/nagios-config.png 3) Overview of NagiosQL Due to the complexity of the config files. I chose to use a php/MySQL frontend called NagiosQL to manage all the config files on my behalf. There are a number of current and orphaned Nagios front end projects I chose NagiosQL because it seemed to concentrate on just getting the job done. With NagiosQL all the configuration information is stored in a MySQL database. All updates are performed via web forms. Java scripting is used to perform some form validation. Once you have made all your changes you click the write config files link and all the local config files are over written with the new config from the database. You will have to restart Nagios for the new config to take effect. You can use either of the 3 following methods.... command line..... /etc/init.d/nagios reload nagios CGI..... lefthand panel --> process info --> Restart the Nagios process ++ nagiosQL..... lefthand panel --> Tools --> Nagios control --> Restart Nagios ++ END of Nagios and NagiosQL Intro Installation of Nagios and Nagios QL List of packages I installed on the nagios server example.com. nagios-nsca-2.6-7.6 nagios-plugins-1.4.5-16.6 nagios-2.6-13.7 nagios-nsca-client-2.6-7.6 nagios-plugins-extras-1.4.5-16.6 nagios-www-2.6-13.7 nagios-nrpe-2.12-1 nagiosQL-2.0.2-1.1 N.B. the reason why i chose nagios 2.x over nagios 3.x is because there are no Mysql frontends for 3.x yet. Steps I took to install Nagios server on SUSE 10.......... Install Nagios and associated dependencies via YAST Install nrpe daemon SUSE doesn't come with the nrpe daemon. So you can either compile it yourself or find a package provided by a a 3rd party. I chose to use an rpm provided by..... http://blog.barfoo.org/2008/07/14/latest-sles10-rpm-additions rpm -ivh nagios-nrpe-2.12-1.i586.rpm Once nagios nrpe and all the plugins have been installed you have to setup the permissions for the web interface. create the /usr/lib/nagios/cgi/.htaccess file with the following AuthName \"Nagios Access\" AuthType Basic AuthUserFile /etc/nagios/.htpasswd.users require valid-user create the password file with /usr/sbin/htpasswd2 -c /etc/nagios/.htpasswd.users {user name} restart Apache and login to the web page to test http://hostname/nagios","title":"Nagios is a scheduling Engine and nothing more!"},{"location":"linux/Monitoring/nagios_2.6_install/#ok-nagios-has-now-been-installed","text":"","title":"OK Nagios has now been installed."},{"location":"linux/Monitoring/nagios_2.6_install/#install-mysql-from-yast","text":"start mysql and setup a password for the root account... /usr/bin/mysqladmin -u root password 'new-password' /usr/bin/mysqladmin -u root -h example.com password 'new-password' Before you install NagiosQL, make you you have meet the following requirements..... Webserver (Apache 1.3.x/2.0.x) PHP Version 4.1 or higher / 5.0 or higher MySQL Version 4.1 or higher / 5.0 or higher Pear Module HTML_Template_IT Version 1.1 (http://pear.php.net) Nagios Version 2.x (1.x is not supported) Javascript supported by your browser Cookies accepted by your browser download the nagiosql front end and php-pear-HTML_Template_IT from wget http://download.opensuse.org/repositories/home:/rhomann/SLE_10/noarch/nagiosQL-2.0.2-1.1.noarch.rpm wget http://download.opensuse.org/repositories/home:/rhomann/SLE_10/noarch/php5-pear-HTML_Template_IT-1.2.1-4.1.noarch.rpm Install the rpms rpm -ivh php5-pear-HTML_Template_IT-1.2.1-4.1.noarch.rpm rpm -ivh nagiosQL-2.0.2-1.1.noarch.rpm Start the mysql database engine. /etc/init.d/mysqld start populate MySQL with data for nagiosQL : mysql -u root -p < /usr/share/nagiosQL/config/nagiosQL_v2_db_mysql.sql For some reason the SQL script above didn't create the user account so I had to perform this step manually... mysql -u root -p mysql> use mysql Database changed mysql> GRANT USAGE ON *.* TO 'nagiosQLusr'@'localhost' IDENTIFIED BY 'nagiosQLpwd'; Query OK, 0 rows affected (0.01 sec) mysql> GRANT SELECT,INSERT,UPDATE,DELETE ON `nagiosQL`.* TO'nagiosQLusr'@'localhost'; Query OK, 0 rows affected (0.00 sec) By default Nagios stores all its configuration information is just 2 files, NagiosQL prefers that you spread the configuration info across multiple files.. You have to manually create directories so that NagiosQL can create the config files in the specified directories. You will also have to make changes to the /etc/nagios/nagios.cfg file to reflect these changes. ~ In my case apache ran as user wwwrun and group daemon ~ mkdir /etc/nagios/hosts mkdir /etc/nagios/services mkdir /etc/nagios/backup mkdir /etc/nagios/backup/hosts mkdir /etc/nagios/backup/services chmod 6755 /etc/nagios chown wwwrun.daemon /etc/nagios chmod 6755 /etc/nagios/hosts chown wwwrun.daemon /etc/nagios/hosts chmod 6755 /etc/nagios/services chown wwwrun.daemon /etc/nagios/services chmod 6755 /etc/nagios/backup chown wwwrun.daemon /etc/nagios/backup chmod 6755 /etc/nagios/backup/hosts chown wwwrun.daemon /etc/nagios/backup/hosts chmod 6755 /etc/nagios/backup/services chown wwwrun.daemon /etc/nagios/backup/services chmod 644 /etc/nagios/*.cfg chown wwwrun.daemon /etc/nagios/*.cfg communication from the web interface to the nagios daemon is performed via a named pipe this allows to to trigger nagios to reread its config files or disable alerts etc.. create command file and set permissions, note the location of this file is specified in the /etc/nagios.cfg touch /var/spool/nagios/nagios.cmd chown nagios.www /var/spool/nagios/nagios.cmd chmod 660 /var/spool/nagios/nagios.cmd edit the /etc/php5/apache2/php.ini config file to allow magicquotes magic_quotes_gpc = On set language to english .... lang = lang_en run http://example.com/nagiosQL/testQL.php to test the configuration Once you have all this working nagios will be able to perform external probes of its clients check_alive # ping check_http # check if a web server is up If you only wish to perform external probes there is no need to install any extra software on the (monitored hosts) . If you wish to check things such as disk usage number of processes users etc you will need to install an agent on the (monitored hosts) below details the steps i took to get nrpe working on the (monitored hosts). End of Installation of Nagios and Nagios QL Start of Client Installation section CLIENTS SUSE RPM dependency order for the nrpe daemon and plugins. rpm -ivh perl-Crypt-DES-2.05-3.2.el4.rf.i386.rpm rpm -ivh perl-Socket6-0.20-1.el4.rf.i386.rpm rpm -ivh perl-Net-SNMP-5.2.0-1.2.el4.rf.noarch.rpm rpm -ivh fping-2.4-1.b2.2.el4.rf.i386.rpm rpm -ivh nagios-plugins-1.4.9-1.el4.rf.i386.rpm rpm -ivh nagios-nrpe-2.5.2-1.el4.rf.i386.rpm","title":"Install mysql from YAST"},{"location":"linux/Monitoring/nagios_2.6_install/#hp-ux","text":"Neither HP or the Nagios team provide runtime binary's. Getting the nrpe agent to work on HPUX requires you to either compile from source or use 3rd party binaries. Unfortunately I never managed to get ssl to work under HPUX so from the nagios server I have to use the -n (noSSL) option with nrpe. eg nrpe_check -n -H mailhost.example.com -c check_load We had the nrpe daemon for hpux-Itanium on a cd, i don't know where it came from i think it was part of \"HP's Internet Express repository\" it may have come from here http://mayoxide.com/naghpux/ The PA_RISC nrep daemon can be downloaded from here http://www.bennyvision.com/projects/nagios/index.php","title":"HP-UX"},{"location":"linux/Monitoring/nagios_2.6_install/#solaris-9","text":"","title":"Solaris 9"},{"location":"linux/Monitoring/nagios_2.6_install/#neither-sun-or-the-nagios-team-provide-runtime-binarys","text":"Instead of compiling from source I chose to go the lazy route of using binary packages provided by www.blastwave.org/packages.php/nrpe . To install this package required a vast number of dependant packages. I used the automated dependency system to do the hard work for me. Fist this required the installation of.. pkg-get I first downloaded a self contained binary of wget http://blastwave.network.com/csw/wget-sparc.bin I then used this binary to (w)get the pkg-get wget http://blastwave.network.com/csw/pkg_get.pkg pkgadd -d pkg_get.pkg #install the package vi /opt/csw/etc/pkg-get.conf #edit the http proxy value and enable the export proxy directive pkg-get -U #download the latest list of packages and their interdependencies pkg-get -i wget #install the full version of wget Now for the big install... pkg-get -i nrpe # this will require many dependant packages, just say yes to all . once this a has finished you will have an installation of nrpe and all the plugins. you can test the plugins by running them directly /opt/csw/libexec/nagios-plugins/check_load -w 15,10,5 -c 30,25,20 /opt/csw/libexec/nagios-plugins/check_dummy 0 If this works fine, you can then configure the nrpe daemon about the only changes you will have to /opt/csw/etc/nrpe.cfg are to allow the nrpe daemon to accept connections from the nagios server allowed_hosts=127.0.0.1,192.168.125.42 and to allow command argument processing dont_blame_nrpe=0 # set to 1 if you want to enable you can now start the daemon manually by running /opt/csw/bin/nrpe -d -c /opt/csw/etc/nrpe.cfg then from a remote host check the daemon by running one of the command that is enabled in the target hosts nrpe.cfg file eg.... /usr/lib/nagios/plugins/check_nrpe -H {target host} -c check_load -a -w 1,1,1,2,2,2 Once this is working kill the daemon and set it up via inet.d add the following line to /etc/inetd.conf nrpe stream tcp nowait nagios /opt/csw/bin/nrpe /opt/csw/bin/nrpe -i -c /opt/csw/etc/nrpe.cfg add the following line to /etc/services nrpe 5666/tcp force the inet daemon to reread the config files pkill -HUP inetd run the remote check again End of Client install section","title":"Neither SUN or the Nagios team provide runtime binary's."},{"location":"linux/Networking/check_dns_records/","text":"Bash script to check DNS records #!/bin/bash #This script will perform.. # 1. forward lookups on a list of dns shortnames # 2. it will then perform reverse lookups on the IPs returned # 3. it will then perform the same steps again for various sub domains # Note: replace example with your domain name , input a list of either shortnames or FQDNs # assumes there are 3 nics each with its own name, remove the last 2 stanzas if you only have a single nic and domain for short_name in `head list_systems | awk -F. '{ print $1 }'` do domain_name=\".example.local\" host $short_name$domain_name if [ $? = '0' ] ; then fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then bw_ip=\"`host $fw_ip | awk '{ print $5 }'`\" >> out else echo \"$short_name$domain_name reverse failed $fw_ip\" >> errors bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors fw_ip=\"\" fi domain_name=\".subdomain1.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then m_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then m_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$m_fw_ip\",\"$m_bw_ip\",\" >> m_out else echo \"$short_name$domain_name reverse failed $m_fw_ip\" >> m_errors m_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors m_fw_ip=\"\" fi domain_name=\".subdomain2.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then b_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then b_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$b_fw_ip\",\"$b_bw_ip\",\" >> b_out else echo \"$short_name$domain_name reverse failed $b_fw_ip\" >> b_errors b_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors b_fw_ip=\"\" fi echo $short_name$domain_name\",\"$fw_ip\",\"$bw_ip\",\"\"$m_fw_ip\",\"$m_bw_ip\",\"\"$b_fw_ip\",\"$b_bw_ip\",\" >> out done","title":"Check dns records"},{"location":"linux/Networking/check_dns_records/#bash-script-to-check-dns-records","text":"#!/bin/bash #This script will perform.. # 1. forward lookups on a list of dns shortnames # 2. it will then perform reverse lookups on the IPs returned # 3. it will then perform the same steps again for various sub domains # Note: replace example with your domain name , input a list of either shortnames or FQDNs # assumes there are 3 nics each with its own name, remove the last 2 stanzas if you only have a single nic and domain for short_name in `head list_systems | awk -F. '{ print $1 }'` do domain_name=\".example.local\" host $short_name$domain_name if [ $? = '0' ] ; then fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then bw_ip=\"`host $fw_ip | awk '{ print $5 }'`\" >> out else echo \"$short_name$domain_name reverse failed $fw_ip\" >> errors bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors fw_ip=\"\" fi domain_name=\".subdomain1.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then m_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then m_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$m_fw_ip\",\"$m_bw_ip\",\" >> m_out else echo \"$short_name$domain_name reverse failed $m_fw_ip\" >> m_errors m_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors m_fw_ip=\"\" fi domain_name=\".subdomain2.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then b_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then b_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$b_fw_ip\",\"$b_bw_ip\",\" >> b_out else echo \"$short_name$domain_name reverse failed $b_fw_ip\" >> b_errors b_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors b_fw_ip=\"\" fi echo $short_name$domain_name\",\"$fw_ip\",\"$bw_ip\",\"\"$m_fw_ip\",\"$m_bw_ip\",\"\"$b_fw_ip\",\"$b_bw_ip\",\" >> out done","title":"Bash script to check DNS records"},{"location":"linux/Networking/determine_the_routes_a_packet_will_take/","text":"determine the routes a packet will take on a Linux server ip route get to 8.8.8.8 8.8.8.8 via 192.168.0.1 dev eth0 src 192.168.0.6 cache","title":"Determine the routes a packet will take"},{"location":"linux/Networking/linux_networking/","text":"Three methods for displaying the current route table on linux [root@spacewalk ~]# netstat -nr Kernel IP routing table Destination Gateway Genmask Flags MSS Window irtt Iface 10.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth1 0.0.0.0 10.0.2.2 0.0.0.0 UG 0 0 0 eth0 [root@spacewalk ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1 0.0.0.0 10.0.2.2 0.0.0.0 UG 0 0 0 eth0 [root@spacewalk ~]# cat /proc/net/route Iface Destination Gateway Flags RefCnt Use Metric Mask MTU Window IRTT eth0 0002000A 00000000 0001 0 0 0 00FFFFFF 0 0 0 eth1 0038A8C0 00000000 0001 0 0 0 00FFFFFF 0 0 0 eth0 0000FEA9 00000000 0001 0 0 1002 0000FFFF 0 0 0 eth1 0000FEA9 00000000 0001 0 0 1003 0000FFFF 0 0 0 eth0 00000000 0202000A 0003 0 0 0 00000000 0 0 0 Command to determine which route will be used for a certain destination ip route get to 10.10.10.0 ip route get to 192.168.0 Simple bash script to simulate network failure #!/bin/bash ethcard=eth0 MAXWAITTIME=30 NUMBEROFRUNS=100 WAITTIME=10 #initial value over written by random_delay() random_delay() { MAXTIME=30 #5 Minutes delay=$RANDOM let \"delay %= $MAXWAITTIME\" WAITTIME=$delay } random_delay for i in `seq 1 $NUMBEROFRUNS` do random_delay echo \"bringing $ethcard Offline for $WAITTIME seconds\" ifdown $ethcard; sleep $WAITTIME random_delay echo \"bringing $ethcard Online for $WAITTIME seconds\" ifup $ethcard ; sleep $WAITTIME done Determine the routes a packet will take on a linuix server ip route get to 8.8.8.8 8.8.8.8 via 192.168.0.1 dev eth0 src 192.168.0.6 cache","title":"Linux networking"},{"location":"linux/Networking/linux_networking/#three-methods-for-displaying-the-current-route-table-on-linux","text":"[root@spacewalk ~]# netstat -nr Kernel IP routing table Destination Gateway Genmask Flags MSS Window irtt Iface 10.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth1 0.0.0.0 10.0.2.2 0.0.0.0 UG 0 0 0 eth0 [root@spacewalk ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1 0.0.0.0 10.0.2.2 0.0.0.0 UG 0 0 0 eth0 [root@spacewalk ~]# cat /proc/net/route Iface Destination Gateway Flags RefCnt Use Metric Mask MTU Window IRTT eth0 0002000A 00000000 0001 0 0 0 00FFFFFF 0 0 0 eth1 0038A8C0 00000000 0001 0 0 0 00FFFFFF 0 0 0 eth0 0000FEA9 00000000 0001 0 0 1002 0000FFFF 0 0 0 eth1 0000FEA9 00000000 0001 0 0 1003 0000FFFF 0 0 0 eth0 00000000 0202000A 0003 0 0 0 00000000 0 0 0","title":"Three methods for displaying the current route table on linux"},{"location":"linux/Networking/linux_networking/#command-to-determine-which-route-will-be-used-for-a-certain-destination","text":"ip route get to 10.10.10.0 ip route get to 192.168.0","title":"Command to determine which route will be used for a certain destination"},{"location":"linux/Networking/linux_networking/#simple-bash-script-to-simulate-network-failure","text":"#!/bin/bash ethcard=eth0 MAXWAITTIME=30 NUMBEROFRUNS=100 WAITTIME=10 #initial value over written by random_delay() random_delay() { MAXTIME=30 #5 Minutes delay=$RANDOM let \"delay %= $MAXWAITTIME\" WAITTIME=$delay } random_delay for i in `seq 1 $NUMBEROFRUNS` do random_delay echo \"bringing $ethcard Offline for $WAITTIME seconds\" ifdown $ethcard; sleep $WAITTIME random_delay echo \"bringing $ethcard Online for $WAITTIME seconds\" ifup $ethcard ; sleep $WAITTIME done","title":"Simple bash script to simulate network failure"},{"location":"linux/Networking/linux_networking/#determine-the-routes-a-packet-will-take-on-a-linuix-server","text":"ip route get to 8.8.8.8 8.8.8.8 via 192.168.0.1 dev eth0 src 192.168.0.6 cache","title":"Determine the routes a packet will take on a linuix server"},{"location":"linux/Networking/local_IP_address_survey/","text":"This was used to create a space separated list of IPs in use on a server. Server_Name eth0 10.10.10.10 Mask:255.255.252.0 Server_Name eth1 10.11.11.11 Mask:255.255.252.0 Server_Name eth2 192.168.0.240 Mask:255.255.255.128 #!/bin/bash HOST_NAME=`hostname -s` NICS=`ls /sys/class/net | grep -v lo` for i in $NICS do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://` done","title":"local IP address survey"},{"location":"linux/RedHat/kickstart/","text":"# kickstart file created by Seamus Nov 29 2006 # The following tasks are AutoMagicly configured with this script # The hard disks will be zeroed but you will be prompted for a partition scheme # User and Group accounts for the unix team will be added # The Unix team will be granted FULL Sudo access # The Message Of The Day will be set # the (CTRL/ALT/DEL) key combination will be disabled on the physical console # Xinetd will be disabled (wont start up automatically) # Various Services will be disabled (wont start up automatically) # Various default redhat user accounts will be deleted # Remote loggin to the root account via SSH will be disabled # The ntpdaemon will be configured started and synced # The mondo packages and configuration files will be installed # The nagios agents will be installed # The strange label created by the installer for the swap partition is changed in the fstab file # A custom grub splash screen is copied # There is a separate file for configuring host for oracle # dont forget that during the post install script phase you can switch to virtual terminal 3 and 4 # and see what the script is doing. This is why there are so many echo's in this file #System language lang en_AU #Language modules to install langsupport en_AU #System keyboard keyboard us #System mouse mouse #Sytem timezone timezone Moon/Crater #Root password rootpw --iscrypted xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx #Reboot after installation reboot #Use text mode install text #Install OS instead of upgrade install #Use NFS installation Media nfs --server=xxx.xxx.com --dir=/u1/Distros/rhes4u3/install #System bootloader configuration bootloader --location=mbr #Clear the Master Boot Record zerombr yes # the following line will cause KickStart to ignor SAN disks during the OS install ignoredisk --drives=sda,sdb,sdc,sdd,sde,sdf,sdg,sdh #Partition clearing information clearpart --all --initlabel part / --fstype ext3 --size 20480 --asprimary part /boot --fstype ext3 --size 150 --asprimary #part swap --size 4096 #part swap --size 8192 part swap --size 5120 part /var --fstype ext3 --size 5120 part /opt --fstype ext3 --size 25600 part /usr --fstype ext3 --size 5120 --asprimary part /tmp --fstype ext3 --size 5120 part /backup --fstype ext3 --size 1 --grow #System authorization infomation auth --useshadow --enablemd5 #Network information network --bootproto=dhcp --device=eth0 #Firewall configuration firewall --disabled #Do not configure XWindows skipx #Package install information %packages --resolvedeps @ base-x @ text-internet #@ ftp-server #@ web-server #@ development-tools #@ admin-tools #@ system-tools kernel-smp -system-config-httpd -webalizer lvm2 grub -postfix -squid -spamassassin -cadaver -fetchmail e2fsprogs mkisofs busybox cdrecord #oracle needs the following binutils compat-db control-center gcc gcc-c++ glibc glibc-common gnome-libs libstdc++ libstdc++-devel make pdksh sysstat xscreensaver libaio # docuemntum needs the following #compat-libstdc++-296 #compat-libstdc++-33 compat-gcc-32 compat-gcc-32-c++ ########### POST CONFIGURATIONS BEGIN AT THIS POINT ############## %post # echo \"setting time\" ntpdate ntp.ntp.com echo \"# disable the startup of unnecessary services\" chkconfig microcode_ctl off chkconfig netfs off chkconfig saslauthd off chkconfig mdmonitor off chkconfig mdmpd off chkconfig irda off chkconfig psacct off chkconfig isdn off chkconfig pcmcia off chkconfig autofs off #chkconfig portmap off #chkconfig nfs off #chkconfig nfslock off chkconfig cups off chkconfig dc_client off chkconfig arptables_jf off chkconfig dc_server off chkconfig bcm5820 off chkconfig squid off chkconfig named off chkconfig tux off chkconfig cups off echo \"removing unnecessary user accounts\" userdel lp userdel sync userdel shutdown userdel halt userdel mail userdel news userdel uucp userdel operator userdel games userdel gopher userdel ftp userdel nscd userdel rpc userdel rpcuser userdel mailnull userdel xfs userdel gdm userdel desktop userdel squid userdel named userdel ldap userdel netdump echo \"#\" echo \"# Disabling SSH ROOT login\" echo \"#\" cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak sed -e \"s/\\#PermitRootLogin/PermitRootLogin\\ no\\#/\" /etc/ssh/sshd_config.bak > /etc/ssh/sshd_config echo;echo echo \"# Creating /etc/ntp.conf file...\" cp /etc/ntp.conf /etc/ntp.conf.bak cat <<EOF >/etc/ntp.conf # this file was created by the XXXX Custom kickstart script # Permit time synchronization with our time source, but do not # permit the source to query or modify the service on this system. restrict default nomodify notrap noquery restrict 127.0.0.1 driftfile /var/lib/ntp/drift broadcastdelay 0.008 keys /etc/ntp/key # add Time servers server 10.10.10.10 EOF echo \"# SYNC WITH NTP SERVER\" service ntpd stop ntpdate 10.10.10.10 echo \"# STARTING NTP SERVICE\" /etc/init.d/ntpd start echo \"# Configuring Console access rights\" cat <<EOF >/etc/default/console CONSOLE=/dev/console PASSREQ=YES PATH=/usr/bin: RETRIES=2 SLEEPTIME=4 SUPATH=/usr/sbin:/usr/bin: SYSLOG=YES SYSLOG_FAILED_LOGINS=2 TIMEOUT=60 UMASK=027 EOF echo \"# Configuring Password Parameters\" cat <<EOF >/etc/default/passwd MAXWEEKS=5 MINWEEKS=0 PASSLENGTH=7 EOF touch /.rhosts /.netrc /etc/hosts.equiv chmod 0 /.rhosts /.netrc /etc/hosts.equiv echo \"# Setup admin user accounts and passwords\" groupadd -g 2000 UNIX useradd XXXXX -u 99999 -c \"XXXXX XXXXX \" -p 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx' -d /home/XXXXX echo \"# Creating SUDO permissions for UNIX team...\" cat <<EOF >/etc/sudoers # sudoers file. # # This file MUST be edited with the 'visudo' command as root. # # See the sudoers man page for the details on how to write a sudoers file. # # Host alias specification # User alias specification # Cmnd alias specification # Defaults specification # User privilege specification root ALL=(ALL) ALL XXXXX ALL=(ALL) ALL # Uncomment to allow people in group wheel to run all commands # %wheel ALL=(ALL) ALL # Same thing without a password # %wheel ALL=(ALL) NOPASSWD: ALL # Samples # %users ALL=/sbin/mount /cdrom,/sbin/umount /cdrom # %users localhost=/sbin/shutdown -h now #aaron ALL= /usr/bin /sbin/mount /cdrom,/sbin/umount /cdrom EOF # the following is to stop windows admins accidentally rebooting linux boxes when they share a KVM with windows echo \"#\" echo \"# Disabling shutdown (CTRL/ALT/DEL)\" echo \"#\" mv /etc/inittab /etc/inittab.bak sed -e \"s/^pf:/#pf:/\" -e \"s/^pr:/#pr:/\" -e \"s/^ca:/#ca:/\" /etc/inittab.bak > /etc/inittab chmod 700 /etc/inittab chown root:root /etc/inittab echo;echo #echo \"#\" #echo \"# Disable xinetd\" #echo \"#\" #for service in `ls -1 /etc/xinetd.d`; do chkconfig $service off; done #echo ;echo echo \" creating MOTD\" echo \"\" > /etc/motd echo \"+----------------------------------------------------+\" >> /etc/motd echo \"| This system is the property of ME |\" >> /etc/motd echo \"+----------------------------------------------------+\" >> /etc/motd echo \"\">> /etc/motd # This will install mondo from an NFS mount and copy the config file #local mount point for NFS TEMP_MOUNT=/tmp/software mkdir $TEMP_MOUNT # Source of NSF share NFS_SERVER=xxxxx.xxxxx.com NFS_SHARE=/u1/software APP_VERSION=mondo-v2.2 NFS_PATH=$NFS_SHARE/$APP_VERSION/ echo \"mounting nfs share to install extra software\" mount $NFS_SERVER:$NFS_SHARE $TEMP_MOUNT # create diectories for mondo cd images mkdir /u1 mkdir /u1/iso mkdir /u1/iso/tmp mkdir /u1/mondo echo \"# install of mondo packages\" #echo \"installing mkisofs\" #rpm -ivh $TEMP_MOUNT/$APP_VERSION/mkisofs-2.0-11.i386.rpm #echo \"installing cdrecord\" #rpm -ivh $TEMP_MOUNT/$APP_VERSION/cdrecord-2.0-11.i386.rpm echo \"installing buffer\" rpm -ivh $TEMP_MOUNT/$APP_VERSION/buffer-1.19-4.i386.rpm echo \"installing afio\" rpm -ivh $TEMP_MOUNT/$APP_VERSION/afio-2.4.7-1.i386.rpm echo \"installing mindi\" rpm -ivh $TEMP_MOUNT/$APP_VERSION/mindi-1.06-266.rhel3.i386.rpm echo \"installing mondo\" rpm -ivh $TEMP_MOUNT/$APP_VERSION/mondo-2.2.0-2.rhel3.i586.rpm # copy mondo run script mkdir /usr/local/admin cp $TEMP_MOUNT/$APP_VERSION/mondo.sh /usr/local/admin/mondo chmod 744 /usr/local/admin/mondo echo \"# Install of Nagios Agent\" # swap this variable from mondo to nagios APP_VERSION=nagios-agent-2.5.2 rpm -Uvh $TEMP_MOUNT/$APP_VERSION/fping-2.4-1.b2.2.el4.rf.i386.rpm rpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Crypt-DES-2.05-3.2.el4.rf.i386.rpm rpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Digest-SHA1-2.07-5.i386.rpm rpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Digest-HMAC-1.01-13.noarch.rpm rpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Socket6-0.19-1.2.el4.rf.i386.rpm rpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Net-SNMP-5.2.0-1.2.el4.rf.noarch.rpm rpm -Uvh $TEMP_MOUNT/$APP_VERSION/nagios-plugins-1.4-2.2.el4.rf.i386.rpm cp $TEMP_MOUNT/$APP_VERSION/nrpe /usr/sbin cp $TEMP_MOUNT/$APP_VERSION/nrpe.cfg /etc/nagios cp $TEMP_MOUNT/$APP_VERSION/nrpe.xinetd /etc/xinetd.d/nrpe echo \"nrpe 5666/tcp #nagios agent\" >> /etc/services useradd nagios cp $TEMP_MOUNT/$APP_VERSION/check_nrpe /usr/lib/nagios/plugins/ cp $TEMP_MOUNT/$APP_VERSION/check_procs /usr/lib/nagios/plugins/ echo \"editing the fstab file to remove the strange LABEL for the swap partition\" # with out changing this mondo restore gets confused cp /etc/fstab /etc/fstab.bak cat /etc/fstab.bak | sed s/LABEL\\=SW\\-/\\\\/dev\\\\// > /etc/fstab echo \"fstab has been edited\" echo \"installing XXXX custom grub splash screen\" APP_VERSION=grub_custom_XXXX echo \"swapping the redhat grub splash screen with custom logo\" cp /boot/grub/splash.xpm.gz /boot/grub/splash.xpm.gz.orig cp $TEMP_MOUNT/$APP_VERSION/XXXX_grub_grey.xpm.gz /boot/grub/splash.xpm.gz","title":"Kickstart"},{"location":"linux/RedHat/satellite_spacewalk/perl_script_to_calulate_uptime_of_satellite_clients/","text":"#!/usr/bin/perl #This unfinished script will poll a satellite/spacewalk server for each registered host and return the uptime of each hosts ##Start of Sample output ########### #Host Name = centos64small01.localdomain #last_boot = 20140426T04:42:41 #last_checkin = 20140426T08:53:03 #lastboot_epoc :1398487320 #time now :1398502383 # seconds 15063 # minutes 251.05 # hours 4.18416666666667 # ########### #Host Name = centos64small02.localdomain #last_boot = 20130808T01:34:21 #last_checkin = 20130808T10:18:03 #lastboot_epoc :1375925640 #time now :1398502383 # seconds 22576743 # minutes 376279.05 # hours 6271.3175 ##End of Sample output use strict; use warnings; use Frontier::Client; use boolean; use Time::Local; #use Data::Dumper; #use DateTime::Format::ISO8601; #use POSIX; my $HOST = 'spacewalk'; my $user = 'admin'; my $pass = 'password'; my $client = new Frontier::Client(url => \"http://$HOST/rpc/api\"); my $session = $client->call('auth.login',$user, $pass) or die; my $systems = $client->call('system.listSystems', $session); foreach my $system (@$systems) { my %client = %{$system}; my $lastcheckin = ${$client{'last_checkin'}}; my $systemid = $system->{'id'}; my $hostname = $system->{'name'}; my $Detail = $client->call('system.getDetails', $session, $systemid); my %clientdetails = %{$Detail}; my $lastboot = ${$clientdetails{'last_boot'}}; print \"\\n###########\\n\"; print \"Host Name = $hostname\\n\"; print \"last_boot = $lastboot\\n\"; print \"last_checkin = $lastcheckin\\n\"; ## convert the non standard ISO8601 date format to a series of variables the to epoc. my $year = substr(\"$lastboot\", 0,4); my $month = substr(\"$lastboot\", 4,2); $month--; ## timegm counts months from 0 ie... Jan = 0 Dec =11 my $day = substr(\"$lastboot\", 6,2); my $hour = substr(\"$lastboot\", 9,2); my $min = substr(\"$lastboot\", 12,2); ## print \" year = $year\\n month = $month -1\\n day = $day\\n hour = $hour\\n min = $min\\n\"; my $lastboot_epoc = timegm(00,$min,$hour,$day,$month,$year); print \"lastboot_epoc :$lastboot_epoc\\n\"; ##calculating the difference between lastboot_epoc and now my $timenow = time; print \"time now :$timenow\\n\"; my $timediff = $timenow - $lastboot_epoc ; print \" seconds $timediff\\n\"; my $minutes = $timediff / 60 ; my $hours = $minutes / 60 ; print \" minutes $minutes\\n\"; print \" hours $hours\\n\"; }","title":"Perl script to calulate uptime of satellite clients"},{"location":"linux/RedHat/satellite_spacewalk/replacing_your_satellite_certificate/","text":"replacing your satellite certificate login to the Red Hat Customer Portal - self-service Satellite Certificate generation tool https://access.redhat.com/management/distributors/ Generate/update your certificate. further information can be found here https://access.redhat.com/site/articles/477863 Download your new certificate Install/Activate the certificate Restart the satellite server [root@server ~]# rhn-satellite-activate -vvv --rhn-cert=/home/seamusmurray/MySat.xml RHN_PARENT: satellite.rhn.redhat.com HTTP_PROXY: None HTTP_PROXY_USERNAME: None HTTP_PROXY_PASSWORD: <password> CA_CERT: /usr/share/rhn/RHNS-CA-CERT Checking cert XML sanity and GPG signature: '/usr/bin/validate-sat-cert.pl --keyring /etc/webapp-keyring.gpg /etc/sysconfig/rhn/rhn-entitlement-cert.xml-XXXX' Database connectioned initialized: refer to /etc/rhn/rhn.conf Attempting local RHN Certificate push (and therefore activation) Executing: remote XMLRPC deactivation (if necessary). Executing: remote XMLRPC activation call. Executing: '/usr/bin/satellite-sync --list-channels --ca-cert /usr/share/rhn/RHNS-CA-CERT' [root@server ~]# /usr/sbin/rhn-satellite restart Shutting down spacewalk services... Stopping RHN Taskomatic... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Stopped RHN Taskomatic. Stopping cobbler daemon: [ OK ] Stopping rhn-search... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Stopped rhn-search. Stopping MonitoringScout ... [ OK ] Stopping Monitoring ... [ OK ] Shutting down osa-dispatcher: [ OK ] Stopping httpd: [ OK ] Stopping tomcat5: [ OK ] Terminating jabberd processes ... Stopping s2s: [ OK ] Stopping c2s: [ OK ] Stopping sm: [ OK ] Stopping router: [ OK ] Done. Starting spacewalk services... Initializing jabberd processes ... Starting router: [ OK ] Starting sm: [ OK ] Starting c2s: [ OK ] Starting s2s: [ OK ] Starting tomcat5: [ OK ] Waiting for tomcat to be ready ... Starting httpd: [ OK ] Starting osa-dispatcher: [ OK ] Starting Monitoring ... [ OK ] Starting MonitoringScout ... [ OK ] Starting rhn-search... Starting cobbler daemon: [ OK ] Starting RHN Taskomatic... Done.","title":"Replacing your satellite certificate"},{"location":"linux/RedHat/satellite_spacewalk/replacing_your_satellite_certificate/#replacing-your-satellite-certificate","text":"login to the Red Hat Customer Portal - self-service Satellite Certificate generation tool https://access.redhat.com/management/distributors/ Generate/update your certificate. further information can be found here https://access.redhat.com/site/articles/477863 Download your new certificate Install/Activate the certificate Restart the satellite server [root@server ~]# rhn-satellite-activate -vvv --rhn-cert=/home/seamusmurray/MySat.xml RHN_PARENT: satellite.rhn.redhat.com HTTP_PROXY: None HTTP_PROXY_USERNAME: None HTTP_PROXY_PASSWORD: <password> CA_CERT: /usr/share/rhn/RHNS-CA-CERT Checking cert XML sanity and GPG signature: '/usr/bin/validate-sat-cert.pl --keyring /etc/webapp-keyring.gpg /etc/sysconfig/rhn/rhn-entitlement-cert.xml-XXXX' Database connectioned initialized: refer to /etc/rhn/rhn.conf Attempting local RHN Certificate push (and therefore activation) Executing: remote XMLRPC deactivation (if necessary). Executing: remote XMLRPC activation call. Executing: '/usr/bin/satellite-sync --list-channels --ca-cert /usr/share/rhn/RHNS-CA-CERT' [root@server ~]# /usr/sbin/rhn-satellite restart Shutting down spacewalk services... Stopping RHN Taskomatic... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Stopped RHN Taskomatic. Stopping cobbler daemon: [ OK ] Stopping rhn-search... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Stopped rhn-search. Stopping MonitoringScout ... [ OK ] Stopping Monitoring ... [ OK ] Shutting down osa-dispatcher: [ OK ] Stopping httpd: [ OK ] Stopping tomcat5: [ OK ] Terminating jabberd processes ... Stopping s2s: [ OK ] Stopping c2s: [ OK ] Stopping sm: [ OK ] Stopping router: [ OK ] Done. Starting spacewalk services... Initializing jabberd processes ... Starting router: [ OK ] Starting sm: [ OK ] Starting c2s: [ OK ] Starting s2s: [ OK ] Starting tomcat5: [ OK ] Waiting for tomcat to be ready ... Starting httpd: [ OK ] Starting osa-dispatcher: [ OK ] Starting Monitoring ... [ OK ] Starting MonitoringScout ... [ OK ] Starting rhn-search... Starting cobbler daemon: [ OK ] Starting RHN Taskomatic... Done.","title":"replacing your satellite certificate"},{"location":"linux/RedHat/satellite_spacewalk/satellite_certificate_licence_install/","text":"replacing your satellite certificate login to the Red Hat Customer Portal - self-service Satellite Certificate generation tool https://access.redhat.com/management/distributors/ Generate/update your certificate. further information can be found here https://access.redhat.com/site/articles/477863 Download your new certificate Install/Activate the certificate Restart the satellite server [root@server ~]# rhn-satellite-activate -vvv --rhn-cert=/home/seamusmurray/MySat.xml RHN_PARENT: satellite.rhn.redhat.com HTTP_PROXY: None HTTP_PROXY_USERNAME: None HTTP_PROXY_PASSWORD: <password> CA_CERT: /usr/share/rhn/RHNS-CA-CERT Checking cert XML sanity and GPG signature: '/usr/bin/validate-sat-cert.pl --keyring /etc/webapp-keyring.gpg /etc/sysconfig/rhn/rhn-entitlement-cert.xml-ZsgzKl' Database connection initialized: refer to /etc/rhn/rhn.conf Attempting local RHN Certificate push (and therefore activation) Executing: remote XMLRPC deactivation (if necessary). Executing: remote XMLRPC activation call. Executing: '/usr/bin/satellite-sync --list-channels --ca-cert /usr/share/rhn/RHNS-CA-CERT' [root@server ~]# /usr/sbin/rhn-satellite restart Shutting down spacewalk services... Stopping RHN Taskomatic... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Stopped RHN Taskomatic. Stopping cobbler daemon: [ OK ] Stopping rhn-search... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Stopped rhn-search. Stopping MonitoringScout ... [ OK ] Stopping Monitoring ... [ OK ] Shutting down osa-dispatcher: [ OK ] Stopping httpd: [ OK ] Stopping tomcat5: [ OK ] Terminating jabberd processes ... Stopping s2s: [ OK ] Stopping c2s: [ OK ] Stopping sm: [ OK ] Stopping router: [ OK ] Done. Starting spacewalk services... Initializing jabberd processes ... Starting router: [ OK ] Starting sm: [ OK ] Starting c2s: [ OK ] Starting s2s: [ OK ] Starting tomcat5: [ OK ] Waiting for tomcat to be ready ... Starting httpd: [ OK ] Starting osa-dispatcher: [ OK ] Starting Monitoring ... [ OK ] Starting MonitoringScout ... [ OK ] Starting rhn-search... Starting cobbler daemon: [ OK ] Starting RHN Taskomatic... Done.","title":"Satellite certificate licence install"},{"location":"linux/RedHat/satellite_spacewalk/satellite_certificate_licence_install/#replacing-your-satellite-certificate","text":"login to the Red Hat Customer Portal - self-service Satellite Certificate generation tool https://access.redhat.com/management/distributors/ Generate/update your certificate. further information can be found here https://access.redhat.com/site/articles/477863 Download your new certificate Install/Activate the certificate Restart the satellite server [root@server ~]# rhn-satellite-activate -vvv --rhn-cert=/home/seamusmurray/MySat.xml RHN_PARENT: satellite.rhn.redhat.com HTTP_PROXY: None HTTP_PROXY_USERNAME: None HTTP_PROXY_PASSWORD: <password> CA_CERT: /usr/share/rhn/RHNS-CA-CERT Checking cert XML sanity and GPG signature: '/usr/bin/validate-sat-cert.pl --keyring /etc/webapp-keyring.gpg /etc/sysconfig/rhn/rhn-entitlement-cert.xml-ZsgzKl' Database connection initialized: refer to /etc/rhn/rhn.conf Attempting local RHN Certificate push (and therefore activation) Executing: remote XMLRPC deactivation (if necessary). Executing: remote XMLRPC activation call. Executing: '/usr/bin/satellite-sync --list-channels --ca-cert /usr/share/rhn/RHNS-CA-CERT' [root@server ~]# /usr/sbin/rhn-satellite restart Shutting down spacewalk services... Stopping RHN Taskomatic... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Waiting for RHN Taskomatic to exit... Stopped RHN Taskomatic. Stopping cobbler daemon: [ OK ] Stopping rhn-search... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Waiting for rhn-search to exit... Stopped rhn-search. Stopping MonitoringScout ... [ OK ] Stopping Monitoring ... [ OK ] Shutting down osa-dispatcher: [ OK ] Stopping httpd: [ OK ] Stopping tomcat5: [ OK ] Terminating jabberd processes ... Stopping s2s: [ OK ] Stopping c2s: [ OK ] Stopping sm: [ OK ] Stopping router: [ OK ] Done. Starting spacewalk services... Initializing jabberd processes ... Starting router: [ OK ] Starting sm: [ OK ] Starting c2s: [ OK ] Starting s2s: [ OK ] Starting tomcat5: [ OK ] Waiting for tomcat to be ready ... Starting httpd: [ OK ] Starting osa-dispatcher: [ OK ] Starting Monitoring ... [ OK ] Starting MonitoringScout ... [ OK ] Starting rhn-search... Starting cobbler daemon: [ OK ] Starting RHN Taskomatic... Done.","title":"replacing your satellite certificate"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/","text":"Installation on RHEL 5 wget http://fedora.mirror.uber.com.au/epel/5/x86_64/spacecmd-1.9.4-1.el5.noarch.rpm sudo yum install python-simplejson.x86_64 sudo rpm -i spacecmd-1.9.4-1.el5.noarch.rpm Configure account credentials [USER@SERVER ~]$ cat .spacecmd/config [spacecmd] server=localhost username=admin password=PASSWORD nossl=1 Running spacecmd spacecmd can be run either interactively (with tab completion and a help menu) or non interactive both modes enable you to run a single spacecmd-command plus wither a single or multiple operand Interactive mode [USER@SERVER ~]$ spacecmd Welcome to spacecmd, a command-line interface to Spacewalk. Type: 'help' for a list of commands 'help <cmd>' for command-specific help 'quit' to quit INFO: Connected to http://localhost/rpc/api as admin spacecmd {SSM:0}> system_details server1.localhost spacecmd {SSM:0}> system_details server1.localhost server2.localhost NonInteractive from the shell (bash, csh etc...) [USER@SERVER ~]$ spacecmd system_details server1.localhost ## output goes to standard out [USER@SERVER ~]$ spacecmd system_details server1.localhost server2.localhost > {ouput file} ##redirected the output to a file Group creation and manipulation Interactive mode USER@SERVER ~]spacecmd <log in> spacecmd {SSM:41}> group_create newgroup spacecmd {SSM:41}> group_addsystems newgroup server1.local server2.local spacecmd {SSM:41}> group_addsystems newgroup search:name:server NonInteractive from the shell (bash, csh etc...) USER@SERVER ~]spacecmd group_create newgroup # you will be prompted for a group description USER@SERVER ~]spacecmd group_addsystems newgroup server1.local server2.local USER@SERVER ~]for i in ``cat list-of-servers`` ; do spacecmd group_addsystems newgroup $i ;done Note: Using the output redirection is handy when dealing with lots of data, if you use sed/awk/grep/perl to parse the output you can save your self a lot of time.-- list all systems registered to the spacewalk server USER@SERVER ~]spacecmd system_list > all-systems List all systems with a certain package+version installed USER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64 > fakepackagename-1.2.3-4.x86_64_installed Comparing the installed packages on 2 servers [USER@SERVER ~]$ spacecmd system_comparepackages server1.local server2.local INFO: Connected to http://localhost/rpc/api as admin Package This System Other System Difference ------------------------------------ ----------------------- ---------------------- ---------- initscripts 8.45.42-1.0.1.el5 8.45.42-1.0.1.el5_8.1 Newer there kernel 2.6.18-308.24.1.0.1.el5 None Only here kernel-fake None 2.6.32-400.11.1.el5 Only there listing negative delta lists.. Unfortunately you cannot query satellite/spacewalk for a list of servers where something does not exist I work around this limitation by in 3 steps Full-list - With-list = WithOut-list list all systems registered to the spacewalk server USER@SERVER ~]spacecmd system_list > all-systems list all systems with a certain package+version installed USER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64 > fakepackagename-1.2.3-4.x86_64_installed Generate a list of the servers that do not have the package installed for i in `cat all-systems` do if grep -q $i fakepackagename-1.2.3-4.x86_64_installed then true else echo $i fi done > fakepackagename-1.2.3-4.x86_64_not_installed you now have a delta list ## note before you compare outputs from spacecmd you have to clean up the files first eg...remove the extra headers from the output files and the MS dos line feeds \\^M etc.... scheduling - keeping track of a scheduled job initiated from the spacewalk web interface Interactive mode [USER@SERVER ~]$ spacecmd Welcome to spacecmd, a command-line interface to Spacewalk. Type: 'help' for a list of commands 'help <cmd>' for command-specific help 'quit' to quit INFO: Connected to http://localhost/rpc/api as admin spacecmd {SSM:0}> schedule_details 243013 NonInteractive from the shell (bash, csh etc...) [USER@SERVER ~]$ spacecmd schedule_details 243013 ## output goes to standard out [USER@SERVER ~]$ spacecmd schedule_details 243013 > {output file} ##redirected the output to a file Interactive mode [USER@SERVER ~]$ spacecmd Welcome to spacecmd, a command-line interface to Spacewalk. Type: 'help' for a list of commands 'help <cmd>' for command-specific help 'quit' to quit INFO: Connected to http://localhost/rpc/api as admin spacecmd {SSM:0}> schedule_getoutput 243013 NonInteractive from the shell (bash, csh etc...) [USER@SERVER ~]$ spacecmd schedule_getoutput 243013 ## output goes to standard out [USER@SERVER ~]$ spacecmd schedule_getoutput 243013 > {output file} ##redirected the output to a file Running scripts on multiple severs. Instead of trying to parse relatively unstructured output I prefer to structure the the output before it goes into spacewalk My preferred method is echo the server name followed by the output of the command, this way it is easy to grep out the server you want eg... if you want to run the following script #!/bin/sh /usr/sbin/dmidecode | grep -m 1 Product output from spacemcd _ spacecmd {SSM:0}> schedule_getoutput 245295 System: server1.local Start Time: 20131118T15:22:52 Stop Time: 20131118T15:22:52 Return Code: 0 Output ------ Product Name: VMware Virtual Platform ############################## System: server2.local Start Time: 20131118T15:24:38 Stop Time: 20131118T15:24:38 Return Code: 0 Output ------ Product Name: VMware Virtual Platform script #!/bin/sh product=`/usr/sbin/dmidecode | grep -m 1 Product` servername=`hostname` echo $servername $product output from spacemcd _ spacecmd {SSM:0}> schedule_getoutput 245295 System: server1.local Start Time: 20131118T15:22:52 Stop Time: 20131118T15:22:52 Return Code: 0 Output ------ server1.local Product Name: VMware Virtual Platform ############################## System: server2.local Start Time: 20131118T15:24:38 Stop Time: 20131118T15:24:38 Return Code: 0 Output ------ server2.local Product Name: VMware Virtual Platform __with the second method it is very easy to just grep for server*local or Vmware__ script to capture the IP details across all Ethernet interfaces run across all hosts to capture all IP's on all hosts #!/bin/bash HOST_NAME=`hostname -s` NICS=`ls /sys/class/net | grep -v lo` for i in $NICS do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://` done you could achieve the same results by querying the oracle database directly or using the satellite API. I chose to use spacecmd, grep and awk because that\u2019s what I'm more comfortable with this method seems to put less load on the satellite server and the back end database (it creates hash tables locally in .spacecmd/localhost/) which I assume reduces hits on the database. If the batch \"list_PackageName-6.1_not_installed\" is too large to run in one go.. you can divide it up based on server name prefix/suffix or simply batch size eg.... name based divide grep servername list_PackageName-6.1_not_installed > list_PackageName-6.1_not_installed_servernames grep '[0-9]pr' list_PackageName-6.1_not_installed > list_PackageName-6.1_not_installed_pr batch size divide STARTNUM=100 LASTNUM=200 awk '$1 == '$STARTNUM' , $1 == '$LASTNUM'' list_PackageName-6.1_not_installed > list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM Create a system group to run the job against GROUP_NAME=PackageName_install spacecmd group_create $GROUP_NAME Populate the group for i in `cat list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM do spacecmd group_addsystems $GROUP_NAME $i done schedule the job from the GUI you can either keep track of the scheduled job from the GUI or from spacecmd spacecmd schedule_details {schedule number} If you need to schedule a job from satellite at a precise time, you can use satellite to run an arbitrary script to set up an at job for the client to check in with the satellite server at a specific time (within 60 seconds) You can schedule a second job in satellite to perform the the actual work. Example. Suppose you want to execute the the following command at 6pm today ps -Aef Schedule an \"at\" job to execute the /usr/sbin/rhn_check at your desired time Note: To ensure that all the clients have polled the satellite server and picked up this job you will have to schedule this at least an hour before your \"desired time\" #!/bin/sh PreciseTime=1800 cat > /tmp/sat-precision-strike.sh <<EOF echo `date` \" starting the at job\" >> /tmp/sat-precision-strike.log /usr/sbin/rhn_check echo `date` \" finished the at job\" >> /tmp/sat-precision-strike.log rm -f /tmp/sat-precision-strike.sh EOF sleep 5 at -f /tmp/sat-precision-strike.sh $PreciseTime echo \"added at job at -f /root/sat-precision-strike $PreciseTime\" >> /tmp/sat-precision-strike.log sleep 5 Schedule a second Job detailing the command that you need to run ensure you set the \"Schedule no sooner than:\" time to 1 minute before your desired time #!/bin/bash /bin/ps -Aef","title":"Using spacecmd"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#installation-on-rhel-5","text":"wget http://fedora.mirror.uber.com.au/epel/5/x86_64/spacecmd-1.9.4-1.el5.noarch.rpm sudo yum install python-simplejson.x86_64 sudo rpm -i spacecmd-1.9.4-1.el5.noarch.rpm","title":"Installation on RHEL 5"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#configure-account-credentials","text":"[USER@SERVER ~]$ cat .spacecmd/config [spacecmd] server=localhost username=admin password=PASSWORD nossl=1","title":"Configure account credentials"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#running-spacecmd","text":"spacecmd can be run either interactively (with tab completion and a help menu) or non interactive both modes enable you to run a single spacecmd-command plus wither a single or multiple operand Interactive mode [USER@SERVER ~]$ spacecmd Welcome to spacecmd, a command-line interface to Spacewalk. Type: 'help' for a list of commands 'help <cmd>' for command-specific help 'quit' to quit INFO: Connected to http://localhost/rpc/api as admin spacecmd {SSM:0}> system_details server1.localhost spacecmd {SSM:0}> system_details server1.localhost server2.localhost NonInteractive from the shell (bash, csh etc...) [USER@SERVER ~]$ spacecmd system_details server1.localhost ## output goes to standard out [USER@SERVER ~]$ spacecmd system_details server1.localhost server2.localhost > {ouput file} ##redirected the output to a file","title":"Running spacecmd"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#group-creation-and-manipulation","text":"Interactive mode USER@SERVER ~]spacecmd <log in> spacecmd {SSM:41}> group_create newgroup spacecmd {SSM:41}> group_addsystems newgroup server1.local server2.local spacecmd {SSM:41}> group_addsystems newgroup search:name:server NonInteractive from the shell (bash, csh etc...) USER@SERVER ~]spacecmd group_create newgroup # you will be prompted for a group description USER@SERVER ~]spacecmd group_addsystems newgroup server1.local server2.local USER@SERVER ~]for i in ``cat list-of-servers`` ; do spacecmd group_addsystems newgroup $i ;done","title":"Group creation and manipulation"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#note","text":"Using the output redirection is handy when dealing with lots of data, if you use sed/awk/grep/perl to parse the output you can save your self a lot of time.--","title":"Note:"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#list-all-systems-registered-to-the-spacewalk-server","text":"USER@SERVER ~]spacecmd system_list > all-systems","title":"list all systems registered to the spacewalk server"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#list-all-systems-with-a-certain-packageversion-installed","text":"USER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64 > fakepackagename-1.2.3-4.x86_64_installed","title":"List all systems with a certain package+version installed"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#comparing-the-installed-packages-on-2-servers","text":"[USER@SERVER ~]$ spacecmd system_comparepackages server1.local server2.local INFO: Connected to http://localhost/rpc/api as admin Package This System Other System Difference ------------------------------------ ----------------------- ---------------------- ---------- initscripts 8.45.42-1.0.1.el5 8.45.42-1.0.1.el5_8.1 Newer there kernel 2.6.18-308.24.1.0.1.el5 None Only here kernel-fake None 2.6.32-400.11.1.el5 Only there","title":"Comparing the installed packages on 2 servers"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#listing-negative-delta-lists","text":"Unfortunately you cannot query satellite/spacewalk for a list of servers where something does not exist I work around this limitation by in 3 steps","title":"listing negative delta lists.."},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#full-list-with-list-without-list","text":"","title":"Full-list - With-list = WithOut-list"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#list-all-systems-registered-to-the-spacewalk-server_1","text":"USER@SERVER ~]spacecmd system_list > all-systems","title":"list all systems registered to the spacewalk server"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#list-all-systems-with-a-certain-packageversion-installed_1","text":"USER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64 > fakepackagename-1.2.3-4.x86_64_installed","title":"list all systems with a certain package+version installed"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#generate-a-list-of-the-servers-that-do-not-have-the-package-installed","text":"for i in `cat all-systems` do if grep -q $i fakepackagename-1.2.3-4.x86_64_installed then true else echo $i fi done > fakepackagename-1.2.3-4.x86_64_not_installed you now have a delta list ## note before you compare outputs from spacecmd you have to clean up the files first eg...remove the extra headers from the output files and the MS dos line feeds \\^M etc....","title":"Generate a list of the servers that do not have the package installed"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#scheduling-keeping-track-of-a-scheduled-job-initiated-from-the-spacewalk-web-interface","text":"","title":"scheduling - keeping track of a scheduled job initiated from the spacewalk web interface"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#interactive-mode","text":"[USER@SERVER ~]$ spacecmd Welcome to spacecmd, a command-line interface to Spacewalk. Type: 'help' for a list of commands 'help <cmd>' for command-specific help 'quit' to quit INFO: Connected to http://localhost/rpc/api as admin spacecmd {SSM:0}> schedule_details 243013","title":"Interactive mode"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#noninteractive-from-the-shell-bash-csh-etc","text":"[USER@SERVER ~]$ spacecmd schedule_details 243013 ## output goes to standard out [USER@SERVER ~]$ spacecmd schedule_details 243013 > {output file} ##redirected the output to a file","title":"NonInteractive from the shell (bash, csh etc...)"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#interactive-mode_1","text":"[USER@SERVER ~]$ spacecmd Welcome to spacecmd, a command-line interface to Spacewalk. Type: 'help' for a list of commands 'help <cmd>' for command-specific help 'quit' to quit INFO: Connected to http://localhost/rpc/api as admin spacecmd {SSM:0}> schedule_getoutput 243013","title":"Interactive mode"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#noninteractive-from-the-shell-bash-csh-etc_1","text":"[USER@SERVER ~]$ spacecmd schedule_getoutput 243013 ## output goes to standard out [USER@SERVER ~]$ spacecmd schedule_getoutput 243013 > {output file} ##redirected the output to a file","title":"NonInteractive from the shell (bash, csh etc...)"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#running-scripts-on-multiple-severs","text":"Instead of trying to parse relatively unstructured output I prefer to structure the the output before it goes into spacewalk My preferred method is echo the server name followed by the output of the command, this way it is easy to grep out the server you want eg... if you want to run the following script #!/bin/sh /usr/sbin/dmidecode | grep -m 1 Product output from spacemcd _ spacecmd {SSM:0}> schedule_getoutput 245295 System: server1.local Start Time: 20131118T15:22:52 Stop Time: 20131118T15:22:52 Return Code: 0 Output ------ Product Name: VMware Virtual Platform ############################## System: server2.local Start Time: 20131118T15:24:38 Stop Time: 20131118T15:24:38 Return Code: 0 Output ------ Product Name: VMware Virtual Platform script #!/bin/sh product=`/usr/sbin/dmidecode | grep -m 1 Product` servername=`hostname` echo $servername $product output from spacemcd _ spacecmd {SSM:0}> schedule_getoutput 245295 System: server1.local Start Time: 20131118T15:22:52 Stop Time: 20131118T15:22:52 Return Code: 0 Output ------ server1.local Product Name: VMware Virtual Platform ############################## System: server2.local Start Time: 20131118T15:24:38 Stop Time: 20131118T15:24:38 Return Code: 0 Output ------ server2.local Product Name: VMware Virtual Platform __with the second method it is very easy to just grep for server*local or Vmware__","title":"Running scripts on multiple severs."},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#script-to-capture-the-ip-details-across-all-ethernet-interfaces","text":"","title":"script to capture the IP details across all Ethernet interfaces"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#run-across-all-hosts-to-capture-all-ips-on-all-hosts","text":"#!/bin/bash HOST_NAME=`hostname -s` NICS=`ls /sys/class/net | grep -v lo` for i in $NICS do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://` done","title":"run across all hosts to capture all IP's on all hosts"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#you-could-achieve-the-same-results-by-querying-the-oracle-database-directly-or-using-the-satellite-api-i-chose-to-use-spacecmd-grep-and-awk-because-thats-what-im-more-comfortable-with","text":"","title":"you could achieve the same results by querying the oracle database directly or using the satellite API. I chose to use spacecmd, grep and awk because that\u2019s what I'm more comfortable with"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#this-method-seems-to-put-less-load-on-the-satellite-server-and-the-back-end-database","text":"","title":"this method seems to put less load on the satellite server and the back end database"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#it-creates-hash-tables-locally-in-spacecmdlocalhost-which-i-assume-reduces-hits-on-the-database","text":"","title":"(it creates hash tables locally in .spacecmd/localhost/) which I assume reduces hits on the database."},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#if-the-batch-list_packagename-61_not_installed-is-too-large-to-run-in-one-go","text":"","title":"If the batch \"list_PackageName-6.1_not_installed\" is too large to run in one go.."},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#you-can-divide-it-up-based-on-server-name-prefixsuffix-or-simply-batch-size-eg","text":"","title":"you can divide it up based on server name prefix/suffix or simply batch size eg...."},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#name-based-divide","text":"grep servername list_PackageName-6.1_not_installed > list_PackageName-6.1_not_installed_servernames grep '[0-9]pr' list_PackageName-6.1_not_installed > list_PackageName-6.1_not_installed_pr","title":"name based divide"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#batch-size-divide","text":"STARTNUM=100 LASTNUM=200 awk '$1 == '$STARTNUM' , $1 == '$LASTNUM'' list_PackageName-6.1_not_installed > list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM","title":"batch size divide"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#create-a-system-group-to-run-the-job-against","text":"GROUP_NAME=PackageName_install spacecmd group_create $GROUP_NAME","title":"Create a system group to run the job against"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#populate-the-group","text":"for i in `cat list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM do spacecmd group_addsystems $GROUP_NAME $i done","title":"Populate the group"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#schedule-the-job-from-the-gui","text":"you can either keep track of the scheduled job from the GUI or from spacecmd spacecmd schedule_details {schedule number}","title":"schedule the job from the GUI"},{"location":"linux/RedHat/satellite_spacewalk/using_spacecmd/#if-you-need-to-schedule-a-job-from-satellite-at-a-precise-time-you-can-use-satellite-to-run-an-arbitrary-script-to-set-up-an-at-job-for-the-client-to-check-in-with-the-satellite-server-at-a-specific-time-within-60-seconds","text":"You can schedule a second job in satellite to perform the the actual work. Example. Suppose you want to execute the the following command at 6pm today ps -Aef Schedule an \"at\" job to execute the /usr/sbin/rhn_check at your desired time Note: To ensure that all the clients have polled the satellite server and picked up this job you will have to schedule this at least an hour before your \"desired time\" #!/bin/sh PreciseTime=1800 cat > /tmp/sat-precision-strike.sh <<EOF echo `date` \" starting the at job\" >> /tmp/sat-precision-strike.log /usr/sbin/rhn_check echo `date` \" finished the at job\" >> /tmp/sat-precision-strike.log rm -f /tmp/sat-precision-strike.sh EOF sleep 5 at -f /tmp/sat-precision-strike.sh $PreciseTime echo \"added at job at -f /root/sat-precision-strike $PreciseTime\" >> /tmp/sat-precision-strike.log sleep 5 Schedule a second Job detailing the command that you need to run ensure you set the \"Schedule no sooner than:\" time to 1 minute before your desired time #!/bin/bash /bin/ps -Aef","title":"If you need to schedule a job from satellite at a precise time, you can use satellite to run an arbitrary script to set up an at job for the client to check in with the satellite server at a specific time (within 60 seconds)"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/","text":"How to create a local yum REPO from the RedHat install media RHEL 7 local repo cat > /etc/yum.repos.d/dvd-iso.repo << EOF [InstallMedia] name=RHEL-7.0 Server.x86_64 metadata_expire=-1 gpgcheck=0 cost=500 baseurl=file:///media/repo EOF mkdir /media/repo mount /dev/sr0 /media/repo RHEL 6 local repo mkdir -p /mnt/DVD-ISO-REPO/ mount /dev/dvd /mnt/DVD-ISO-REPO/ copy the media.repo file from the ISO cp /mnt/DVD-ISO-REPO/media.repo /etc/yum.repos.d/media.repo echo \"baseurl=file:///mnt/DVD-ISO-REPO/\" >> /etc/yum.repos.d/media.repo RHEL 5 local repo cat > /etc/yum.repos.d/dvd-iso.repo << EOF ##repo file for mounting the RHEL 5.x DVD ISO [local_Cluster] name=Local Cluster baseurl=file:///mnt/DVD-ISO-REPO/Cluster/ enabled=1 gpgcheck=0 [local_ClusterStorage] name=Local Cluster Storage baseurl=file:///mnt/DVD-ISO-REPO/ClusterStorage/ enabled=1 gpgcheck=0 [Local_Server] name= Local Server baseurl=file:///mnt/DVD-ISO-REPO/Server enabled=1 gpgcheck=0 EOF mkdir -p /mnt/DVD-ISO-REPO mount /dev/dvd /mnt/DVD-ISO-REPO/ yum repolist RHEL 4 local repo Mount the RHEL 4 installation ISO. mount -o loop,ro RHEL4.8-x86_64-AS-DVD.iso /media/cdrom Comment out the following line in /etc/sysconfig/rhn/sources This will prevent up2date from connecting to Red Hat Network up2date default Change to: #up2date default create a line like the following in /etc/sysconfig/rhn/sources dir mydvdrepo /media/cdrom/RedHat/RPMS List the REPOS yum repolist epel repo install rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm","title":"Create local repos from redhat install media"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#rhel-7-local-repo","text":"cat > /etc/yum.repos.d/dvd-iso.repo << EOF [InstallMedia] name=RHEL-7.0 Server.x86_64 metadata_expire=-1 gpgcheck=0 cost=500 baseurl=file:///media/repo EOF","title":"RHEL 7 local repo"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#_1","text":"mkdir /media/repo mount /dev/sr0 /media/repo","title":""},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#rhel-6-local-repo","text":"mkdir -p /mnt/DVD-ISO-REPO/ mount /dev/dvd /mnt/DVD-ISO-REPO/","title":"RHEL 6 local repo"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#copy-the-mediarepo-file-from-the-iso","text":"cp /mnt/DVD-ISO-REPO/media.repo /etc/yum.repos.d/media.repo echo \"baseurl=file:///mnt/DVD-ISO-REPO/\" >> /etc/yum.repos.d/media.repo","title":"copy the media.repo file from the ISO"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#rhel-5-local-repo","text":"cat > /etc/yum.repos.d/dvd-iso.repo << EOF ##repo file for mounting the RHEL 5.x DVD ISO [local_Cluster] name=Local Cluster baseurl=file:///mnt/DVD-ISO-REPO/Cluster/ enabled=1 gpgcheck=0 [local_ClusterStorage] name=Local Cluster Storage baseurl=file:///mnt/DVD-ISO-REPO/ClusterStorage/ enabled=1 gpgcheck=0 [Local_Server] name= Local Server baseurl=file:///mnt/DVD-ISO-REPO/Server enabled=1 gpgcheck=0 EOF","title":"RHEL 5 local repo"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#_2","text":"mkdir -p /mnt/DVD-ISO-REPO mount /dev/dvd /mnt/DVD-ISO-REPO/ yum repolist","title":""},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#rhel-4-local-repo","text":"","title":"RHEL 4 local repo"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#mount-the-rhel-4-installation-iso","text":"mount -o loop,ro RHEL4.8-x86_64-AS-DVD.iso /media/cdrom","title":"Mount the RHEL 4 installation ISO."},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#comment-out-the-following-line-in-etcsysconfigrhnsources","text":"This will prevent up2date from connecting to Red Hat Network up2date default","title":"Comment out the following line in /etc/sysconfig/rhn/sources"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#change-to","text":"#up2date default","title":"Change to:"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#create-a-line-like-the-following-in-etcsysconfigrhnsources","text":"dir mydvdrepo /media/cdrom/RedHat/RPMS List the REPOS yum repolist","title":"create a line like the following in /etc/sysconfig/rhn/sources"},{"location":"linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/#epel-repo-install","text":"rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm","title":"epel repo install"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/","text":"working with yum and rpm download the latest repodata yum clean yum repolist list current repos yum repolist Querying data from the RPM database List the available tags rpm --querytags you can specify 1 or more tags plus what ever formatting you want /n /t etc.. rpm -q -qf \"%{TAGNAME}\\n\" fake-rpm or rpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm determine which rpm has the largest file on the server rpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on thrid field determine when an rpm was installed, displayed in human readable time rpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm fixing python on RHEL when someone breaks it for i in `rpm -qa | grep python` ; do yum reinstall $i -y ; done display a list of the last 10 rpms to be installed rpm -q --all --last | head -10 determine inter dependencies rpm -q --whatrequires /usr/lib/libstdc++.so.5 rpm -q --whatprovides /usr/lib/libstdc++.so.5 disable yum plugins on RHEL if your not registering the server with a satellite server Resigning a vendor package with the your own GPG key [root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 14214 License: GPLv2+ Signature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132 Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server. [root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm Enter pass phrase: Pass phrase is good. spacewalk-selinux-1.4.1-1.el5-test.src.rpm: gpg: WARNING: standard input reopened gpg: WARNING: standard input reopened [root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 14214 License: GPLv2+ Signature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server.","title":"Working with yum and rpm"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#working-with-yum-and-rpm","text":"","title":"working with yum and rpm"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#download-the-latest-repodata","text":"yum clean yum repolist","title":"download the latest repodata"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#list-current-repos","text":"yum repolist","title":"list current repos"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#querying-data-from-the-rpm-database","text":"","title":"Querying data from the RPM database"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#list-the-available-tags","text":"rpm --querytags you can specify 1 or more tags plus what ever formatting you want /n /t etc.. rpm -q -qf \"%{TAGNAME}\\n\" fake-rpm or rpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm","title":"List the available tags"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#determine-which-rpm-has-the-largest-file-on-the-server","text":"rpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on thrid field","title":"determine which rpm has the largest file on the server"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#determine-when-an-rpm-was-installed-displayed-in-human-readable-time","text":"rpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm","title":"determine when an rpm was installed, displayed in human readable time"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#fixing-python-on-rhel-when-someone-breaks-it","text":"for i in `rpm -qa | grep python` ; do yum reinstall $i -y ; done","title":"fixing python on RHEL when someone breaks it"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#display-a-list-of-the-last-10-rpms-to-be-installed","text":"rpm -q --all --last | head -10","title":"display a list of the last 10 rpms to be installed"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#determine-inter-dependencies","text":"rpm -q --whatrequires /usr/lib/libstdc++.so.5 rpm -q --whatprovides /usr/lib/libstdc++.so.5 disable yum plugins on RHEL if your not registering the server with a satellite server","title":"determine inter dependencies"},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#_1","text":"","title":""},{"location":"linux/RedHat/yum_rpm/working_with_yum_and_rpm/#resigning-a-vendor-package-with-the-your-own-gpg-key","text":"[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 14214 License: GPLv2+ Signature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132 Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server. [root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm Enter pass phrase: Pass phrase is good. spacewalk-selinux-1.4.1-1.el5-test.src.rpm: gpg: WARNING: standard input reopened gpg: WARNING: standard input reopened [root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 14214 License: GPLv2+ Signature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server.","title":"Resigning a vendor package with the your own GPG key"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/","text":"working with yum and rpm download the latest repodata yum clean yum repolist list current repos yum repolist Querying data from the RPM database List the available tags rpm --querytags you can specify 1 or more tags plus what ever formatting you want /n /t etc.. rpm -q -qf \"%{TAGNAME}\\n\" fake-rpm or rpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm determine which rpm has the largest file on the server rpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on third field determine when an rpm was installed, displayed in human readable time rpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm determine inter dependencies rpm -q --whatrequires /usr/lib/libstdc++.so.5 rpm -q --whatprovides /usr/lib/libstdc++.so.5 disable yum plugins on RHEL if your not registering the server with a satellite server Resigning a vendor package with the example GPG key [root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 144 License: GPLv2+ Signature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132 Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server. [root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm Enter pass phrase: Pass phrase is good. spacewalk-selinux-1.4.1-1.el5-test.src.rpm: gpg: WARNING: standard input reopened gpg: WARNING: standard input reopened [root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 144 License: GPLv2+ Signature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server. RPM packaging and development Building a custom meta package that encompasses multiple customer child package source files /usr/src/redhat/SOURCES/example-1.1.tar.gz member packages example-1.1-4.el5.i386.rpm (meta-package) example-tools-1.1-4.el5.i386.rpm example-devops-1.1-4.el5.i386.rpm example-security-1.1-4.el5.i386.rpm example-vmware-1.1-4.el5.i386.rpm RPM Package development server. The RPM build directory is /usr/src/redhat/ packages are built as the root user You will need the pass-phrase for the example gpg signing key When building the example-tools there are distribution specific dependencies. example meta package is built as 32bit. as it contains no binary executables it will work on both 32 & 64bit. Walk though of updating/modifying the example package Delete the previously extracted package tarball, this is required in case someone has made changes to the files rm -R /usr/src/redhat/BUILD/example-1.1 Extract the package tar ball that was created during the previous RPM creation cd /usr/src/redhat/BUILD/ tar xvf /usr/src/redhat/SOURCES/example-1.1.tar.gz Make the desired change to the files in the rpm. eg... vi /usr/src/redhat/BUILD/example-1.1/etc/cron.daily/housekeeping-daily Create a compressed tarball of the files that will be including in the RPM cd /usr/src/redhat/BUILD/ tar zcvf /usr/src/redhat/SOURCES/example-1.1.tar.gz ./example-1.1 Update the release version and change log in the spec file vi /usr/src/redhat/SPECS/example.spec Build the rpms for the various distribution targets rpmbuild --define 'dist .el5' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec rpmbuild --define 'dist .el6' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec.el6 Repo locations... /data/redhat/linux/enterprise/5Server/en/example/i386/ /data/redhat/linux/enterprise/5Server/en/example/x86_64/ /data/redhat/linux/enterprise/6Server/en/example/x86_64/ /data/redhat/linux/enterprise/7Server/en/example/x86_64/ Copy new RPMs to the repo directories NEWVERSION=\"1.1-4\" # set to the desired version cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/i386/ cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el6.i386.rpm /data/redhat/linux/enterprise/6Server/en/example/x86_64/ cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/x86_64/ cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el7.i386.rpm /data/redhat/linux/enterprise/7Server/en/example/x86_64/ Updating the repos cd /data/redhat/linux/enterprise/5Server/en/example/i386/ createrepo . cd /data/redhat/linux/enterprise/6Server/en/example/x86_64/ createrepo . cd /data/redhat/linux/enterprise/5Server/en/example/x86_64/ createrepo . cd /data/redhat/linux/enterprise/7Server/en/example/x86_64/ createrepo . Syncing the YUM repos with the satellite server Login to the REDHAT satellite server and sync the channels individually spacewalk-repo-sync --channel rhel6-x86_64-example --type yum spacewalk-repo-sync --channel rhel5-x86_64-example --type yum spacewalk-repo-sync --channel rhel5-i386-example --type yum spacewalk-repo-sync --channel rhel7-x86_64-example --type yum or Sync all the channels including to RedHat ( warning this will take a long time ) [root@satellite1 ~]# /opt/example/bin/satellite-sync.sh No new packages to sync. Repo http://server.example.local/redhat/linux/enterprise/5Server/en/rhel-example/x86_64/ has 0 errata. Sync completed. Total time: 0:00:00 Repo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/ Packages in repo: 46 Packages already synced: 37 Packages to sync: 9 1/9 : kmod-vmware-tools-pvscsi-1.0.2.0-2.6.18.8.el5.3-0.x86_64 2/9 : vmware-tools-xorg-drv-mouse-12.6.4.0-4-0.x86_64 3/9 : kmod-vmware-tools-vsock-1.0.0.0-2.6.18.8.el5.3-0.x86_64 4/9 : kmod-vmware-tools-vmsync-1.1.0.1-2.6.18.8.el5.3-0.x86_64 5/9 : kmod-vmware-tools-vmci-9.0.1.1-2.6.18.8.el5.3-0.x86_64 6/9 : kmod-vmware-tools-vmmemctl-1.2.1.2-2.6.18.8.el5.3-0.x86_64 7/9 : kmod-vmware-tools-vmblock-1.1.2.0-2.6.18.8.el5.3-0.x86_64 8/9 : kmod-vmware-tools-vmhgfs-1.4.1.1-2.6.18.8.el5.3-0.x86_64 9/9 : vmware-tools-xorg-drv-display-11.0.2.10-4-0.x86_64 Repo http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/ has 0 errata. Sync completed. Total time: 0:00:04 Repo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/ Packages in repo: 90 Packages already synced: 86 Packages to sync: 4 1/4 : example-tools-1.1-4.el5-0.i386 2/4 : example-tools-1.1-213.el5-0.i386 3/4 : tidalagent-3.1.0.13-8-0.i386 Repo http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/ has 0 errata. Sync completed. Dealing with a package interdependency issue of my own creation.... To explain the issue, in the package dependencies we have: example-tools requires example at the same version example-devops requires example at the same version There is no version lock between tools and devops The upgrade appears to go as follows: (Assume installed version is \u20181\u2019, target version is \u20182\u2019 and latest in repo is \u20183\u2019 All three packages installed at version 1 Request example-tools upgrade to version 2, requires example also at version 2. Conflict as example-devops is at version 1, and requires example at version 1 yum attempts to resolve conflict, by trying to upgrade example-devops. (This is an upgrade that will just pick the latest available, so picks version 3 example-devops version 3 depends on example version 3 Conflict now between example and example-tools, so upgrade example-tools to resolve All packages now at version 3. Two things to try: Version lock all of the packages together (This will work) Create a RHN errata, and try applying it. yum install example-devops --exclude= 1.1-2 --exclude= 1.1-3 Installed: example-devops.i386 0:1.1-207.el5","title":"Yum and rpm"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#working-with-yum-and-rpm","text":"","title":"working with yum and rpm"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#download-the-latest-repodata","text":"yum clean yum repolist","title":"download the latest repodata"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#list-current-repos","text":"yum repolist","title":"list current repos"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#querying-data-from-the-rpm-database","text":"","title":"Querying data from the RPM database"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#list-the-available-tags","text":"rpm --querytags you can specify 1 or more tags plus what ever formatting you want /n /t etc.. rpm -q -qf \"%{TAGNAME}\\n\" fake-rpm or rpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm","title":"List the available tags"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#determine-which-rpm-has-the-largest-file-on-the-server","text":"rpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on third field","title":"determine which rpm has the largest file on the server"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#determine-when-an-rpm-was-installed-displayed-in-human-readable-time","text":"rpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm","title":"determine when an rpm was installed, displayed in human readable time"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#determine-inter-dependencies","text":"rpm -q --whatrequires /usr/lib/libstdc++.so.5 rpm -q --whatprovides /usr/lib/libstdc++.so.5 disable yum plugins on RHEL if your not registering the server with a satellite server","title":"determine inter dependencies"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#_1","text":"","title":""},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#resigning-a-vendor-package-with-the-example-gpg-key","text":"[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 144 License: GPLv2+ Signature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132 Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server. [root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm Enter pass phrase: Pass phrase is good. spacewalk-selinux-1.4.1-1.el5-test.src.rpm: gpg: WARNING: standard input reopened gpg: WARNING: standard input reopened [root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm Name : spacewalk-selinux Relocations: (not relocatable) Version : 1.4.1 Vendor: Koji Release : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST Install Date: (not installed) Build Host: domU-12-31-38-00-09-D1 Group : System Environment/Base Source RPM: (none) Size : 144 License: GPLv2+ Signature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed Packager : Koji URL : http://fedorahosted.org/spacewalk Summary : SELinux policy module supporting Spacewalk Server Description : SELinux policy module supporting Spacewalk Server.","title":"Resigning a vendor package with the example GPG key"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#_2","text":"","title":""},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#rpm-packaging-and-development","text":"","title":"RPM packaging and development"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#building-a-custom-meta-package-that-encompasses-multiple-customer-child-package","text":"source files /usr/src/redhat/SOURCES/example-1.1.tar.gz member packages example-1.1-4.el5.i386.rpm (meta-package) example-tools-1.1-4.el5.i386.rpm example-devops-1.1-4.el5.i386.rpm example-security-1.1-4.el5.i386.rpm example-vmware-1.1-4.el5.i386.rpm","title":"Building a custom meta package that encompasses multiple customer child package"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#rpm-package-development-server","text":"The RPM build directory is /usr/src/redhat/ packages are built as the root user You will need the pass-phrase for the example gpg signing key When building the example-tools there are distribution specific dependencies. example meta package is built as 32bit. as it contains no binary executables it will work on both 32 & 64bit.","title":"RPM Package development server."},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#walk-though-of-updatingmodifying-the-example-package","text":"","title":"Walk though of updating/modifying the example package"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#delete-the-previously-extracted-package-tarball-this-is-required-in-case-someone-has-made-changes-to-the-files","text":"rm -R /usr/src/redhat/BUILD/example-1.1","title":"Delete the previously extracted package tarball, this is required in case someone has made changes to the files"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#extract-the-package-tar-ball-that-was-created-during-the-previous-rpm-creation","text":"cd /usr/src/redhat/BUILD/ tar xvf /usr/src/redhat/SOURCES/example-1.1.tar.gz","title":"Extract the package tar ball that was created during the previous RPM creation"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#make-the-desired-change-to-the-files-in-the-rpm-eg","text":"vi /usr/src/redhat/BUILD/example-1.1/etc/cron.daily/housekeeping-daily","title":"Make the desired change to the files in the rpm. eg..."},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#create-a-compressed-tarball-of-the-files-that-will-be-including-in-the-rpm","text":"cd /usr/src/redhat/BUILD/ tar zcvf /usr/src/redhat/SOURCES/example-1.1.tar.gz ./example-1.1","title":"Create a compressed tarball of the files that will be including in the RPM"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#update-the-release-version-and-change-log-in-the-spec-file","text":"vi /usr/src/redhat/SPECS/example.spec","title":"Update the release version and change log in the spec file"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#build-the-rpms-for-the-various-distribution-targets","text":"rpmbuild --define 'dist .el5' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec rpmbuild --define 'dist .el6' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec.el6","title":"Build the rpms for the various distribution targets"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#repo-locations","text":"/data/redhat/linux/enterprise/5Server/en/example/i386/ /data/redhat/linux/enterprise/5Server/en/example/x86_64/ /data/redhat/linux/enterprise/6Server/en/example/x86_64/ /data/redhat/linux/enterprise/7Server/en/example/x86_64/","title":"Repo locations..."},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#copy-new-rpms-to-the-repo-directories","text":"NEWVERSION=\"1.1-4\" # set to the desired version cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/i386/ cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el6.i386.rpm /data/redhat/linux/enterprise/6Server/en/example/x86_64/ cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/x86_64/ cp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el7.i386.rpm /data/redhat/linux/enterprise/7Server/en/example/x86_64/","title":"Copy new RPMs to the repo directories"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#updating-the-repos","text":"cd /data/redhat/linux/enterprise/5Server/en/example/i386/ createrepo . cd /data/redhat/linux/enterprise/6Server/en/example/x86_64/ createrepo . cd /data/redhat/linux/enterprise/5Server/en/example/x86_64/ createrepo . cd /data/redhat/linux/enterprise/7Server/en/example/x86_64/ createrepo .","title":"Updating the repos"},{"location":"linux/RedHat/yum_rpm/yum_and_rpm/#syncing-the-yum-repos-with-the-satellite-server","text":"Login to the REDHAT satellite server and sync the channels individually spacewalk-repo-sync --channel rhel6-x86_64-example --type yum spacewalk-repo-sync --channel rhel5-x86_64-example --type yum spacewalk-repo-sync --channel rhel5-i386-example --type yum spacewalk-repo-sync --channel rhel7-x86_64-example --type yum or Sync all the channels including to RedHat ( warning this will take a long time ) [root@satellite1 ~]# /opt/example/bin/satellite-sync.sh No new packages to sync. Repo http://server.example.local/redhat/linux/enterprise/5Server/en/rhel-example/x86_64/ has 0 errata. Sync completed. Total time: 0:00:00 Repo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/ Packages in repo: 46 Packages already synced: 37 Packages to sync: 9 1/9 : kmod-vmware-tools-pvscsi-1.0.2.0-2.6.18.8.el5.3-0.x86_64 2/9 : vmware-tools-xorg-drv-mouse-12.6.4.0-4-0.x86_64 3/9 : kmod-vmware-tools-vsock-1.0.0.0-2.6.18.8.el5.3-0.x86_64 4/9 : kmod-vmware-tools-vmsync-1.1.0.1-2.6.18.8.el5.3-0.x86_64 5/9 : kmod-vmware-tools-vmci-9.0.1.1-2.6.18.8.el5.3-0.x86_64 6/9 : kmod-vmware-tools-vmmemctl-1.2.1.2-2.6.18.8.el5.3-0.x86_64 7/9 : kmod-vmware-tools-vmblock-1.1.2.0-2.6.18.8.el5.3-0.x86_64 8/9 : kmod-vmware-tools-vmhgfs-1.4.1.1-2.6.18.8.el5.3-0.x86_64 9/9 : vmware-tools-xorg-drv-display-11.0.2.10-4-0.x86_64 Repo http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/ has 0 errata. Sync completed. Total time: 0:00:04 Repo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/ Packages in repo: 90 Packages already synced: 86 Packages to sync: 4 1/4 : example-tools-1.1-4.el5-0.i386 2/4 : example-tools-1.1-213.el5-0.i386 3/4 : tidalagent-3.1.0.13-8-0.i386 Repo http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/ has 0 errata. Sync completed. Dealing with a package interdependency issue of my own creation.... To explain the issue, in the package dependencies we have: example-tools requires example at the same version example-devops requires example at the same version There is no version lock between tools and devops The upgrade appears to go as follows: (Assume installed version is \u20181\u2019, target version is \u20182\u2019 and latest in repo is \u20183\u2019 All three packages installed at version 1 Request example-tools upgrade to version 2, requires example also at version 2. Conflict as example-devops is at version 1, and requires example at version 1 yum attempts to resolve conflict, by trying to upgrade example-devops. (This is an upgrade that will just pick the latest available, so picks version 3 example-devops version 3 depends on example version 3 Conflict now between example and example-tools, so upgrade example-tools to resolve All packages now at version 3. Two things to try: Version lock all of the packages together (This will work) Create a RHN errata, and try applying it. yum install example-devops --exclude= 1.1-2 --exclude= 1.1-3 Installed: example-devops.i386 0:1.1-207.el5","title":"Syncing the YUM repos with the satellite server"},{"location":"linux/Software_Packaging/apt_get/","text":"Show all packages in the cache apt-cache pkgnames Show all packages in the cache starting with vim apt-cache show vim Show all packages in the cache mentioning vim apt-cache search vim Show the details of a certain package apt-cache pkgnames vim Show the dependencies of a package apt-cache showpkg vim Update the local cache from the upstream repos apt-get update Install a package sudo apt-get install vim Upgrade a package sudo apt-get install vim --only-upgrade Install a package version sudo apt-get install vim=2.3.5-3ubuntu1 Uninstall a package apt-get remove vim Remove config files for a package apt-get purge vim Flush the local cache apt-get clean Download a package apt-get download vim","title":"Apt get"},{"location":"programming/c_programming/fork_and_maloc_demo/","text":"simple program that I created to demonstrate the fork system call and how memory gets copied on write from one process to another #include <stdio.h> #include <unistd.h> #include <stdlib.h> #include <string.h> #include <sys/wait.h> int calc_pi() //I copied this function from the net somewhere { int r[2800 + 1]; int i, k; int b, d; int c = 0; for (i = 0; i < 2800; i++) { r[i] = 2000; } for (k = 2800; k > 0; k -= 14) { d = 0; i = k; for (;;) { d += r[i] * 10000; b = 2 * i - 1; r[i] = d % b; d /= b; i--; if (i == 0) break; d *= i; } //printf(\"%.4d\", c + d / 10000); c = d % 10000; } return 0; } int main(int argc, char ** argv) { int pid; printf(\"I am the parent proccess \\n\"); int counter = 0; while ( counter <= 2000 ) { calc_pi(); counter += 1; } int * myparentmemory = malloc(10000000 * sizeof(int)); pid = fork(); sleep(10); if (pid == 0){ sleep(1); printf(\"I am the child proccess\\n\"); argv[0][1] = 'c'; argv[0][2] = 'h'; argv[0][3] = 'i'; argv[0][4] = 'l'; argv[0][5] = 'd'; int * mychildmemory = malloc(10000000 * sizeof(int)); memset (myparentmemory,'x',5000); } else { int exitstatus = 0; sleep(1); printf(\"I am the parent of the child proccess ID %d\\n\", pid); memset (myparentmemory,'x',50000); printf(\"child exited with status of = %d\\n\",wait(&exitstatus)); } sleep(1000); return 0; }","title":"Fork and maloc demo"},{"location":"programming/c_programming/fork_and_maloc_demo/#simple-program-that-i-created-to-demonstrate-the-fork-system-call","text":"and how memory gets copied on write from one process to another #include <stdio.h> #include <unistd.h> #include <stdlib.h> #include <string.h> #include <sys/wait.h> int calc_pi() //I copied this function from the net somewhere { int r[2800 + 1]; int i, k; int b, d; int c = 0; for (i = 0; i < 2800; i++) { r[i] = 2000; } for (k = 2800; k > 0; k -= 14) { d = 0; i = k; for (;;) { d += r[i] * 10000; b = 2 * i - 1; r[i] = d % b; d /= b; i--; if (i == 0) break; d *= i; } //printf(\"%.4d\", c + d / 10000); c = d % 10000; } return 0; } int main(int argc, char ** argv) { int pid; printf(\"I am the parent proccess \\n\"); int counter = 0; while ( counter <= 2000 ) { calc_pi(); counter += 1; } int * myparentmemory = malloc(10000000 * sizeof(int)); pid = fork(); sleep(10); if (pid == 0){ sleep(1); printf(\"I am the child proccess\\n\"); argv[0][1] = 'c'; argv[0][2] = 'h'; argv[0][3] = 'i'; argv[0][4] = 'l'; argv[0][5] = 'd'; int * mychildmemory = malloc(10000000 * sizeof(int)); memset (myparentmemory,'x',5000); } else { int exitstatus = 0; sleep(1); printf(\"I am the parent of the child proccess ID %d\\n\", pid); memset (myparentmemory,'x',50000); printf(\"child exited with status of = %d\\n\",wait(&exitstatus)); } sleep(1000); return 0; }","title":"simple program that I created to demonstrate the fork system call"},{"location":"programming/c_programming/fork_demo_menu/","text":"#include <stdio.h> #include <stdlib.h> #include <string.h> #include <unistd.h> #include <sys/types.h> int main_menu(void) { int main_choice = 0; int child_choice = 0; while (!(main_choice == 1 || main_choice == 2 || main_choice == 3)) { printf(\"choose a menu option..1,2 or 3:\\n\"); printf(\"1) fork children\\n\"); printf(\"2) list children\\n\"); printf(\"3) kill children\\n\"); printf(\">>> <<<\\b\\b\\b\\b\"); scanf(\"%1d\", &main_choice); while (getchar() != '\\n'); } printf(\"%d\\n\", main_choice); if (main_choice == 2) { printf(\"listing all child process id's\\n\"); } if (main_choice == 1) { while (!(child_choice >= 1 && child_choice <= 9)) { printf(\"how many children... 1 - 9 :\\n\"); printf(\">>> <<<\\b\\b\\b\\b\"); scanf(\"%1d\", &child_choice); while (getchar() != '\\n'); } } if (main_choice == 3) { printf(\"killing all child proccess's's's's's\\n\"); } main_choice *= 10; // multiplying by 10 so i can return 2 values with one integer // 12 = fork 2 children // 19 = fork 9 children // 20 = list children // 30 = kill children return main_choice + child_choice; } int main(void) { int i; int user_choice; int child_proccess_array[10] = {0}; user_choice = main_menu(); int parent_pid = getpid(); printf(\"i am %d\\n\",getpid()); if ((user_choice > 10) && (user_choice < 20)) { int num_of_children = user_choice % 10; printf(\"forking %d child proccesses.....\\n\",num_of_children); for ( i = 1; i <= num_of_children ; i++) { pid_t child_pid = fork(); child_proccess_array[i] = child_pid; printf(\"i am %d\\n\",getpid()); printf(\"I %d forked my %d child proccess with a pid of %d\\n\",getpid(),i, child_pid); if (child_pid == 0) printf(\"%d says child pid is 0 and parent_pid = %d \\n\",getpid(),parent_pid); if (getpid() != parent_pid) break; } } if (user_choice == 20){ for ( i = 1; i <10 ; i++) { if ( child_proccess_array[i] >= 1 ) { printf(\"child proccess %d = %d\\n\", i, child_proccess_array[i]); } } } if (user_choice == 30){ for ( i = 1; i <10 ; i++) { if ( child_proccess_array[i] >= 1 ) { printf(\"killing child proccess %d = %d\\n\", i, child_proccess_array[i]); } } } //printf(\"my global_pid = %ld\\n\",global_pid); sleep(60); return 0; }","title":"Fork demo menu"},{"location":"programming/c_programming/linux_clocksource_benchmark/","text":"#include <stdio.h> #include <sys/time.h> #include <time.h> int main() { char clocksource[4][15]; clocksource[0][0] = '\\0'; clocksource[1][0] = '\\0'; clocksource[2][0] = '\\0'; clocksource[3][0] = '\\0'; clocksource[4][0] = '\\0'; char initial_clocksource[15] = {'\\0'}; struct timeval mytime; struct timezone mytimezone; clock_t start, end; int x = 0; int y = 0; //saving the initial clocksource value so we can set it back when we are finished FILE *get_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"r\"); fscanf(get_clocksource, \"%s\", initial_clocksource); //saving initial value fclose(get_clocksource); printf(\"Inital ClockSource setting is : %s \\n\",initial_clocksource); //reading in all the available clocksource oprions, this seems to vary between 2 and 4 values FILE *in_file = fopen(\"/sys/devices/system/clocksource/clocksource0/available_clocksource\", \"r\"); fscanf(in_file, \"%s %s %s %s %s\", clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]); printf(\"Available ClockSources : %s %s %s %s %s\\n\",clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]); while ( (x <= 4 ) && (clocksource[x][0] >= 1) ){ //itterate through the available clocksource values and set them as the current value FILE *current_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\"); printf(\"Testing %s \",clocksource[x]); fprintf(current_clocksource,\"%s\", clocksource[x]); x++; fclose(current_clocksource); y = 0 ;//execute the gettimeofday call start = clock(); while ( y < 1000000 ) { gettimeofday(&mytime, &mytimezone); y++; } end = clock(); printf(\"Execution took %f \\n\",((double) (end - start)) / CLOCKS_PER_SEC); } //setting the clocksource back to the initial value FILE *set_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\"); fprintf(set_clocksource,\"%s\", initial_clocksource); //setting clocksource back to initial value fclose(set_clocksource); return 0; }","title":"Linux clocksource benchmark"},{"location":"programming/c_programming/programmable_keyboard/","text":"#include <stdio.h> #include <strings.h> /* this program will generate a keyboard macro file that can be uploaded to the maxkeybaord nighthawk x8. The file can be uploaded to the keyboard by importing via the maxkeyboard \"Gaming keyboard config\" software. * * usage: binaryname < inputfile of text you want to type Maxkeyboard nighthawk x8 Macro generator Todo.. * figure out the upload protocol and validation * write a linux program to upload the macro * figure out how to upload data to the flash memory in the 8 bit micro controller Freescale MC9S08JM16 http://cache.freescale.com/files/microcontrollers/doc/data_sheet/MC9S08JM16.pdf USB-LITE Stack by CMX and the USB-MINI Stack by Freescale * capture and analyse the usbdata using usbpcap http://desowin.org/usbpcap/ and wireshark //done * write a http://desowin.org/usbpcap/dissectors.html http://www.maxkeyboard.com/download.html Details regarding the file format that the Maykeyboard software uses. The config file is a plain text file each macro is stored as a series of bytes sample blank config with the 11202 zeros redacted... [Setting] MacroLength=0 DiableWinKey=0 [BUTTONCONFIG0] FunID=255 Mappinkey=255 [MACROG0] RepeatOption=0 Macro={hex values for the macro + a checksum} MacroLength=0 [BUTTONCONFIG1] FunID=255 Mappinkey=255 [MACROG1] RepeatOption=0 Macro={hex values for the macro + a checksum} MacroLength=0 [BUTTONCONFIG2] FunID=255 Mappinkey=255 [MACROG2] RepeatOption=0 Macro={hex values for the macro + a checksum} MacroLength=0 [BUTTONCONFIG0] the macro number/id FunID=32 (seems to indicate keyboard macro) Mappinkey= this is the physical key on the keyboard that is assigned to this macro, 1 = F1 , 2 = F2 etc [MACROG0] the macro group this macro belongs to ( I haven't used this yet) RepeatOption=1 this needs to be set to 1 or the macro will not run Macro= a single line with 11204 chars, seems to be divided up into 700 x 16 byte words.. the last 2 chars on this line seem to be a checksum that isn't validated (it changes when anything else changes) MacroLength= a count of the 16 char words,this needs to be set based on the line above # The Macro format......... each char that requires a single key press takes 4 x 16 byte words .1 key value press .2 key time down .3 key value release .4 key time release ## if the inputed key requires 2 key presses eg.. upper case or special chars !@#...... then 8 x 16 byte words are needed .1 shift key value press .2 shift time down .3 key value press .4 key time down .5 key value release .6 key time release .7 shift key release .8 shift time release the timing is based on milliseconds and stored in hex the largest timing value seems to be 6375 milliseconds or 18E7 eg 50 milliseconds = 32 100 milliseconds = 64 125 milliseconds = 7D the values are stored in the file in little endian notation ( least significant byte first ) eg... the hex value 18E7 is stored as E718 Decimal = 6375 Hex = 18E7 In File = FF000000E718000000 example press of 'a' for 100 milliseconds with a 100 millisecond gap at end X = value that can be changed ____________ ____________ ____________ ____________ / \\ / \\ / \\ / \\ | Key to press || Time to press||Key to release|| Time after | 1400000001000000FF000000640000001400000000000000FF00000064000000 Key Value XX XX Static value 0000000 000000 0000000 000000 Press/Release X X Static value FF000000 FF000000 Time Value XXXX XXXX example press of 'a' for 1000 milliseconds with a 100 millisecond gap at end ____________ ____________ ____________ ____________ / \\ / \\ / \\ / \\ | Key to press || Time to press||Key to release|| Time after | 1400000001000000FF000000E60300001400000000000000FF00000064000000 Key Value XX XX Static value 0000000 000000 0000000 000000 Press/Release X X Static value FF000000 FF000000 Time Value XXXX XXXX 100mSec FF00000064000000 Key_press 00000001000000 key_release 00000000000000 to enter the exclamation mark ! on the US keyboard...requires that the shift key is pressed whilst the 1 key is pressed shift key on, 8300000001000000FF00000012020000 //this time has to extend past the next key 1 key on and off, 1100000001000000FF0000005E0000001100000000000000FF00000048010000 shiftkey off, 8300000000000000FF00000000000000 below is a list of the key values for the US keyboard layout key value Incomplete list of the control keys shift 83 capslk 1B space 57 TAB 13 L CTRL 88 L ALT 7D Enter 56 BKSPC 53 List of the keys that only require one key to the pressed a 14 b 2F c 26 d 24 e 22 f 2C g 2D h 35 i 3A j 34 k 3C l 44 m 36 n 37 o 42 p 4A q 12 r 2A s 1C t 2B u 32 v 2E w 1A x 1E y 33 z 16 1 11 2 19 3 21 4 29 5 28 6 30 7 31 8 39 9 41 0 49 - 48 = 38 [ 4B ] 3B \\ 54 ; 4C ' 4D , 3E . 46 / 4F ` 10 57 Keys that require the shift key to be pressed simultaneously ~ 10 ! 11 @ 19 # 21 $ 29 % 28 ^ 30 & 31 * 39 ( 41 ) 49 _ 48 + 38 { 4B } 3B | 54 : 4C \" 4D < 3E > 46 ? 4F A 14 B 2F C 26 D 24 E 22 F 2C G 2D H 35 I 3A J 34 K 3C L 44 M 36 N 37 O 42 P 4A Q 12 R 2A S 1C T 2B U 32 V 2E W 1A X 1E Y 33 Z 16 each macro will be an array of 11204 chars, initialized with zeros, monitoring the usb bus whilst uploading a config that contains mostly zeros not seems to be transferred to the keyboard i assume that are only used my the maxkeyboard software Macro length limit... we are limited to 700 16Byte words in each macro , #1 key value press = 16 Byte word #2 key time down = 16 Byte word #3 key value release = 16 Byte word #4 key time release = 16 Byte word this equates to 175 key press and key release actions. Note if you require multiple keys to be pressed simultaneously it will use more bytes eg the shift + key stated above make sure you do not go over the 700 limit, leave a few bytes free. If you enter a invalid macro config...sometimes all the keyboard macros stop working performing a reset from the app or uploading an empty config does not reset the key board. Strangely overwriting/applying a valid (previously exported) config containing multiple macros will restore the keyboards macro functions */ int main() { int c, char_tally, MacroLength; unsigned char key_value; int shift_status; //for future use if i decided to extend the shift press for sequential upper keys char* key_press = \"00000001000000\"; char* key_release = \"00000000000000\"; // there are 4 different timings 25 milliseconds doesn't seem to work // 50 milliseconds seems like a good mix between speed and consistency // 100 milliseconds may be needed when working through multiple remote nested RDP citrix vnx etc.. // 150 milliseconds is to hold the shift down past the timing of the upper keys char* key_time_down_25 = \"FF00000019000000\"; char* key_time_down_50 = \"FF00000032000000\"; char* key_time_down_100 = \"FF00000064000000\"; char* key_time_down_150 = \"FF00000096000000\"; char* key_time_release_25 = \"FF00000019000000\"; char* key_time_release_50 = \"FF00000032000000\"; char* key_time_release_100 = \"FF00000064000000\"; char* key_time_release_150 = \"FF00000096000000\"; char* shift_key_press = \"8300000001000000\"; char* shift_key_release = \"8300000000000000\"; char* zero_fill = \"0000000000000000\"; key_value = 0; char_tally = 0; MacroLength = 0; shift_status = 0; char single_lowwer(char c) { //printf(\"%hhX%s%s%hhX%s%s\",key_value, key_press, key_time_down_100, key_value,key_release, key_time_release_100 ); printf(\"%hhX%s%s%hhX%s%s\",key_value, key_press, key_time_down_50, key_value,key_release, key_time_release_50 ); MacroLength = MacroLength + 4; } char single_upper(char c) { printf(\"%s%s\",shift_key_press, key_time_down_150 ); printf(\"%hhX%s%s%hhX%s%s\", key_value, key_press, key_time_down_100, key_value,key_release, key_time_release_100 ); printf(\"%s%s\", shift_key_release, key_time_release_50 ); MacroLength = MacroLength + 8; } /*I acknowledge that there are more efficient ways to organize this data, such as using arrays, matrices, or other data structures. I am also aware that a lengthy if/else statement are visually unappealing. I generated the if/else statements from a bash script. */ while ((c= getchar()) != EOF) { ++char_tally; if (c == 'a' ){ key_value = 0x14; single_lowwer(c); } else if ( c == 'b' ){ key_value = 0x2F; single_lowwer(c); } else if ( c == 'c' ){ key_value = 0x26; single_lowwer(c); } else if ( c == 'd' ){ key_value = 0x24; single_lowwer(c); } else if ( c == 'e' ){ key_value = 0x22; single_lowwer(c); } else if ( c == 'f' ){ key_value = 0x2C; single_lowwer(c); } else if ( c == 'g' ){ key_value = 0x2D; single_lowwer(c); } else if ( c == 'h' ){ key_value = 0x35; single_lowwer(c); } else if ( c == 'i' ){ key_value = 0x3A; single_lowwer(c); } else if ( c == 'j' ){ key_value = 0x34; single_lowwer(c); } else if ( c == 'k' ){ key_value = 0x3C; single_lowwer(c); } else if ( c == 'l' ){ key_value = 0x44; single_lowwer(c); } else if ( c == 'm' ){ key_value = 0x36; single_lowwer(c); } else if ( c == 'n' ){ key_value = 0x37; single_lowwer(c); } else if ( c == 'o' ){ key_value = 0x42; single_lowwer(c); } else if ( c == 'p' ){ key_value = 0x4A; single_lowwer(c); } else if ( c == 'q' ){ key_value = 0x12; single_lowwer(c); } else if ( c == 'r' ){ key_value = 0x2A; single_lowwer(c); } else if ( c == 's' ){ key_value = 0x1C; single_lowwer(c); } else if ( c == 't' ){ key_value = 0x2B; single_lowwer(c); } else if ( c == 'u' ){ key_value = 0x32; single_lowwer(c); } else if ( c == 'v' ){ key_value = 0x2E; single_lowwer(c); } else if ( c == 'w' ){ key_value = 0x1A; single_lowwer(c); } else if ( c == 'x' ){ key_value = 0x1E; single_lowwer(c); } else if ( c == 'y' ){ key_value = 0x33; single_lowwer(c); } else if ( c == 'z' ){ key_value = 0x16; single_lowwer(c); } else if ( c == '1' ){ key_value = 0x11; single_lowwer(c); } else if ( c == '2' ){ key_value = 0x19; single_lowwer(c); } else if ( c == '3' ){ key_value = 0x21; single_lowwer(c); } else if ( c == '4' ){ key_value = 0x29; single_lowwer(c); } else if ( c == '5' ){ key_value = 0x28; single_lowwer(c); } else if ( c == '6' ){ key_value = 0x30; single_lowwer(c); } else if ( c == '7' ){ key_value = 0x31; single_lowwer(c); } else if ( c == '8' ){ key_value = 0x39; single_lowwer(c); } else if ( c == '9' ){ key_value = 0x41; single_lowwer(c); } else if ( c == '0' ){ key_value = 0x49; single_lowwer(c); } else if ( c == '-' ){ key_value = 0x48; single_lowwer(c); } else if ( c == '=' ){ key_value = 0x38; single_lowwer(c); } else if ( c == '[' ){ key_value = 0x4B; single_lowwer(c); } else if ( c == ']' ){ key_value = 0x3B; single_lowwer(c); } else if ( c == '\\\\' ){ key_value = 0x54; single_lowwer(c); } else if ( c == ';' ){ key_value = 0x4C; single_lowwer(c); } else if ( c == '8' ){ key_value = 0x39; single_lowwer(c); } else if ( c == '9' ){ key_value = 0x41; single_lowwer(c); } else if ( c == '0' ){ key_value = 0x49; single_lowwer(c); } else if ( c == '-' ){ key_value = 0x48; single_lowwer(c); } else if ( c == '=' ){ key_value = 0x38; single_lowwer(c); } else if ( c == '[' ){ key_value = 0x4B; single_lowwer(c); } else if ( c == ']' ){ key_value = 0x3B; single_lowwer(c); } else if ( c == '\\\\' ){ key_value = 0x54; single_lowwer(c); } else if ( c == '\\'' ){ key_value = 0x4D; single_lowwer(c); } else if ( c == ',' ){ key_value = 0x3E; single_lowwer(c); } else if ( c == '.' ){ key_value = 0x46; single_lowwer(c); } else if ( c == '/' ){ key_value = 0x4F; single_lowwer(c); } else if ( c == '`' ){ key_value = 0x10; single_lowwer(c); } else if ( c == ' ' ){ key_value = 0x57; single_lowwer(c); } //start of shift key else if ( c == '~' ){ key_value = 0x10; single_upper(c); } else if ( c == '!' ){ key_value = 0x11; single_upper(c); } else if ( c == '@' ){ key_value = 0x19; single_upper(c); } else if ( c == '#' ){ key_value = 0x21; single_upper(c); } else if ( c == '$' ){ key_value = 0x29; single_upper(c); } else if ( c == '%' ){ key_value = 0x28; single_upper(c); } else if ( c == '^' ){ key_value = 0x30; single_upper(c); } else if ( c == '&' ){ key_value = 0x31; single_upper(c); } else if ( c == '*' ){ key_value = 0x39; single_upper(c); } else if ( c == '(' ){ key_value = 0x41; single_upper(c); } else if ( c == ')' ){ key_value = 0x49; single_upper(c); } else if ( c == '_' ){ key_value = 0x48; single_upper(c); } else if ( c == '+' ){ key_value = 0x38; single_upper(c); } else if ( c == '{' ){ key_value = 0x4B; single_upper(c); } else if ( c == '}' ){ key_value = 0x3B; single_upper(c); } else if ( c == '|' ){ key_value = 0x54; single_upper(c); } else if ( c == ':' ){ key_value = 0x4C; single_upper(c); } else if ( c == '\"' ){ key_value = 0x4D; single_upper(c); } else if ( c == '<' ){ key_value = 0x3E; single_upper(c); } else if ( c == '>' ){ key_value = 0x46; single_upper(c); } else if ( c == '?' ){ key_value = 0x4F; single_upper(c); } else if ( c == 'A' ){ key_value = 0x14; single_upper(c); } else if ( c == 'B' ){ key_value = 0x2F; single_upper(c); } else if ( c == 'C' ){ key_value = 0x26; single_upper(c); } else if ( c == 'D' ){ key_value = 0x24; single_upper(c); } else if ( c == 'E' ){ key_value = 0x22; single_upper(c); } else if ( c == 'F' ){ key_value = 0x2C; single_upper(c); } else if ( c == 'G' ){ key_value = 0x2D; single_upper(c); } else if ( c == 'H' ){ key_value = 0x35; single_upper(c); } else if ( c == 'I' ){ key_value = 0x3A; single_upper(c); } else if ( c == 'J' ){ key_value = 0x34; single_upper(c); } else if ( c == 'K' ){ key_value = 0x3C; single_upper(c); } else if ( c == 'L' ){ key_value = 0x44; single_upper(c); } else if ( c == 'M' ){ key_value = 0x36; single_upper(c); } else if ( c == 'N' ){ key_value = 0x37; single_upper(c); } else if ( c == 'O' ){ key_value = 0x42; single_upper(c); } else if ( c == 'P' ){ key_value = 0x4A; single_upper(c); } else if ( c == 'Q' ){ key_value = 0x12; single_upper(c); } else if ( c == 'R' ){ key_value = 0x2A; single_upper(c); } else if ( c == 'S' ){ key_value = 0x1C; single_upper(c); } else if ( c == 'T' ){ key_value = 0x2B; single_upper(c); } else if ( c == 'U' ){ key_value = 0x32; single_upper(c); } else if ( c == 'V' ){ key_value = 0x2E; single_upper(c); } else if ( c == 'W' ){ key_value = 0x1A; single_upper(c); } else if ( c == 'X' ){ key_value = 0x1E; single_upper(c); } else if ( c == 'Y' ){ key_value = 0x33; single_upper(c); } else if ( c == 'Z' ){ key_value = 0x16; single_upper(c); } else if ( c == '\\n' ){ // printf(\"new line\\n\"); //write something so that an empty line equates to an enter maybe use get line } else printf(\"Invalid input char\\n\"); } if ( MacroLength > 700 ) printf(\"\\n\\n\\nWARNING ####### Macro length too long\\n\"); int i; for(i = (700 - MacroLength) ; i > 0 ; i--) printf(\"%s\",zero_fill); printf(\"AB\\n\");//fake checksum printf(\"MacroLength=%d\\n\" , MacroLength); printf(\"\\nzero fill bytes %d\\n\", (700 - MacroLength )); printf(\"\\n\"); return 0; }","title":"Programmable keyboard"},{"location":"puppet/puppet_study_notes/","text":"Seamus Murray's Puppet study notes as of January 2015 PuppetLabs own documentation https://puppetlabs.com/learn Sign up to the PuppetLabs learning portal https://puppetlabs.com/learn 2) go through the puppetlabs videos and questionnaires, it will give you an idea of the language used in the tests https://puppetlabs.com/learn/library some of the videos seem to be based on lessons that can be found elsewhere of the puppet website (sorry I cant find them now ) 3) puppetlabs provides a vm with puppet enterprise installed, inside the zip file there is a pdf with a series of exercises http://puppetlabs.com/download-learning-vm 4) puppet practice exam https://docs.puppetlabs.com/references/glossary.html 6) glossary of the key puppet terminology https://puppetlabs.com/services/certification/puppet-professional-practice-exam Puppet installation System requirements Puppet Enterprise 3.7 supports the following systems: +------------------+---------------------------+-------------+------------------------------------+ |Operating system |Version(s) | Arch |Component(s) | |------------------|---------------------------|-------------|------------------------------------| |RHEL |4, 5, 6, 7 |x86_64 |all (RHEL 4 supports agent only) | |CentOS |4, 5, 6, 7 |x86 & x86_64 |all (CentOS 4 supports agent only) | |Ubuntu LTS |10.04, 12.04, 14.04 |i386 & amd64 |all | |Debian |Squeeze (6), Wheezy (7) |i386 & amd64 |all | |Oracle Linux |4, 5, 6, 7 |x86 & x86_64 |all (Oracle Linux 4 agent only) | |Scientific Linux |4, 5, 6 |x86 & x86_64 |all (Scientific Linux 4 agent only) | |SUSE Enterprise |10 (SP4 only), 11(SP1), 12 |x86 & x86_64 |all (SLES 10 supports agent only) | |Solaris |10 (Update 9 or later) 11 |SPARC & i386 |agent | |Microsoft Windows |2008 2008R2 7 Ultimate SP1 |x86 & x64 |agent | | |8 Pro, 8.1 Pro 2012 2012R2 |x86 & x64 |agent | |Microsoft Windows |2003, 2003R2 |x86 |agent | |AIX |5.3, 6.1, 7.1 |Power |agent | |Mac OS X |Mavericks (10.9) |x86_64 |agent | +------------------+---------------------------+-------------+------------------------------------+ https://docs.puppetlabs.com/pe/latest/install_basic.html#about-puppet-enterprise-components the puppet enterprise 3.1 guide refers to puppet master role database support role console role Monolithic (all-in-one) Installation Monolithic installs are suitable for deployments up to 500 nodes. We recommend that your hardware meets the following: The puppet master, PE console, and PuppetDB node: at least 4-8 processor cores, 8 GB RAM All machines require very accurate timekeeping Puppet agent nodes: any hardware able to run the supported operating system For /var/, at least 1 GB of free space for each PE component on a given node For PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering For no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available Split Installation For larger deployments (500-1000, or more nodes), we recommend a split install. We recommend that your hardware meets the following: Puppet master, PE console, and PuppetDB nodes: at least 8 processor cores, 8 GB RAM (per node) All machines require very accurate timekeeping Puppet agent nodes: any hardware able to run the supported operating system For /var/, at least 1 GB of free space for each PE component on a given node For PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering For no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available Firewall Configuration Agent nodes contact the puppet master server on TCP ports 8140 (for Puppet) and 61613 (for orchestration). Any hosts that need access to the html GUI will contact the console server on port 443(can be changed to a different port). +-----------------+---------+---------+---------+----------+ | | AGENT | MASTER | CONSOLE | ADMIN PC | |-----------------|---------|---------|---------|----------| |puppet | ----> | 8140 | | | |Orchestration | 61613-->|<--61613 | | | |HTML GUI | | | 443 | <---- | |Master Installer | | 3000 | | <---- | #html installer +-----------------+---------+---------+---------+----------+ From 3.1 installation guide... https://docs.puppetlabs.com/pe/3.1/install_system_requirements.html Hardware requirements for the various puppet roles/components Puppet Enterprise\u2019s hardware requirements depend on the roles a machine performs. The puppet master role should be installed on a robust, dedicated server. Minimum requirements: 2 processor cores, 1 GB RAM, and very accurate timekeeping. Recommended requirements: 2-4 processor cores, at least 4 GB RAM, and very accurate timekeeping. Performance will vary, but this configuration can generally manage approximately 1,000 agent nodes. The database support role can be installed on the same server as the console or, optionally, on a separate, dedicated server. Minimum requirements: These will vary considerably depending on the size of your deployment. However, you\u2019ll need a machine able to handle moderate network traffic, perform processor-intensive background tasks, and run a disk-intensive PostgreSQL database server. The machine should have two to four processor cores. As a rough rule of thumb for RAM needed, start here: 1-500 nodes: 192-1024MB, 500-1000 nodes: 1-2GB, 1000-2000 nodes: 2-4 GB, 2000+ nodes, 4GB or greater. So as your deployment scales, make sure to scale RAM allocations accordingly. More information about scaling PuppetDB is available in the PuppetDB manual\u2019s scaling guidelines. The console role should usually be installed on a separate server from the puppet master, but can optionally be installed on the same server in smaller deployments. Minimum requirements: A machine able to handle moderate network traffic and perform processor-intensive background tasks. It should have a very fast network connection to the database support server, which it uses for all of the console\u2019s database requirements. Requirements will vary significantly depending on the size and complexity of your site. The optional cloud provisioner role has very modest requirements. Minimum requirements: A system which provides interactive shell access for trusted users. This system should be kept very secure, as the cloud provisioning tools must be given cloud service account credentials in order to function. The puppet agent role has very modest requirements. Minimum requirements: Any hardware able to comfortably run a supported operating system. Install script [root@master puppet-enterprise-3.7.1-el-6-x86_64]# ./puppet-enterprise-installer -h Puppet Enterprise v3.7.1 installer Puppet Enterprise documentation can be found at http://docs.puppetlabs.com/pe/3.7/ USAGE: puppet-enterprise-installer [-a ANSWER_FILE] [-A ANSWER_FILE] [-D] [-h] [-l LOG_FILE] [-n] [-q] [-s ANSWER_FILE] [-V] OPTIONS: -a ANSWER_FILE - Read answers from file and quit with error if an answer is missing. -A ANSWER_FILE - Read answers from file and prompt for input if an answer is missing. -D - Display debugging information. -h - Display this help. -l LOG_FILE - Log commands and results to file. -n - Run in 'noop' mode; show commands that would have been run during installation without running them -q - Run in quiet mode; the installation process is not displayed. Requires answer file. -s ANSWER_FILE - Save answers to file and quit without installing. -V - Display very verbose debugging information. Install the pe-agent curl -k https://<puppet master server>:8140/packages/current/install.bash | sudo bash puppet.conf /etc/puppetlabs/puppet/puppet.conf {puppet enterprise} or /etc/puppet/puppet.conf {puppet opensource} [main] certname = learning.puppetlabs.vm vardir = /var/opt/lib/pe-puppet logdir = /var/log/pe-puppet rundir = /var/run/pe-puppet basemodulepath = /etc/puppetlabs/puppet/modules:/opt/puppet/share/puppet/modules environmentpath = /etc/puppetlabs/puppet/environments server = learning.puppetlabs.vm user = pe-puppet group = pe-puppet archive_files = true archive_file_server = learning.puppetlabs.vm module_groups = base+pe_only [agent] report = true classfile = $vardir/classes.txt localconfig = $vardir/localconfig graph = true pluginsync = true environment = production SSL certificates and accurate time If there is a clock sync issue between an agent and the master you will receive certificate verification errors.. root@node3:~# puppet agent -t Warning: Unable to fetch my node definition, but the agent run will continue: Warning: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Info: Retrieving plugin Error: /File[/var/lib/puppet/lib]: Failed to generate additional resources using 'eval_generate': SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Error: /File[/var/lib/puppet/lib]: Could not evaluate: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Could not retrieve file metadata for puppet://puppet/plugins: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Error: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Warning: Not using cache on failed catalog Error: Could not retrieve catalog; skipping run Error: Could not send report: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] resolution sync the clock's and restart puppet agent service or manually run puppet agent -t Resetting a nodes certificate +--------------------------------+-----------------------------+ | PUPPET AGENT | PUPPET MASTER | |--------------------------------|-----------------------------| | service puppet stop | | | puppet config print ssldir | | | rm -rf /var/lib/puppet/ssl | | | | puppet cert list --all | | | puppet cert clean <certname>| | puppet agent -t | | | | puppet cert list | | | puppet sign <certname> | | puppet agent -t | | | service puppet start | | +--------------------------------+-----------------------------+ Until the puppet master signs the nodes cert the agent will display the following message root@node3:~# puppet agent -t Exiting; no certificate found and waitforcert is disabled Puppet Cert command line options puppet cert <action> <host> clean: Revoke a host's certificate (if applicable) and remove all files related to that host from puppet cert's storage. This is useful when rebuilding hosts, since new certificate signing requests will only be honored if puppet cert does not have a copy of a signed certificate for that host. If '--all' is specified then all host certificates, both signed and unsigned, will be removed. fingerprint: Print the DIGEST (defaults to the signing algorithm) fingerprint of a host's certificate. generate: Generate a certificate for a named client. A certificate/keypair will be generated for each client named on the command line. list: List outstanding certificate requests. If '--all' is specified, signed certificates are also listed, prefixed by '+', and revoked or invalid certificates are prefixed by '-' (the verification outcome is printed in parenthesis). print: Print the full-text version of a host's certificate. revoke: Revoke the certificate of a client. The certificate can be specified either by its serial number (given as a hexadecimal number prefixed by '0x') or by its hostname. The certificate is revoked by adding it to the Certificate Revocation List given by the 'cacrl' configuration option. Note that the puppet master needs to be restarted after revoking certificates. sign: Sign an outstanding certificate request. verify: Verify the named certificate against the local CA certificate. reinventory: Build an inventory of the issued certificates. This will destroy the current inventory file specified by 'cert_inventory' and recreate it from the certificates found in the 'certdir'. Ensure the puppet master is stopped before running this action. Node Classification site.pp ENC Certname By default each nodes certification name is its FQDN. When the puppet agent on the nodes request at catalogue from the puppet master it does so using its current FQDN {even if they had already generated a certificate and had it signed by the master}. Therefore if the nodes host or domain name changes it will make a request based on the new name and you will have to sign a new cert on the master. Classification of nodes In puppet the term classification refers to the linking/assigning of classes to nodes. This can be achieved in many ways ** Using site.pp ** node '<group name>' { include <class> } node '<node name>' { include <class> } ** Using the puppet enterprise console** Click on Classification in the console navigation bar. ( Optional steps to create a new group ) Enter a new node group name click Add group Click on the desired group and set the rules for this group based on facter values name/osfamily etc... click add rule Click commit change If your changes to the site.pp manifest are'nt reflected in a puppet run triggered by the puppet agent -t command, try running the command again. ** Using an ENC External node Classifier ** An executable that can be called by puppet master; it doesn\u2019t have to be written in Ruby. Its only argument is the name of the node to be classified, and it returns a YAML document describing the node. Node statements are an optional feature of Puppet. They can be replaced by or combined with an external node classifier. You can also use conditional statements upon facts to classify nodes. site.pp syntax The Default Node The site.pp file has an optional default node declaration . It follows the same format as a normal node declaration except the word default (without quotes) is used in place of a cert/host name. If a node requests a catalogue and there is no matching cert/host name in the site.pp then the node will be assigned what ever settings have be set for the default node. You can simply use the node name : node 'www.example.com' { include common include apache, squid } You can use a comma-separated list of names to create a group of nodes with a single node statement: node 'www1.example.com', 'www2.example.com', 'www3.example.com' { include common include apache, squid } You can use Regular expressions (regexes) can be used as node names. In puppet regexes must be surrounded by forward slashes / match any node with the name www followed by 1 or more digits (www1 , www12345) node /^www\\d+$/ { include common } match foo.example.com and bar.example.com node /^(foo|bar)\\.example\\.com$/ { include common } If site.pp contains at least one node definition, it must have one for every node; compilation for a node will fail if one cannot be found. (Hence the usefulness of the default node.) If site.pp contains no node definitions, this requirement is dropped. Matching If a nodes certname/hostname is matched by more than one node statement it will only get the contents of one node definition. Puppet will do the following checks in order when deciding which definition to use: If there is a node definition with the node\u2019s exact name, Puppet will use it. If there is a regular expression node statement that matches the node\u2019s name, Puppet will use it. (If more than one regex node matches, Puppet will use one of them, with no guarantee as to which.) If the node\u2019s name looks like a fully qualified domain name (i.e. multiple period-separated groups of letters, numbers, underscores and dashes), Puppet will chop off the final group and start again at step 1. (That is, if a definition for www01.example.com isn\u2019t found, Puppet will look for a definition matching www01.example.) Puppet will use the default node. Thus, for the node www1.example.com, Puppet would try the following, in order: www1.example.com A regex that matches www1.example.com www1.example A regex that matches www1.example www1 A regex that matches www1 default In my testing with a site.pp node 'node2.puppetlabs' { include hosts } node /^node(2|4)/ { include regexmod include definedmod include ntp } with the node/cert name of node2.puppetlabs it is matched my the most accurate name ie \"node2.puppetlabs\" if the site.pp only has the shortname and a shortname+regex then the regex its matched eg.. node 'node2' { include hosts } node /^node(2|4)/ { include regexmod include definedmod include ntp } Merging site.pp With ENC Data Node definitions and external node classifiers can co-exist. Puppet merges their data as follows: Variables from an ENC are set at top scope and can thus be overridden by variables in a node definition. Classes from an ENC are declared at node scope, which means they will be affected by any variables set in the node definition. Although both will work together it is recommend to use either node definitions or an ENC. External nodes override node configuration in the manifest files. If you enable an external node classifier, any duplicate node definitions in your manifest files will not be processed; they will be ignored by Puppet. The use of LDAP nodes overrides node definitions in your manifest files and your ENC. If you use LDAP node definitions, you cannot define nodes in your manifest files or in an ENC. Inheritance In earlier versions of Puppet, nodes could inherit from other nodes using the inherits keyword. This feature is deprecated in Puppet 3.7, node classification in puppet enterprise console click the classification tab select the group or create a new group set the rules to match the node static matching , add the nodes FQDN in the certname field and pin the node to the group dynamic matching , create a fact + operator + value based rule to match the node, e.g. Table detailing possible matching rules +-----------+----------------+------------------------+ | FACT | OPERATOR | VALUE or REGEX | +-----------+----------------+------------------------+ | hostname | is | pe-node2 | | name | is | pe-node3.puppetlabs | | name | matches regex | pe-node\\d.puppetlabs | | name | matches regex | node(4|5).puppetlabs | | osfamily | is | RedHat | +-----------+----------------+------------------------+ Note Don't forget to click add Rule and then click commit x change default node group PE comes preconfigured with the default node group . This is the root of the node group hierarchy and is a parent to all other node groups. The classes that are assigned to the default node group are applied to all of the nodes in your deployment. Deleting Nodes from puppet enterprise console There are three options for deleting a node: Hide Node Removes the node from the node list view. To hide a node: On the Nodes page, click the node. Click Hide. Delete Node Removes all reports and information for the node from the console. The node no longer appears in the list of nodes on the Nodes page, but it continues to appear in Matching nodes until it is purged from PuppetDB. The node will reappear as a new node on the Nodes page if it submits a new Puppet run report. To delete a node: On the Nodes page, click the node. Click Delete. Deactivate Node Completely deactivates the node and frees up the license assigned to the node. Any deactivated node will be reactivated if PuppetDB receives new catalogues or facts for it. For details, see Deactivating a PE agent node. Stop the agent service on the node you want to deactivate. On the Puppet master, deactivate the agent; run puppet node deactivate {NODE NAME} . This deactivates the agent in PuppetDB. In some cases, the PE license count in the console will not decrease for up to 24 hours, but you can restart the pe-memcached service to update the license count sooner. On the Puppet master, revoke the agent certificate; run puppet cert revoke {AGENT CERTNAME} . Still on the Puppet master, run puppet agent -t to kick off a Puppet run. This Puppet run will copy the certificate revocation list (CRL) to the correct SSL directory for delivery to the agent. Restart the Puppet master with service pe-puppetserver restart . The certificate is only revoked after running service pe-puppetserver restart. In addition, the Puppet server process won\u2019t re-read the certificate revocation list until the service is restarted. If you don\u2019t run service pe-puppetserver restart, the node will check in again on the next Puppet run and re-register with PuppetDB, which will increment the license count again. Delete the node from the console. In the console, click Nodes. Click the node that you want to delete and click the Delete button. This action does NOT disable MCollective/live management on the node. To disable MCollective/live management on the node, uninstall the Puppet agent, stop the pe-mcollective service (on the agent, run service pe-mcollective stop), or destroy the agent altogether. On the agent, remove the node certificates in /etc/puppetlabs/mcollective/ssl/clients . clean the cert from the pauppet master ... puppet cert clean {AGENT CERTNAME} Note: If you delete a node from the node view without first deactivating the node, the node will be absent from the node list in the console, but the license count will not decrease, and on the next Puppet run, the node will be listed in the console. At this point, the node should be fully deactivated. Regex in puppet Regexes in Puppet cannot have options or encodings appended after the final slash. However, you may turn options on or off for portions of the expression using the (? : ) and (?- : ) notation. The following example enables the i option while disabling the m and x options: $packages = $operatingsystem ? { /(?i-mx:ubuntu|debian)/ => 'apache2', /(?i-mx:centos|fedora|redhat)/ => 'httpd', } The following options are allowed: i \u2014 Ignore case m \u2014 Treat a newline as a character matched by . x \u2014 Ignore white space and comments in the pattern Puppet run overview The SSL checks are made first The agent checks for a certificate matching its FQDN If one is not found the agent generates one. The agent checks for the CA cert. if not it will send its cert to the CA servers and request for its cert to be signed. If plugin sync is enabled, the agent checks the master for new plugins and downloads them if necessary. The agent ask's Facter for a set of facts about itself. { facter -p } The agent sends the facts the master whilst requesting a catalog. The master injects those facts as variables in the root scope and processes the manifests. The master then sends the catalog to the agent. The agent apply's the catalog. If reporting is enabled the agent sends a report back to the master. Diagram of puppet run +-----------------------+ +-----------------------+ | AGENT | | MASTER | | +-----------------+ | | | | | Check local Cert| | | +--------------+ | | | create if needed| | | +-->| Send CA Cert | | | +-----------------+ | | | +--------------+ | | +------------------+ | | | | | | Check 4 copy of | | | | | | | CA Cert,retrieve | | | | | | | if needed |<---------+ | | +------------------+ | | | | +-------------------+ | | +------------------+ | | | Is my Cert signed |<-------->| Sign Client Cert | | | +-------------------+ | | +------------------+ | | | | | | +-----------------+ | | +-----------------+ | | | Request plugins |---------> | Send Plugins | | | +-----------------+ | | +-----------------+ | | | | | | | +---------------- + | | | | | | Import plugins |<-------------------+ | | +-----------------+ | | | | | | | | +-----------------+ | | +--------------+ | | | | | | | CLASSIFY | | | | REQUEST CATALOG +------------> |The Node based| | | | Send node name | | | |on certname + | | | | and facts to | | | | node groups | | | | the master | | | +--------------+ | | | | | | ______|______ | | ------------------+ | | | | | | | | | +--V--++--V--++--V--+ | | | | |CLASS||CLASS||CLASS| | | +---------------+ | | +-----++-----++-----+ | | | | | | \\ | / | | | CATALOG | <-----------+ +-V---V----V-+ | | | | | | | | | | | +--------+------+ | | | | COMPILE | | | | | | | | | | | +--------V---------+ | | | +------------+ | | |APPLY +--------+ | | | | V | | | | QUERY | | | | | | | | | | STATUS | | | | +----<---+ | | | +----|---+ | | | | | | +------V----+ | | | | | | | ENFORCE | | | | | | | | DEFINED | | | | | | | | STATE | | | | | | | +-----------+ | | | +-------------+ | | +----+-------------+ | | | | | | | | | | REPORT | | | |--------------------> | | | | | | | +-------------+ | | +--------V---------+ | | | | | DEFINED SYSTEM | | | | | | STATE | | | | | +------------------+ | | | +-----------------------+ +-----------------------+ Resources ---} Classes ---} Manifest Manifests A puppet manifest is text file that contains puppet code and is appended by the .pp file extension. Catalog In a standard agent + master puppet configuration, the agent never receives a copy of the modules, manifests, functions or variables. The agent only receives the compiled catalogue of resources and relationships. It is not recommended by puppetlabs, however you can send each node an entire copy of the manifests for the nodes the compile the catalog and apply it locally, This may be desirable in situations where there is no direct network connectivity between the agent and master. Warning: each node will be able to see the entire manifest which may contain sensitive information relating to other nodes in your environment. Modules Modules are just directories and files with a predictable structure located in the module path. The following commands will display the module path. # puppet agent --configprint modulepath # puppet config print modulepath /etc/puppet/modules:/usr/share/puppet/modules Modules will often have a main class that shares the name of the module. module-name |-manifests/ |-files/ |-templates/ |-lib/ |-tests/ |-spec/ If a class is defined in a module you can then declare that class in any manifest by name. Autoloading in Puppet means that your modules will be loaded by Puppet at compile time, as long as they follow a predictable structure. puppet _\\ compile _\\ load run / catalog / modules Example module's tree /etc/puppetlabs/puppet/environments/production/modules/sshd /etc/puppetlabs/puppet/environments/production/modules/sshd \u251c\u2500\u2500 files \u2502 \u2514\u2500\u2500 sshd_config \u251c\u2500\u2500 manifests \u2502 \u2514\u2500\u2500 init.pp \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp example module with class includes cowsayings \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 init.pp -----> class cowsayings { \u2502 \u2502 include cowsayings::cowsay \u2502 \u2502 include cowsayings::fortune \u2502 \u2502 } \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 cowsay.pp -----> class cowsayings::cowsay { \u2502 \u2502 package { 'cowsay': \u2502 \u2502 ensure => 'present', \u2502 \u2502 } \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 fortune.pp -----> class cowsayings::fortune { \u2502 package { 'fortune-mod': \u2502 ensure => 'present', \u2502 } \u2502 } \u2502 \u2514\u2500\u2500 tests \u2502 \u251c\u2500\u2500 init.pp --> include cowsayings \u2502 \u2502 \u251c\u2500\u2500 cowsay.pp --> include cowsayings::cowsay \u2502 \u2502 \u2514\u2500\u2500 fortune.pp --> include cowsayings::fortune puppet module command line #puppet module <action> ACTIONS: build - Build a module release package. changes - Show modified files of an installed module. generate - Generate boilerplate for a new module. install - Install a module from the Puppet Forge or a release archive. list - List installed modules search - Search the Puppet Forge for a module. uninstall - Uninstall a puppet module. upgrade - Upgrade a puppet module. When uploading a module to the forge your must include a ** modulefile ** containing the modules metadata (name, version, source, author, description, etc.) Resources Each resource describes some aspect of a system and its state e.g.... a service that must be running a package that must be installed a user that must be configured A resource declaration is the puppet block of code that describes a resource. Resource declarations are written the in the puppet DML Declarative Modelling Language. Puppet's DML is a declarative language rather than an imperative one. This means that instead of defining a process or set of commands, Puppet code describes (or declares) only the desired end state, and relies on built-in providers to deal with implementation. Resource declaration syntax example type { 'title': attribute_1 => value_1, attribute_2 => value_2, } type and title must be unique for a node values must be alphanumeric (quote strings) each attribute + value pair must be followed by a comma Individual resources are combined together to represent the desired system configuration. Similar resources can be grouped into types. such as the user type Resource Abstraction Layer +---------------------------------------+ | file | package | service | user | Resources |---------------------------------------| | Ruby | Apt | Redhat | Useradd | Providers | | Yum | Launchd | LDAP | | | Gems | SMF | Netinfo | | | Deb | Debian | | | | RPM | | | +---------------------------------------+ Query a resource using the puppet command line tool puppet resource <type> {display all the instances of the specified type} puppet resource package puppet resource host puppet resource user puppet resource <type> <title> {display the details of a particular resources instance } puppet resource package apache puppet resource host example.com puppet resource user seamus puppet resource --type {list all types} puppet describe <type> {display a description of the type and its options} Puppet Apply You can use the Puppet resource declaration syntax with the puppet apply tool to make quick changes to resources on the system. You can either change the values directly from the command line using the syntax.. puppet apply -e \"user { 'seamus': ensure => 'present', }\" Or you can open the resource declaration in vim with the syntax... puppet apply -e user seamus The resource will be opened in vim. Make any desired changed and when you quit vim the new resource values will be applied. Defined Resource Type (aka defined types or defines ) are types that can be evaluated multiple times using different parameters during declaration. Upon each new declaration with new parameters they act like a new resource type. Seamus sample module using defined resources [root@master modules]# tree definedmod definedmod \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 definedresource.pp \u2502 \u2514\u2500\u2500 init.pp \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp [root@master modules]# cat definedmod/manifests/init.pp class definedmod { definedresource {'mydefinedresource': var2 => 'twovar', var1 => 'variable1', var3 => 'thirdvariable', } } [root@master modules]# cat definedmod/manifests/definedresource.pp define definedresource ($var1, $var2 = default2 , $var3 = default3) { file { \"/tmp/${var1}\" : ensure => present, content => \"this file name is not $var2 or $var3\", } file { \"/tmp/${var2}\" : ensure => present, content => \"this file name is not $var1 or $var3\", } } [root@master modules]# cat definedmod/tests/init.pp include definedmod [root@node4 ~]# cat /tmp/variable1 this file name is not twovar or thirdvariable [root@node4 ~]# cat /tmp/twovar this file name is not variable1 or thirdvariable [root@node4 ~]# cat /tmp/thirdvariable this file name is not variable1 or twovar[root@node4 ~]# Resource duplicates Instances of type + title must be singleton/unique for each node, otherwise the nodes catalog will fail to compile. sample duplicate resource declarations: class resourceduplicates { user { 'user1': # duplicate title of the user resource below; # the nodes catalog will fail to compile. ensure => 'present', uid => '1007', } user { 'user1': ensure => 'present', uid => '1007', # duplicate UID's, puppet catalog will compile; } # But the useradd provider will fail on the client. } sample error of duplicate resource: Error: Could not retrieve catalogue from remote server: Error 400 on SERVER: Duplicate declaration: User[user2] is already declared in file Resource refresh Puppet will refresh a service for each instance of a refresh, e.g if 3 files are being updated and each update triggers a refresh of the same service 3 separate refreshes will be triggered doe each of the file updates. # This is a test to see if puppet will restart a service multiple time or- # just once when multiple refresh events are triggered class loop { file { '/tmp/loop1' : ensure => present, content => \"$::uptime_seconds \\n\", # the \"uptime_seconds\" is used here to ensure that } # the content of the file will change upon each run file { '/tmp/loop2' : ensure => present, content => \"$::uptime_seconds \\n\", } file { '/tmp/loop3' : ensure => present, content => \"$::uptime_seconds \\n\", } service { 'crond' : ensure => running, } File['/tmp/loop1'] ~> Service['crond'] File['/tmp/loop2'] ~> Service['crond'] File['/tmp/loop3'] ~> Service['crond'] } Resource ordering and relationships with metaparameters Via a mechanism called autorequires ; For some of the built in resource types such as users and groups, Puppet is able to automatically determine the dependency relationships among some of the built-in resources such as user and groups without a user/admin having to declare the resource interdependencies. For the majority of resources there is no autorequire and therefore the order of resources in a puppet manifest is generally ignored. If a group of resources have interdependencies you should use one of the metaparameters to specify the relationships between them before - Causes a resource to be applied before the target resource require - Causes a resource to be applied after the target resource notify - Causes a resource to be applied before the target resource. The target resource will refresh if the notifying resource changes. subscribe - Causes a resource to be applied after the target resource. The subscribing resource will refresh if the target resource changes. Note: when creating a resource reference use the syntax Type['title'] Note: Use lower-case for the type name when declaring it but use a capitalised type name when calling it. metaparameter declaration examples... before package { 'openssh-server': ensure => present, before => File['/etc/ssh/sshd_config'], } require file { '/etc/ssh/sshd_config': ensure => file, mode => 600, source => 'puppet:///modules/sshd/sshd_config', require => Package['openssh-server'], } Ordering Arrows Package['openssh-server'] -> File['/etc/ssh/sshd_config'] Causes the resource on the left to be applied before the resource on the right. notify file { '/etc/ssh/sshd_config': ensure => file, mode => 600, source => 'puppet:///modules/sshd/sshd_config', notify => Service['sshd'], } subscribe service { 'sshd': ensure => running, enable => true, subscribe => File['/etc/ssh/sshd_config'], } Notification Arrow File['/etc/ntp.conf'] ~> Service['ntpd'] Causes the resource on the left to be applied first and sends a refresh to the resource on the right if the resource on the left changes. Complete example class sshd { package { 'openssh-server': ensure => present, before => File['/etc/ssh/sshd_config'], } file { '/etc/ssh/sshd_config': ensure => file, mode => '0600', source => 'puppet:///modules/sshd/sshd_config', } } service { 'ssh': ensure => running, enable => true, subscribe => File['/etc/ssh/sshd_config'], } extra metaparameters.. Manage dependencies (before, require, subscribe, notify, stage) Manage resources' application policies (audit, noop, schedule, loglevel) Add information to a resource (alias, tag) Virtual resources are a hack to get around puppets limitation of sigleton resource definitions. They enable an a particular instance of a resource type+title to be used in multiple classes. They are defined as virtual by pre-pending them with the \"@\" symbol @user { seamus: ensure => present } They can then be declared in multiple classes by using one of the following 3 syntaxes using either the realise function or the spaceship collector. User <| title == seamus |> realize User[seamus] realize(User[seamus]) Exported resources Enable a resource that is defined on a host to be exported to the puppetdb and then be used by 1 or more other hosts. Puppet collects and stores the exported resources during configuration runs. /etc/hosts example [root@master modules]# cat exportedresource/manifests/init.pp class exportedresource { @@host { $hostname: ## sends the local nodes host resource to the puppetdb and ## tags it as exported ip => $::ipaddress, name => $::fqdn, host_aliases => $::hostname, comment => 'addded by the exported resource class', } Host <<| |>> ## retrieves all the host resources from the puppetdb } SSH example class ssh::hostkeys { @@sshkey { \"${::fqdn}_dsa\": host_aliases => [ $::fqdn, $::hostname, $::ipaddress ], type => dsa, key => $::sshdsakey, } @@sshkey { \"${::fqdn}_rsa\": host_aliases => [ $::fqdn, $::hostname, $::ipaddress ], type => rsa, key => $::sshrsakey, } } SSH example of retrieving the exported resources and applying them class ssh::knownhosts { Sshkey <<| |>> { ensure => present } } The ssh::knownhosts class should be included in the catalog for all nodes where Puppet should manage the SSH known_hosts file. Notice that we\u2019ve used double angle braces to collect resources from PuppetDB. define balancermember($url) { file { '/etc/httpd/conf.d.members/worker_${name}.conf': ensure => file, owner => 'root', group => 'root', mode => '0644', content => \"BalancerMember $url \\n\", } } This configuration file fragment contains a single line, the URL to a member of the load balancer pool. Puppet recommends using a defined resource type because all resources declared within the type will be exported when the defined type itself is exported. class loadbalancer_members { Balancermember <<| |>> { notify => Service['apache'] } } This code uses the double angle brace syntax to collect all balancermember resources from the stored configuration database. In addition, it uses a parameter block to notify the Apache service of any changes Puppet makes to the balancermember resources. Just as with virtual resources, a parameter block may be specified to add further parameters to collected resources. Removing retired nodes from PuppetDB # puppet node deactivate mail.example.com Submitted 'deactivate node' for <node_name> with UUID aaaaa-bbbbb-ccccc-ddddd-eeee After you\u2019ve run this on the puppet master, any resources exported by this node will no longer be collected on your Puppet clients. Note: Deactivated node resources will not be removed from the systems that collected them. You will need to clean up those configurations manually; or some resources can be purged using the resource metatype. Exec Resource Exec resources execute external commands, it is important that any commands executed are Idempotent Any command in an exec resource must be able to be run multiple times without casing harm. There are 3 main ways for commands in exec resources to be idempotent 1. The commands them selves can be idempotent 2. You use an (onlyif, unless, or creates) attribute, which will prevent puppet from running a specific command unless a specific condition is met. 3. The exec resource has the ( refreshonly=>true ) value, which only allows puppet to run the command when some other resource is changed syntax exec { 'resource-title': attribute_1 => value_1, attribute_2 => value_2, } example exec { \"updatedb': path => '/usr/sbin', command => 'updatedb', } Note: if you do not set the command value it will default to the exec resource title. You must specifiy the path because the exec resource does not inherit paths. Classes Classes define a collection of resources that are managed together as a single unit. You can also think of them as named blocks of Puppet code, which are created in one place and invoked elsewhere. Using a Puppet class requires two steps. First, you'l need to define it by writing a class definition and saving it in a manifest file. When Puppet runs, it will parse this manifest and store your class definition. Secondly, The class can then be declared to apply it to nodes in your infrastructure. example class for ssh that contains 3 resources { package, file, service}.. resource class package \\ file >- ssh service / Example class syntax....{with no relationships defined} class ssh { package { 'openssh-clients': ensure => present, } file { '/etc/ssh/ssh_config': ensure => file, owner => 'root', group => 'root', source => 'puppet:///modules/ssh/ssh_config', } service { 'sshd': ensure => running, enable => true, } } Defining a class specifies the contents and behaviour but does not automatically include (apply) it in a configuration Declaring a class, includes the class in the catalogue which will then be applied upon next agent run. To declare a class use either of the following syntaxes in site.pp.. or in /tests/init.pp ** include my_calss ** or ** class { 'my_class': } ** classes are reusable classes are singleton classes can only be used once per node When applying a class make sure you run puppet apply module/tests/init.pp and NOT puppet apply module/manifest/init.pp otherwise no actions will take place and no errors will be logged, this is because you would be defining the class but not declared it anywhere (on a node). Parameterised Classes Class parameters provide a method to set variables in a class as it's declared. The syntax for parameterised classes is similar to resource declarations. class { 'ntp': servers => ['node1.example.com','node2.example.com','node3.example.com'] } The servers parameter can be populated with a single server or an array of servers. Parameterised class definitions can be set in the site.pp file. node default { # This is where you can declare classes for all nodes. # Example: # class { 'my_class': } class { 'ntp': servers => ['node1.example.com','node2.example.com','node3.example.com'] } } You can also override values with class { '::mysql::server': override_options => { 'mysqld' => { 'max_connections' => '1024' } }, } Class Inheritance / Derived Classes By using the inherits keyword classes can be derived from other classes. When a derived class is declared, its base class is automatically declared first and the variable are set as the parent scope. The new class receives a copy of all the base class's variables and resource defaults. Code in the derived class is able to override any resource attributes that were originally set in the base class. Class Inheritance is only useful for overriding resource attributes. For any other use case it is better archive your objective by using some other method. example... We create a new class zsh::developer that will enable us to deploy the zshrc.dev file instead of the zshrc file on certain nodes.. [root@learn modules]# tree zsh/ zsh/ \u251c\u2500\u2500 files \u2502 \u251c\u2500\u2500 zshrc \u2502 \u2514\u2500\u2500 zshrc.dev \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 developer.pp \u2502 \u2514\u2500\u2500 init.pp \u251c\u2500\u2500 Modulefile \u251c\u2500\u2500 README \u251c\u2500\u2500 spec \u2502 \u2514\u2500\u2500 spec_helper.rb \u2514\u2500\u2500 tests \u251c\u2500\u2500 developer.pp \u2514\u2500\u2500 init.pp cat zsh/manifests/init.pp class zsh { package { 'zsh': ensure => present, before => File['/etc/zshrc'], } file { '/etc/zshrc': ensure => file, owner => 'root', group => 'root', source => 'puppet:///modules/zsh/zshrc', } } [root@learn modules]# cat zsh/manifests/developer.pp #Note there is no need for a \"package { 'zsh':\" resource definition here as it will be inherited from the zsh class class zsh::developer inherits zsh { File ['/etc/zshrc'] { source => 'puppet:///modules/zsh/zshrc.dev', } } [root@learn modules]# cat zsh/tests/init.pp include zsh [root@learn modules]# cat zsh/tests/developer.pp include zsh::developer Variables and Class Parameters Variables are prefixed with the $ sign and assigned with the = operator $mystring = 'this is my string' $mypath = '/tmp/puppet/' Once a variable it is defined it can be used anywhere in the manifest in place of an regular assigned value. Unlike resource declarations, variable assignments are parse-order dependent. This means that you must assign a variable in your manifest before you can use it. If you try to use a variable that has not been defined, the Puppet parser won't complain. Instead, Puppet will treat the variable as having the special undef value. You can only assign a variable once within a single scope. Once it's assigned, the value cannot be changed. (therefore it should be called a constant but hey its not like the puppet people actually follow best practices or common conventions anywhere else so why would this be any different) Variable interpolation enables a string that is stored as a variable to be inserted into another string. eg.. file { \"${mypath}file1.txt\": ... } file { \"${mypath}file2.txt\": ... } Note: A string that includes an interpolated variable must be wrapped in double quotation marks (\"...\"), rather than the single quotation marks that surround an ordinary string Class Parameters provide a method of setting the variables within a class when the class is declared rather than when it is defined. Once defined a Parameterised Class can be declared with a similar syntax to a resource declaration When defining a class, include a list of parameters and optional default values between the class name and the opening curly brace. So a parameterised class is defined as below: class classname ( $parameter = 'default' ) { ... } Once defined, a parameterised class can be declared with a syntax similar to that of resource declarations, including key value pairs for each parameter you want to set. class {'classname': parameter => 'value', } Seamus sample module using defined resource type class with paramaters (as described in the \"defined resource section above\" [root@master modules]# tree definedmod definedmod \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 definedresource.pp \u2502 \u2514\u2500\u2500 init.pp \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp [root@master modules]# cat definedmod/manifests/init.pp class definedmod { definedresource {'mydefinedresource': var2 => 'twovar', var1 => 'variable1', var3 => 'thirdvariable', } } [root@master modules]# cat definedmod/manifests/definedresource.pp define definedresource ($var1, $var2 = default2 , $var3 = default3) { file { \"/tmp/${var1}\" : ensure => present, content => \"this file name is not $var2 or $var3\", } file { \"/tmp/${var2}\" : ensure => present, content => \"this file name is not $var1 or $var3\", } } [root@master modules]# cat definedmod/tests/init.pp include definedmod [root@node4 ~]# cat /tmp/variable1 this file name is not twovar or thirdvariable [root@node4 ~]# cat /tmp/twovar this file name is not variable1 or thirdvariable [root@node4 ~]# cat /tmp/thirdvariable this file name is not variable1 or twovar[root@node4 ~]# puppet-lint A 3rd party tool used to display common syntax and style errors. Packaged as a ruby gem. gem install puppet-lint puppet-lint /etc/puppetlabs/puppet/modules/ntplint/manifests/init.pp you can ignore certain syntax checks on command line puppet-lint <path>/<file>.pp --no-80chars-check puppet-lint <path>/<file>.pp --no-ensure_first_param-check ~/.puppet-lint.rc --no-80chars-check --no-ensure_first_param-check using a rake file to test multiple manifests in one go by including the following values in your rake file require 'puppet-lint/tasks/puppet-lint' PuppetLint.configuration.send(\"disable_<check>\") Idempotency By default puppet modules describe the desired final state rather than detail a series of step to follow, this mean that no matter how many times they are run the same end state will occur. Puppet Console The puppet enterprise console is a web-based gui. It can help.. manage node requests to join the puppet deployment assign puppet classes to nodes and groups view reports and activity graphs browse and compare resources on your nodes view inventory data manage console users and their access privileges Event Inspector The Event Inspector is part of the Puppet Enterprise (PE) Console. It is a reporting tool that provides data for investigating the current state of your infrastructure. Its focus is on correlating information and presenting it from multiple perspectives, in order to reveal common causes behind related events. Event inspector lets you accomplish two important tasks: monitoring a summary of your infrastructure\u2019s activity and analyzing the details of important changes and failures. It displays events from three perspectives Classes Nodes and Resources Resetting the admin user password This must be performed from a shell session on the console server and executing a ruby script that was part of the Puppet Enterprise installer. q_puppet_enterpriseconsole_auth_password=newpassword q_puppetagent_certname=$(puppet config print certname) /opt/puppet/bin/ruby update-superuser-password.rb Environments By default, all nodes are assigned to a default environment named production. There are three ways to assign nodes to a different environment: Via your ENC or node terminus Via each agent node\u2019s puppet.conf Via the PE console to set the environment for each node group. Assigning Environments Via an ENC The interface to set the environment for a node will be different for each ENC. Some ENCs cannot manage environments. When writing an ENC, simply ensure that the environment: key is set in the YAML output that the ENC returns. See the documentation on writing ENCs for details. If the environment key isn\u2019t set in the ENC\u2019s YAML output, the Puppet master will just use the environment requested by the agent. Assigning Environments Via the Agent\u2019s Config File Assigning Environment via agent config In puppet.conf on each agent node, you can set the environment setting in either the agent or main config section. When that node requests a catalog from the Puppet master, it will request that environment. Note: If you are using an ENC and it specifies an environment for that node, it will override whatever is in the config file. Assigning Environments via PE console Click Classification - select node group - click \"edit node group metadata\" - select enviroment Scope Scope helps to organise classes, telling Puppet where to look within the module directory structure to find each class. It also separates namespaces within the module and your Puppet manifests, preventing conflicts between variables or classes with the same name. The init.pp in the /manifest/ directory must contain a class with the same name as the module name Class name = mysql File location = /modules/mysql/manifests/init.pp ) Class name = mysql::server File location = /modules/mysql/manifests/server.pp Class name = mysql::server::account_security File location = /modules/mysql/manifests/server/account_security.pp Types and Providers A type defines the interface for a resource: the set of properties you can use to define a desired state for the resource, and the parameters that don't directly map to things on the system, but tell Puppet how to manage the resource. Both properties and parameters appear in the resource declaration syntax as attribute value pairs. command to show the types available [root@master ~]# puppet resource --type +-----------------------------------------------------------------------------+ | anchor | mcx | puppetdb_conn_validator | | apt_key | mount | resources | | augeas | nagios_command | router | | computer | nagios_contact | schedule | | cron | nagios_contactgroup | scheduled_task | | exec | nagios_host | selboolean | | file | nagios_hostdependency | selmodule | | file_line | nagios_hostescalation | service | | filebucket | nagios_hostextinfo | ssh_authorized_key | | firewall | nagios_hostgroup | sshkey | | firewallchain | nagios_ser^ice | stage | | group | nagios_ser|icedependency | tidy | | host | nagios_ser|iceescalation | user | | ini_setting | nagios_ser|iceextinfo | vlan | | ini_subsetting | nagios_servicegroup | whit | | interface | nagios_timeperiod | yumrepo | | k5login | notify | zfs | | macauthorization | package | zone | | mailalias | postgresql_conf | zpool | | maillist | postgresql_psql | | +-----------------------------------------------------------------------------+ Core Types File manages local files and ensures if a file should exist sets its parameters present / absent file / directory / link source puppet:///modules/ Package Manages software packages ensures that a package is present / absent / latest /purged / version Service Manages services ensures running / stopped enable true / false notify echo's message to the agent run-time log message => \" hello world ! \", exec executes an arbitrary command on the agent node. cron manages cron jobs user manages user accounts ensure present / absent / role name, uid, gid, groups, home, shell group ensure present / absent name, gid A provider is what does the heavy lifting to bring the system into line with the state defined by a resource declaration. Providers are implemented for a wide variety of supported operating systems. They are a key component of the Resource Abstraction Layer (RAL), translating the universal interface defined by the type into system-specific implementations. Resource Abstraction Layer +---------------------------------------+ | file | package | service | user | Resources |---------------------------------------| | Ruby | Apt | Redhat | Useradd | Providers | | Yum | Launchd | LDAP | | | Gems | SMF | Netinfo | | | Deb | Debian | | | | RPM | | | +---------------------------------------+ Conditional Statements **Note: ** string matches are not case sensitive if (optional elseif and else) class accounts ($name) { if $::operatingsystem == 'centos' { $groups = 'wheel' } elsif $::operatingsystem == 'debian' { $groups = 'admin' } else { fail( \"This module doesn't support ${::operatingsystem}.\" ) } notice ( \"Groups for user ${name} set to ${groups}\" ) ... } unless unless $memorysize > 1024 { $maxclient = 500 } case case $::operatingsystem { 'CentOS': { $apache_pkg = 'httpd' } 'Redhat': { $apache_pkg = 'httpd' } 'Debian': { $apache_pkg = 'apache2' } 'Ubuntu': { $apache_pkg = 'apache2' } default: { fail(\"Unrecognised operating system for webserver.\") } } package { $apache_pkg : ensure => present, } selector Selector statements are similar to case statements, but return a value instead of executing a code block. Selectors can only be used at places in the code where a plain value is expected an not inside another selector or case statement. $rootgroup = $::osfamily ? { 'Solaris' => 'wheel', 'Darwin' => 'wheel', 'FreeBSD' => 'wheel', 'default' => 'root', } my sample class with all the selectors [root@pe-puppet modules]# cat noticemod/manifests/init.pp class noticemod { notify { \"Hello World!\": } ## SELECTOR statement notify { \" SELECTOR warning !\": message => $seamus ? { 'mytrue' => \" the value is true.\", 'myfalse' => \" the value is false.\", default => \" the value is not set.\", }, # selector statements warn you if no match is found and there is no default } ## CASE statement case $seamus { 'mytrue': { notify { \" CASE warning ! the value is true.\": } } 'myfalse': { notify { \" CASE warning ! the value is false.\": } } default: { fail(\" CASE warning ! failing now... if the variable is undef the whole catalog compile will fail\") } } # case statements silently fall through the bottom if no match is found ## IF statement if 'rue' in $seamus { notify { \" IF warning ! the value is true.\": } } elsif 'alse' in $seamus { notify { \" IF warning ! the value is false.\": } } else { notify { \" IF warning ! the value does not match.\": } } ## UNLESS statement unless $seamus == 'myfalse' { notify { \" UNLESS statement - \\$seamus does not == myfalse, it is set to $seamus \": } } unless $seamus == 'mytrue' { notify { \" UNLESS statement - \\$seamus does not == mytrue, it is set to $seamus \": } } } PuppetDB stores the most recent facts from every node the most recent catalog from every node (optionally) 7 days of event reports from every node it is searchable using either... PuppetDB's query API Puppet's inventory service exported resources example [root@learning ~]# puppet node status learning.puppetlabs.vm learning.puppetlabs.vm Currently active Last catalog: 2015-01-24T13:31:16.291Z Last facts: 2015-01-24T13:31:03.782Z Declaring an exported resource causes that resource to be added to the catalog and marked with an \u201cexported\u201d flag, which prevents the puppet agent from managing the resource. When PuppetDB receives the catalog, it stores a record for each resource with the flag set. Delete reports older then 1 month : sudo -u puppet-dashboard rake RAILS_ENV=production reports:prune upto=1 unit=mon Hiera https://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/ Hiera is a key/value lookup tool for configuration data, built to make puppet better and let you set node-specific data without repeating yourself. Hiera support is built into puppet 3, and is available as an add-on for puppet 2.7. It keeps site specific data out of the manifest. Puppet classes can query any data they need and hiera will act as a site wide config file. Hiera makes it easier to: 1. separate configuration data from the modules code. 2. configure your own nodes: default data with multiple levels of overrides. 3. re-use public puppet modules: you don't have to edit the code you just have to put the necessary data in hiera. 4. makes it easier to publish your own developed modules to the forge without clashing variable names or having to redact your config values. 5. create common data for most nodes and.. - Override some values for machines located at a particular facility - Override some values for specific machines 6. enables you to only write down the differences that are needed (doesn't make sense) To get started with hiera there are 5 steps.. 1) download and install hiera 2) create a hiera.yaml config file 3) arrange a hierachy that suite your site and data 4) write your data sources 5) configure puppet to use hiera https://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/ hiera - Performs a standard priority lookup and returns the most specific value for a given key. The returned value can be data of any type (strings, arrays, or hashes). hiera_array - Returns all matches throughout the hierarchy \u2014 not just the first match \u2014 as a flattened array of unique values. If any of the matched values are arrays, they\u2019re flattened and included in the results. hiera_hash - Returns a merged hash of matches from throughout the hierarchy. In cases where two or more hashes share keys, the hierarchy order determines which key/value pair will be used in the returned hash, with the pair in the highest priority data source winning. example of using hiera.... [root@learn ~]# vim /etc/puppetlabs/puppet/hieradata/common.yaml --- message: This string is the value that is returned when hiera('message') is called. motd: Hello there Seamus test that the key is set correctly using the puppet apply -e [root@learn hieradata]# puppet apply -e 'notice(hiera(\"motd\"))' Notice: Scope(Class[main]): Hello there Seamus Notice: Compiled catalog for learn.puppetlabs.com in environment production in 0.04 seconds Notice: Finished catalog run in 0.02 seconds Edit the motd module to use the key from hiera instead of the value in the module files [root@learn hieradata]# cd /etc/puppetlabs/puppet/modules/ [root@learn modules]# tree motd/ motd/ \u251c\u2500\u2500 manifests \u2502 \u2514\u2500\u2500 init.pp \u251c\u2500\u2500 Modulefile \u251c\u2500\u2500 README \u251c\u2500\u2500 spec \u2502 \u2514\u2500\u2500 spec_helper.rb \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp [root@learn modules]# vim motd/manifests/init.pp Class: motd class motd { file { '/etc/motd': ensure => file, owner => 'root', group => 'root', content => (hiera(\"motd\")), } } [root@learn modules]# puppet parser validate motd/manifests/init.pp [root@learn modules]# puppet apply motd/tests/init.pp Notice: Compiled catalog for learn.puppetlabs.com in environment production in 0.09 seconds Notice: /Stage[main]/Motd/File[/etc/motd]/ensure: defined content as '{md5}13c0016553b5315a8cd66f13b23f14a3' Notice: Finished catalog run in 0.03 seconds [root@learn modules]# cat /etc/motd Hello there Seamus Facter Facter stores facts as pairs of keys and values. Facter command line options -y, --yaml Emit facts in YAML format. -j, --json Emit facts in JSON format. --plaintext Emit facts in plaintext format. --trace Enable backtraces. --external-dir DIR The directory to use for external facts. --no-external-dir Turn off external facts. -d, --debug Enable debugging. -t, --timing Enable timing. -p, --puppet Load the Puppet libraries, thus allowing Facter to load Puppet-specific facts. -v, --version Print the version and exit. -h, --help Print this help message. A shell enviroment variable prepened with the name FACTER_ will be readable by facter as a fact. eg... [root@node1 ~]# export FACTER_seamus=murray [root@node1 ~]# facter seamus murray [root@node1 ~]# facter |grep seamus seamus => murray Facter values can be temporarily overridden by setting a shell variable prepended with FACTER_ and the name of the fact. eg... FACTER_operatingsystem=Debian You can even test the effect that the fake fact will have using puppet apply.. FACTER_operatingsystem=Debian puppet apply --noop accounts/tests/init.p The best way to distribute facts is to include them in your modules using the puppet plug-ins. Puppet will distribute the custom facts, custom types, providers and functions to any host that includes the module. you can either distribute the facts with the modules that need them or you can create a module combining all your custom facts needed by all your other modules. Facter is called by the puppet agent. Facts apear as normal and top scope variables they can be called by $ipaddress or ${::ipaddress} (best practice) Pluginsync Custom Facts and types can be exported to the the agent nodes from the puppet master using the plugin sync mechanism. Before the client requests a catlog it checks to see if the are any new plugins or if its plugins have been updated. If there has been a change it pulls down the plugins and executes the code in the custom types and facts. This and be disabled from the agent nodes puppet.conf config file [main] pluginsync = false Custom facts you can create a custom fact with a bit of ruby code on the puppet master and use plugins and modules to distribute it to the clients/nodes Create a ruby file in <modulename>/lib/facter/<modulename>.rb For simple shell commands, just insert the shell command in between the setcode do and the end . For more complex comands use Facter::Core::Execution.exec(' ') between the setcode do and the end . Facter.add('mycustomfact') do setcode do Facter::Core::Execution.exec('/bin/date') end end on the agents use plugin sync to automatically pull down the facts to /var/lib/puppet/lib/facter/mycustomfact.rb puppet agent -t use the -p facter flag to test the puppet fact facter -p mycustomfact A fact can be confined to run only if another fact is of a certain value e.g. Facter.add(:powerstates) do confine :kernel => 'Linux' setcode do Facter::Core::Execution.exec('cat /sys/power/states') end end External facts Can be written in any language as long as they return key/value pairs to stdout. They are stored in .. <MODULEPATH>/<MODULE>/facts.d/ and are distributed to the clients via the pluginsync mechanism. Example fact written in bash... $ cat/etc/puppet/modules/myexternalfact/facts.d/myexternalfacts.sh #!/bin/bash echo \"myexternalfact1=`/bin/df -P / |/usr/bin/tail -1 | /bin/awk '{print $5}'`\" echo \"extfact1=one\" echo \"extfact2=two\" echo \"extfact3=three\" Calling the external fact from the command line on a node that has recieved the external fact by pluginsync... $ facter --external-dir /var/lib/puppet/facts.d/ extfact1 extfact2 extfact3 myexternalfact1 myexternalfact1 => 33% extfact1 => one extfact2 => two extfact3 => three Example fact written in c... #include <stdio.h> int main() { printf(\"my_external_fact_in_c=\\\"Hello World\\\"\\n\"); return 0; } Calling the external fact written in c... # facter --external-dir ~/puppet-study/external-facts/ my_external_fact_in_c \"Hello World\" Distributing the external facts to the puppet client nodes It is best to put the external facts into a module so that they can be distributed to all the agent nodes via plugin sync.. [root@master myexternalfact]# tree /etc/puppet/modules/myexternalfact /etc/puppet/modules/myexternalfact \u2514\u2500\u2500 facts.d \u251c\u2500\u2500 myexternalfactinc \u2514\u2500\u2500 myexternalfact.sh [root@master ~]# ls -l /etc/puppet/modules/myexternalfact/facts.d/ total 16 -rwxr-xr-x. 1 root root 8523 Feb 10 00:27 myexternalfactinc -rwxr-xr-x. 1 root root 235 Feb 10 00:36 myexternalfact.sh Trigger the plugin sync to trasfer the new external facts.. [root@node1 ~]# puppet agent -t Use the puppet apply -e tool on the agent nodes to test the facter values.. [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_c}\" ) ' Notice: Scope(Class[main]): \"Hello World\" Notice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds Notice: Finished catalog run in 0.03 seconds [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_1}\" ) ' Notice: Scope(Class[main]): 32% Notice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds Notice: Finished catalog run in 0.03 seconds [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_2}\" ) ' Notice: Scope(Class[main]): two Notice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds Notice: Finished catalog run in 0.03 seconds Structured facts Can come straight from YAML,JSON or txt files in the directories.. /etc/facter/facts.d/ or /etc/puppetlabs/facter/facts.d/ $ cat /etc/facter/facts.d/mystructuredfact.yaml --- mystructuredfact1: factone mystructuredfact2: facttwo mystructuredfact3: factthree $ facter mystructuredfact1 mystructuredfact2 mystructuredfact3 mystructuredfact1 => factone mystructuredfact2 => facttwo mystructuredfact3 => factthree Roles and Profiles Roles and profiles are just modules written in a specific way. roles are assigned to nodes profiles are assigned to roles example role class role::weibserver inherits role { include profile:: web } class role::dbserver inherits role { include profile::db } class role::webdbserver inherits role { include profile:web include profile::db } classification = assignment of classes to nodes classification = assignment of roles to nodes example profile class profile::web { include apache include php include tomcat include jdk include memcache } __ Tip: classes are singletons, they can be included multiple times on a node, but they will only be evaluated once . A defined class can be declared multiple times, because it takes parameters and each new declaration will be evaluated. You can see we\u2019ve included our two classes but not the definition, apache::vhost. This is because of some module magic called autoloading. Puppet scans your module and loads any .pp file in the manifests directory that is named after the class it contains; for example, the install.pp file contains the apache::install class and so is autoloaded. The same thing happens with definitions: The vhost.pp file contains the definition apache::vhost, and Puppet autoloads it. However, as we declare definitions, for example calling apache::vhost where we need it, we don\u2019t need to do an include apache::vhost because calling it implies inclusion. ............................................... class puppet::params { $puppetserver = hiera('puppetserver') } In Puppet 3, but not Puppet 2.7, there is an automatic lookup of parameters in a parameterized class. We can use this to rewrite the puppet::params class further: class puppet::params ( $puppetserver, ){ } When this is called with no arguments, Puppet 3 will attempt to look up the puppet::params::puppetserver key in Hiera and populate it accordingly. It will not fail unless the Hiera lookup fails. Puppet Templates Puppet supports templates written in the ERB templating language, which is part of the Ruby standard library. Templates are always evaluated by the parser, not by the client. This means that if you are using a puppet master server, then the templates only need to be on the server, and you never need to download them to the client. The client sees no difference between using a template and specifying all of the text of the file as a string. Note that the template function simply returns a string, which can be used as a value anywhere \u2014 the most common use is to fill file contents, but templates can also provide values for variables: ags The tags available in an ERB file depend on the way the ERB processor is configured. Puppet always uses the same configuration for its templates (see \u201ctrim mode\u201d below), which makes the following tags available: ** <%= Ruby expression %> ** \u2014 This tag will be replaced with the value of the expression it contains. ** <% Ruby code %> ** \u2014 This tag will execute the code it contains, but will not be replaced by a value. Useful for conditional or looping logic, setting variables, and manipulating data before printing it. ** <%# comment %> ** \u2014 Anything in this tag will be suppressed in the final output. ** <%% or %%> ** \u2014 A literal <% or %>, respectively. ** <%- \u2014 Same as <% ** , but suppresses any leading whitespace in the final output. Useful when indenting blocks of code for readability. ** -%> ** \u2014 Same as ** %> ** , but suppresses the subsequent line break in the final output. Useful with many lines of non-printing code in a row, which would otherwise appear as a long stretch of blank lines. Puppet parser Is the part of puppet that pareses the puppet DSL code in manifests files. It can be called from the command line to validate your puppet code [root@pe-puppet]# puppet parser validate /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp Error: Could not parse for environment production: Syntax error at '{'; expected '}' at /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp:3 Functions Functions are extensions of the Puppet Parser. They are located and are executed on the master and only have access to resources and data that is available on the master. During the parsing of the manifest's any function code is executed and the subsequent return value/s are inserted into the resulting compilation. There are two types of functions: statements and rvalues. Statements , such as the fail function, which stops the Puppet run with a parser error, perform some action. Rvalues return values and can be used anywhere a normal value is expected. (This includes resource attributes, variable assignments, conditions, selector values, the arguments of other functions, etc.) These values can come from a variety of places; template function reads and evaluates a template to return a string. stdlib\u2019s str2bool and num2bool functions convert values from one data type to another. split function parses a string and returns array elements. RBAC Role Based Access Control Puppet enterprise ships with 3 default User Roles Administrators Operators Viewers Todo................................. Seamus learn the +> operator Does site.pp override the values in an enc? Does the values in an enc override the values in a class? Yumrepo['custom_packages'] -> Package <| tag == 'custom' |> Will create an order relationship with several package resources? Using puppet node to set password when the nodes cant contact the password server, do you use an exec resource ? Do you have to configure the agents to use heira or just the master? What is a node_terminus where is it set and what overrides it https://docs.puppetlabs.com/guides/external_nodes.html Learn the spaceship operator The puppet db has multiple terminus's where data is stored Facts Catalogs Resources Watch the puppetdb talk by james sweanie puppetny https://www.youtube.com/watch?v=HTr4b02aU7A at 14 mins Do the values set in the console override enc or site.pp (where are the values stored) What is the effect of node default {} in site.pp can this be overridden by ENC or by a specific node hostname {} in site.pp https://docs.puppetlabs.com/guides/external_nodes.html Workout resource interdependency loops ............ sources of information I used to study 1) {book} pro puppet second edition Last update: (made some formatting changes on 13 Feb 2024)","title":"Puppet study notes"},{"location":"puppet/puppet_study_notes/#puppetlabs-own-documentation-httpspuppetlabscomlearn","text":"Sign up to the PuppetLabs learning portal https://puppetlabs.com/learn 2) go through the puppetlabs videos and questionnaires, it will give you an idea of the language used in the tests https://puppetlabs.com/learn/library some of the videos seem to be based on lessons that can be found elsewhere of the puppet website (sorry I cant find them now ) 3) puppetlabs provides a vm with puppet enterprise installed, inside the zip file there is a pdf with a series of exercises http://puppetlabs.com/download-learning-vm 4) puppet practice exam https://docs.puppetlabs.com/references/glossary.html 6) glossary of the key puppet terminology https://puppetlabs.com/services/certification/puppet-professional-practice-exam","title":"PuppetLabs own documentation https://puppetlabs.com/learn"},{"location":"puppet/puppet_study_notes/#puppet-installation","text":"","title":"Puppet installation"},{"location":"puppet/puppet_study_notes/#system-requirements","text":"Puppet Enterprise 3.7 supports the following systems: +------------------+---------------------------+-------------+------------------------------------+ |Operating system |Version(s) | Arch |Component(s) | |------------------|---------------------------|-------------|------------------------------------| |RHEL |4, 5, 6, 7 |x86_64 |all (RHEL 4 supports agent only) | |CentOS |4, 5, 6, 7 |x86 & x86_64 |all (CentOS 4 supports agent only) | |Ubuntu LTS |10.04, 12.04, 14.04 |i386 & amd64 |all | |Debian |Squeeze (6), Wheezy (7) |i386 & amd64 |all | |Oracle Linux |4, 5, 6, 7 |x86 & x86_64 |all (Oracle Linux 4 agent only) | |Scientific Linux |4, 5, 6 |x86 & x86_64 |all (Scientific Linux 4 agent only) | |SUSE Enterprise |10 (SP4 only), 11(SP1), 12 |x86 & x86_64 |all (SLES 10 supports agent only) | |Solaris |10 (Update 9 or later) 11 |SPARC & i386 |agent | |Microsoft Windows |2008 2008R2 7 Ultimate SP1 |x86 & x64 |agent | | |8 Pro, 8.1 Pro 2012 2012R2 |x86 & x64 |agent | |Microsoft Windows |2003, 2003R2 |x86 |agent | |AIX |5.3, 6.1, 7.1 |Power |agent | |Mac OS X |Mavericks (10.9) |x86_64 |agent | +------------------+---------------------------+-------------+------------------------------------+ https://docs.puppetlabs.com/pe/latest/install_basic.html#about-puppet-enterprise-components the puppet enterprise 3.1 guide refers to puppet master role database support role console role Monolithic (all-in-one) Installation Monolithic installs are suitable for deployments up to 500 nodes. We recommend that your hardware meets the following: The puppet master, PE console, and PuppetDB node: at least 4-8 processor cores, 8 GB RAM All machines require very accurate timekeeping Puppet agent nodes: any hardware able to run the supported operating system For /var/, at least 1 GB of free space for each PE component on a given node For PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering For no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available Split Installation For larger deployments (500-1000, or more nodes), we recommend a split install. We recommend that your hardware meets the following: Puppet master, PE console, and PuppetDB nodes: at least 8 processor cores, 8 GB RAM (per node) All machines require very accurate timekeeping Puppet agent nodes: any hardware able to run the supported operating system For /var/, at least 1 GB of free space for each PE component on a given node For PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering For no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available","title":"System requirements"},{"location":"puppet/puppet_study_notes/#firewall-configuration","text":"Agent nodes contact the puppet master server on TCP ports 8140 (for Puppet) and 61613 (for orchestration). Any hosts that need access to the html GUI will contact the console server on port 443(can be changed to a different port). +-----------------+---------+---------+---------+----------+ | | AGENT | MASTER | CONSOLE | ADMIN PC | |-----------------|---------|---------|---------|----------| |puppet | ----> | 8140 | | | |Orchestration | 61613-->|<--61613 | | | |HTML GUI | | | 443 | <---- | |Master Installer | | 3000 | | <---- | #html installer +-----------------+---------+---------+---------+----------+ From 3.1 installation guide... https://docs.puppetlabs.com/pe/3.1/install_system_requirements.html","title":"Firewall Configuration"},{"location":"puppet/puppet_study_notes/#hardware-requirements-for-the-various-puppet-rolescomponents","text":"Puppet Enterprise\u2019s hardware requirements depend on the roles a machine performs. The puppet master role should be installed on a robust, dedicated server. Minimum requirements: 2 processor cores, 1 GB RAM, and very accurate timekeeping. Recommended requirements: 2-4 processor cores, at least 4 GB RAM, and very accurate timekeeping. Performance will vary, but this configuration can generally manage approximately 1,000 agent nodes. The database support role can be installed on the same server as the console or, optionally, on a separate, dedicated server. Minimum requirements: These will vary considerably depending on the size of your deployment. However, you\u2019ll need a machine able to handle moderate network traffic, perform processor-intensive background tasks, and run a disk-intensive PostgreSQL database server. The machine should have two to four processor cores. As a rough rule of thumb for RAM needed, start here: 1-500 nodes: 192-1024MB, 500-1000 nodes: 1-2GB, 1000-2000 nodes: 2-4 GB, 2000+ nodes, 4GB or greater. So as your deployment scales, make sure to scale RAM allocations accordingly. More information about scaling PuppetDB is available in the PuppetDB manual\u2019s scaling guidelines. The console role should usually be installed on a separate server from the puppet master, but can optionally be installed on the same server in smaller deployments. Minimum requirements: A machine able to handle moderate network traffic and perform processor-intensive background tasks. It should have a very fast network connection to the database support server, which it uses for all of the console\u2019s database requirements. Requirements will vary significantly depending on the size and complexity of your site. The optional cloud provisioner role has very modest requirements. Minimum requirements: A system which provides interactive shell access for trusted users. This system should be kept very secure, as the cloud provisioning tools must be given cloud service account credentials in order to function. The puppet agent role has very modest requirements. Minimum requirements: Any hardware able to comfortably run a supported operating system.","title":"Hardware requirements for the various puppet roles/components"},{"location":"puppet/puppet_study_notes/#install-script","text":"[root@master puppet-enterprise-3.7.1-el-6-x86_64]# ./puppet-enterprise-installer -h Puppet Enterprise v3.7.1 installer Puppet Enterprise documentation can be found at http://docs.puppetlabs.com/pe/3.7/ USAGE: puppet-enterprise-installer [-a ANSWER_FILE] [-A ANSWER_FILE] [-D] [-h] [-l LOG_FILE] [-n] [-q] [-s ANSWER_FILE] [-V] OPTIONS: -a ANSWER_FILE - Read answers from file and quit with error if an answer is missing. -A ANSWER_FILE - Read answers from file and prompt for input if an answer is missing. -D - Display debugging information. -h - Display this help. -l LOG_FILE - Log commands and results to file. -n - Run in 'noop' mode; show commands that would have been run during installation without running them -q - Run in quiet mode; the installation process is not displayed. Requires answer file. -s ANSWER_FILE - Save answers to file and quit without installing. -V - Display very verbose debugging information.","title":"Install script"},{"location":"puppet/puppet_study_notes/#install-the-pe-agent","text":"curl -k https://<puppet master server>:8140/packages/current/install.bash | sudo bash","title":"Install the pe-agent"},{"location":"puppet/puppet_study_notes/#puppetconf","text":"/etc/puppetlabs/puppet/puppet.conf {puppet enterprise} or /etc/puppet/puppet.conf {puppet opensource} [main] certname = learning.puppetlabs.vm vardir = /var/opt/lib/pe-puppet logdir = /var/log/pe-puppet rundir = /var/run/pe-puppet basemodulepath = /etc/puppetlabs/puppet/modules:/opt/puppet/share/puppet/modules environmentpath = /etc/puppetlabs/puppet/environments server = learning.puppetlabs.vm user = pe-puppet group = pe-puppet archive_files = true archive_file_server = learning.puppetlabs.vm module_groups = base+pe_only [agent] report = true classfile = $vardir/classes.txt localconfig = $vardir/localconfig graph = true pluginsync = true environment = production","title":"puppet.conf"},{"location":"puppet/puppet_study_notes/#ssl-certificates-and-accurate-time","text":"If there is a clock sync issue between an agent and the master you will receive certificate verification errors.. root@node3:~# puppet agent -t Warning: Unable to fetch my node definition, but the agent run will continue: Warning: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Info: Retrieving plugin Error: /File[/var/lib/puppet/lib]: Failed to generate additional resources using 'eval_generate': SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Error: /File[/var/lib/puppet/lib]: Could not evaluate: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Could not retrieve file metadata for puppet://puppet/plugins: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Error: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Warning: Not using cache on failed catalog Error: Could not retrieve catalog; skipping run Error: Could not send report: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] resolution sync the clock's and restart puppet agent service or manually run puppet agent -t","title":"SSL certificates and accurate time"},{"location":"puppet/puppet_study_notes/#resetting-a-nodes-certificate","text":"+--------------------------------+-----------------------------+ | PUPPET AGENT | PUPPET MASTER | |--------------------------------|-----------------------------| | service puppet stop | | | puppet config print ssldir | | | rm -rf /var/lib/puppet/ssl | | | | puppet cert list --all | | | puppet cert clean <certname>| | puppet agent -t | | | | puppet cert list | | | puppet sign <certname> | | puppet agent -t | | | service puppet start | | +--------------------------------+-----------------------------+ Until the puppet master signs the nodes cert the agent will display the following message root@node3:~# puppet agent -t Exiting; no certificate found and waitforcert is disabled","title":"Resetting a nodes certificate"},{"location":"puppet/puppet_study_notes/#puppet-cert-command-line-options","text":"puppet cert <action> <host> clean: Revoke a host's certificate (if applicable) and remove all files related to that host from puppet cert's storage. This is useful when rebuilding hosts, since new certificate signing requests will only be honored if puppet cert does not have a copy of a signed certificate for that host. If '--all' is specified then all host certificates, both signed and unsigned, will be removed. fingerprint: Print the DIGEST (defaults to the signing algorithm) fingerprint of a host's certificate. generate: Generate a certificate for a named client. A certificate/keypair will be generated for each client named on the command line. list: List outstanding certificate requests. If '--all' is specified, signed certificates are also listed, prefixed by '+', and revoked or invalid certificates are prefixed by '-' (the verification outcome is printed in parenthesis). print: Print the full-text version of a host's certificate. revoke: Revoke the certificate of a client. The certificate can be specified either by its serial number (given as a hexadecimal number prefixed by '0x') or by its hostname. The certificate is revoked by adding it to the Certificate Revocation List given by the 'cacrl' configuration option. Note that the puppet master needs to be restarted after revoking certificates. sign: Sign an outstanding certificate request. verify: Verify the named certificate against the local CA certificate. reinventory: Build an inventory of the issued certificates. This will destroy the current inventory file specified by 'cert_inventory' and recreate it from the certificates found in the 'certdir'. Ensure the puppet master is stopped before running this action.","title":"Puppet Cert command line options"},{"location":"puppet/puppet_study_notes/#node-classification-sitepp-enc","text":"","title":"Node Classification  site.pp ENC"},{"location":"puppet/puppet_study_notes/#certname","text":"By default each nodes certification name is its FQDN. When the puppet agent on the nodes request at catalogue from the puppet master it does so using its current FQDN {even if they had already generated a certificate and had it signed by the master}. Therefore if the nodes host or domain name changes it will make a request based on the new name and you will have to sign a new cert on the master.","title":"Certname"},{"location":"puppet/puppet_study_notes/#classification-of-nodes","text":"In puppet the term classification refers to the linking/assigning of classes to nodes. This can be achieved in many ways ** Using site.pp ** node '<group name>' { include <class> } node '<node name>' { include <class> } ** Using the puppet enterprise console** Click on Classification in the console navigation bar. ( Optional steps to create a new group ) Enter a new node group name click Add group Click on the desired group and set the rules for this group based on facter values name/osfamily etc... click add rule Click commit change If your changes to the site.pp manifest are'nt reflected in a puppet run triggered by the puppet agent -t command, try running the command again. ** Using an ENC External node Classifier ** An executable that can be called by puppet master; it doesn\u2019t have to be written in Ruby. Its only argument is the name of the node to be classified, and it returns a YAML document describing the node. Node statements are an optional feature of Puppet. They can be replaced by or combined with an external node classifier. You can also use conditional statements upon facts to classify nodes.","title":"Classification of nodes"},{"location":"puppet/puppet_study_notes/#sitepp-syntax","text":"","title":"site.pp syntax"},{"location":"puppet/puppet_study_notes/#the-default-node","text":"The site.pp file has an optional default node declaration . It follows the same format as a normal node declaration except the word default (without quotes) is used in place of a cert/host name. If a node requests a catalogue and there is no matching cert/host name in the site.pp then the node will be assigned what ever settings have be set for the default node. You can simply use the node name : node 'www.example.com' { include common include apache, squid } You can use a comma-separated list of names to create a group of nodes with a single node statement: node 'www1.example.com', 'www2.example.com', 'www3.example.com' { include common include apache, squid } You can use Regular expressions (regexes) can be used as node names. In puppet regexes must be surrounded by forward slashes / match any node with the name www followed by 1 or more digits (www1 , www12345) node /^www\\d+$/ { include common } match foo.example.com and bar.example.com node /^(foo|bar)\\.example\\.com$/ { include common } If site.pp contains at least one node definition, it must have one for every node; compilation for a node will fail if one cannot be found. (Hence the usefulness of the default node.) If site.pp contains no node definitions, this requirement is dropped. Matching If a nodes certname/hostname is matched by more than one node statement it will only get the contents of one node definition. Puppet will do the following checks in order when deciding which definition to use: If there is a node definition with the node\u2019s exact name, Puppet will use it. If there is a regular expression node statement that matches the node\u2019s name, Puppet will use it. (If more than one regex node matches, Puppet will use one of them, with no guarantee as to which.) If the node\u2019s name looks like a fully qualified domain name (i.e. multiple period-separated groups of letters, numbers, underscores and dashes), Puppet will chop off the final group and start again at step 1. (That is, if a definition for www01.example.com isn\u2019t found, Puppet will look for a definition matching www01.example.) Puppet will use the default node. Thus, for the node www1.example.com, Puppet would try the following, in order: www1.example.com A regex that matches www1.example.com www1.example A regex that matches www1.example www1 A regex that matches www1 default In my testing with a site.pp node 'node2.puppetlabs' { include hosts } node /^node(2|4)/ { include regexmod include definedmod include ntp } with the node/cert name of node2.puppetlabs it is matched my the most accurate name ie \"node2.puppetlabs\" if the site.pp only has the shortname and a shortname+regex then the regex its matched eg.. node 'node2' { include hosts } node /^node(2|4)/ { include regexmod include definedmod include ntp }","title":"The Default Node"},{"location":"puppet/puppet_study_notes/#merging-sitepp-with-enc-data","text":"Node definitions and external node classifiers can co-exist. Puppet merges their data as follows: Variables from an ENC are set at top scope and can thus be overridden by variables in a node definition. Classes from an ENC are declared at node scope, which means they will be affected by any variables set in the node definition. Although both will work together it is recommend to use either node definitions or an ENC. External nodes override node configuration in the manifest files. If you enable an external node classifier, any duplicate node definitions in your manifest files will not be processed; they will be ignored by Puppet. The use of LDAP nodes overrides node definitions in your manifest files and your ENC. If you use LDAP node definitions, you cannot define nodes in your manifest files or in an ENC. Inheritance In earlier versions of Puppet, nodes could inherit from other nodes using the inherits keyword. This feature is deprecated in Puppet 3.7,","title":"Merging site.pp With ENC Data"},{"location":"puppet/puppet_study_notes/#node-classification-in-puppet-enterprise-console","text":"click the classification tab select the group or create a new group set the rules to match the node static matching , add the nodes FQDN in the certname field and pin the node to the group dynamic matching , create a fact + operator + value based rule to match the node, e.g. Table detailing possible matching rules +-----------+----------------+------------------------+ | FACT | OPERATOR | VALUE or REGEX | +-----------+----------------+------------------------+ | hostname | is | pe-node2 | | name | is | pe-node3.puppetlabs | | name | matches regex | pe-node\\d.puppetlabs | | name | matches regex | node(4|5).puppetlabs | | osfamily | is | RedHat | +-----------+----------------+------------------------+ Note Don't forget to click add Rule and then click commit x change","title":"node classification in puppet enterprise console"},{"location":"puppet/puppet_study_notes/#default-node-group","text":"PE comes preconfigured with the default node group . This is the root of the node group hierarchy and is a parent to all other node groups. The classes that are assigned to the default node group are applied to all of the nodes in your deployment.","title":"default node group"},{"location":"puppet/puppet_study_notes/#deleting-nodes-from-puppet-enterprise-console","text":"There are three options for deleting a node:","title":"Deleting Nodes from puppet enterprise console"},{"location":"puppet/puppet_study_notes/#hide-node","text":"Removes the node from the node list view. To hide a node: On the Nodes page, click the node. Click Hide.","title":"Hide Node"},{"location":"puppet/puppet_study_notes/#delete-node","text":"Removes all reports and information for the node from the console. The node no longer appears in the list of nodes on the Nodes page, but it continues to appear in Matching nodes until it is purged from PuppetDB. The node will reappear as a new node on the Nodes page if it submits a new Puppet run report. To delete a node: On the Nodes page, click the node. Click Delete.","title":"Delete Node"},{"location":"puppet/puppet_study_notes/#deactivate-node","text":"Completely deactivates the node and frees up the license assigned to the node. Any deactivated node will be reactivated if PuppetDB receives new catalogues or facts for it. For details, see Deactivating a PE agent node. Stop the agent service on the node you want to deactivate. On the Puppet master, deactivate the agent; run puppet node deactivate {NODE NAME} . This deactivates the agent in PuppetDB. In some cases, the PE license count in the console will not decrease for up to 24 hours, but you can restart the pe-memcached service to update the license count sooner. On the Puppet master, revoke the agent certificate; run puppet cert revoke {AGENT CERTNAME} . Still on the Puppet master, run puppet agent -t to kick off a Puppet run. This Puppet run will copy the certificate revocation list (CRL) to the correct SSL directory for delivery to the agent. Restart the Puppet master with service pe-puppetserver restart . The certificate is only revoked after running service pe-puppetserver restart. In addition, the Puppet server process won\u2019t re-read the certificate revocation list until the service is restarted. If you don\u2019t run service pe-puppetserver restart, the node will check in again on the next Puppet run and re-register with PuppetDB, which will increment the license count again. Delete the node from the console. In the console, click Nodes. Click the node that you want to delete and click the Delete button. This action does NOT disable MCollective/live management on the node. To disable MCollective/live management on the node, uninstall the Puppet agent, stop the pe-mcollective service (on the agent, run service pe-mcollective stop), or destroy the agent altogether. On the agent, remove the node certificates in /etc/puppetlabs/mcollective/ssl/clients . clean the cert from the pauppet master ... puppet cert clean {AGENT CERTNAME} Note: If you delete a node from the node view without first deactivating the node, the node will be absent from the node list in the console, but the license count will not decrease, and on the next Puppet run, the node will be listed in the console. At this point, the node should be fully deactivated.","title":"Deactivate Node"},{"location":"puppet/puppet_study_notes/#regex-in-puppet","text":"Regexes in Puppet cannot have options or encodings appended after the final slash. However, you may turn options on or off for portions of the expression using the (? : ) and (?- : ) notation. The following example enables the i option while disabling the m and x options: $packages = $operatingsystem ? { /(?i-mx:ubuntu|debian)/ => 'apache2', /(?i-mx:centos|fedora|redhat)/ => 'httpd', } The following options are allowed: i \u2014 Ignore case m \u2014 Treat a newline as a character matched by . x \u2014 Ignore white space and comments in the pattern","title":"Regex in puppet"},{"location":"puppet/puppet_study_notes/#puppet-run-overview","text":"The SSL checks are made first The agent checks for a certificate matching its FQDN If one is not found the agent generates one. The agent checks for the CA cert. if not it will send its cert to the CA servers and request for its cert to be signed. If plugin sync is enabled, the agent checks the master for new plugins and downloads them if necessary. The agent ask's Facter for a set of facts about itself. { facter -p } The agent sends the facts the master whilst requesting a catalog. The master injects those facts as variables in the root scope and processes the manifests. The master then sends the catalog to the agent. The agent apply's the catalog. If reporting is enabled the agent sends a report back to the master. Diagram of puppet run +-----------------------+ +-----------------------+ | AGENT | | MASTER | | +-----------------+ | | | | | Check local Cert| | | +--------------+ | | | create if needed| | | +-->| Send CA Cert | | | +-----------------+ | | | +--------------+ | | +------------------+ | | | | | | Check 4 copy of | | | | | | | CA Cert,retrieve | | | | | | | if needed |<---------+ | | +------------------+ | | | | +-------------------+ | | +------------------+ | | | Is my Cert signed |<-------->| Sign Client Cert | | | +-------------------+ | | +------------------+ | | | | | | +-----------------+ | | +-----------------+ | | | Request plugins |---------> | Send Plugins | | | +-----------------+ | | +-----------------+ | | | | | | | +---------------- + | | | | | | Import plugins |<-------------------+ | | +-----------------+ | | | | | | | | +-----------------+ | | +--------------+ | | | | | | | CLASSIFY | | | | REQUEST CATALOG +------------> |The Node based| | | | Send node name | | | |on certname + | | | | and facts to | | | | node groups | | | | the master | | | +--------------+ | | | | | | ______|______ | | ------------------+ | | | | | | | | | +--V--++--V--++--V--+ | | | | |CLASS||CLASS||CLASS| | | +---------------+ | | +-----++-----++-----+ | | | | | | \\ | / | | | CATALOG | <-----------+ +-V---V----V-+ | | | | | | | | | | | +--------+------+ | | | | COMPILE | | | | | | | | | | | +--------V---------+ | | | +------------+ | | |APPLY +--------+ | | | | V | | | | QUERY | | | | | | | | | | STATUS | | | | +----<---+ | | | +----|---+ | | | | | | +------V----+ | | | | | | | ENFORCE | | | | | | | | DEFINED | | | | | | | | STATE | | | | | | | +-----------+ | | | +-------------+ | | +----+-------------+ | | | | | | | | | | REPORT | | | |--------------------> | | | | | | | +-------------+ | | +--------V---------+ | | | | | DEFINED SYSTEM | | | | | | STATE | | | | | +------------------+ | | | +-----------------------+ +-----------------------+","title":"Puppet run overview"},{"location":"puppet/puppet_study_notes/#resources-classes-manifest","text":"","title":"Resources ---}  Classes   ---}  Manifest"},{"location":"puppet/puppet_study_notes/#manifests","text":"A puppet manifest is text file that contains puppet code and is appended by the .pp file extension.","title":"Manifests"},{"location":"puppet/puppet_study_notes/#catalog","text":"In a standard agent + master puppet configuration, the agent never receives a copy of the modules, manifests, functions or variables. The agent only receives the compiled catalogue of resources and relationships. It is not recommended by puppetlabs, however you can send each node an entire copy of the manifests for the nodes the compile the catalog and apply it locally, This may be desirable in situations where there is no direct network connectivity between the agent and master. Warning: each node will be able to see the entire manifest which may contain sensitive information relating to other nodes in your environment.","title":"Catalog"},{"location":"puppet/puppet_study_notes/#modules","text":"Modules are just directories and files with a predictable structure located in the module path. The following commands will display the module path. # puppet agent --configprint modulepath # puppet config print modulepath /etc/puppet/modules:/usr/share/puppet/modules Modules will often have a main class that shares the name of the module. module-name |-manifests/ |-files/ |-templates/ |-lib/ |-tests/ |-spec/ If a class is defined in a module you can then declare that class in any manifest by name. Autoloading in Puppet means that your modules will be loaded by Puppet at compile time, as long as they follow a predictable structure. puppet _\\ compile _\\ load run / catalog / modules","title":"Modules"},{"location":"puppet/puppet_study_notes/#example-modules","text":"tree /etc/puppetlabs/puppet/environments/production/modules/sshd /etc/puppetlabs/puppet/environments/production/modules/sshd \u251c\u2500\u2500 files \u2502 \u2514\u2500\u2500 sshd_config \u251c\u2500\u2500 manifests \u2502 \u2514\u2500\u2500 init.pp \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp example module with class includes cowsayings \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 init.pp -----> class cowsayings { \u2502 \u2502 include cowsayings::cowsay \u2502 \u2502 include cowsayings::fortune \u2502 \u2502 } \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 cowsay.pp -----> class cowsayings::cowsay { \u2502 \u2502 package { 'cowsay': \u2502 \u2502 ensure => 'present', \u2502 \u2502 } \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 fortune.pp -----> class cowsayings::fortune { \u2502 package { 'fortune-mod': \u2502 ensure => 'present', \u2502 } \u2502 } \u2502 \u2514\u2500\u2500 tests \u2502 \u251c\u2500\u2500 init.pp --> include cowsayings \u2502 \u2502 \u251c\u2500\u2500 cowsay.pp --> include cowsayings::cowsay \u2502 \u2502 \u2514\u2500\u2500 fortune.pp --> include cowsayings::fortune","title":"Example module's"},{"location":"puppet/puppet_study_notes/#puppet-module-command-line","text":"#puppet module <action> ACTIONS: build - Build a module release package. changes - Show modified files of an installed module. generate - Generate boilerplate for a new module. install - Install a module from the Puppet Forge or a release archive. list - List installed modules search - Search the Puppet Forge for a module. uninstall - Uninstall a puppet module. upgrade - Upgrade a puppet module. When uploading a module to the forge your must include a ** modulefile ** containing the modules metadata (name, version, source, author, description, etc.)","title":"puppet module command line"},{"location":"puppet/puppet_study_notes/#resources","text":"Each resource describes some aspect of a system and its state e.g.... a service that must be running a package that must be installed a user that must be configured A resource declaration is the puppet block of code that describes a resource. Resource declarations are written the in the puppet DML Declarative Modelling Language. Puppet's DML is a declarative language rather than an imperative one. This means that instead of defining a process or set of commands, Puppet code describes (or declares) only the desired end state, and relies on built-in providers to deal with implementation. Resource declaration syntax example type { 'title': attribute_1 => value_1, attribute_2 => value_2, } type and title must be unique for a node values must be alphanumeric (quote strings) each attribute + value pair must be followed by a comma Individual resources are combined together to represent the desired system configuration. Similar resources can be grouped into types. such as the user type","title":"Resources"},{"location":"puppet/puppet_study_notes/#resource-abstraction-layer","text":"+---------------------------------------+ | file | package | service | user | Resources |---------------------------------------| | Ruby | Apt | Redhat | Useradd | Providers | | Yum | Launchd | LDAP | | | Gems | SMF | Netinfo | | | Deb | Debian | | | | RPM | | | +---------------------------------------+","title":"Resource Abstraction Layer"},{"location":"puppet/puppet_study_notes/#query-a-resource-using-the-puppet-command-line-tool","text":"puppet resource <type> {display all the instances of the specified type} puppet resource package puppet resource host puppet resource user puppet resource <type> <title> {display the details of a particular resources instance } puppet resource package apache puppet resource host example.com puppet resource user seamus puppet resource --type {list all types} puppet describe <type> {display a description of the type and its options}","title":"Query a resource using the puppet command line tool"},{"location":"puppet/puppet_study_notes/#puppet-apply","text":"You can use the Puppet resource declaration syntax with the puppet apply tool to make quick changes to resources on the system. You can either change the values directly from the command line using the syntax.. puppet apply -e \"user { 'seamus': ensure => 'present', }\" Or you can open the resource declaration in vim with the syntax... puppet apply -e user seamus The resource will be opened in vim. Make any desired changed and when you quit vim the new resource values will be applied.","title":"Puppet Apply"},{"location":"puppet/puppet_study_notes/#defined-resource-type-aka-defined-types-or-defines","text":"are types that can be evaluated multiple times using different parameters during declaration. Upon each new declaration with new parameters they act like a new resource type. Seamus sample module using defined resources [root@master modules]# tree definedmod definedmod \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 definedresource.pp \u2502 \u2514\u2500\u2500 init.pp \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp [root@master modules]# cat definedmod/manifests/init.pp class definedmod { definedresource {'mydefinedresource': var2 => 'twovar', var1 => 'variable1', var3 => 'thirdvariable', } } [root@master modules]# cat definedmod/manifests/definedresource.pp define definedresource ($var1, $var2 = default2 , $var3 = default3) { file { \"/tmp/${var1}\" : ensure => present, content => \"this file name is not $var2 or $var3\", } file { \"/tmp/${var2}\" : ensure => present, content => \"this file name is not $var1 or $var3\", } } [root@master modules]# cat definedmod/tests/init.pp include definedmod [root@node4 ~]# cat /tmp/variable1 this file name is not twovar or thirdvariable [root@node4 ~]# cat /tmp/twovar this file name is not variable1 or thirdvariable [root@node4 ~]# cat /tmp/thirdvariable this file name is not variable1 or twovar[root@node4 ~]#","title":"Defined Resource Type (aka defined types or defines)"},{"location":"puppet/puppet_study_notes/#resource-duplicates","text":"Instances of type + title must be singleton/unique for each node, otherwise the nodes catalog will fail to compile. sample duplicate resource declarations: class resourceduplicates { user { 'user1': # duplicate title of the user resource below; # the nodes catalog will fail to compile. ensure => 'present', uid => '1007', } user { 'user1': ensure => 'present', uid => '1007', # duplicate UID's, puppet catalog will compile; } # But the useradd provider will fail on the client. } sample error of duplicate resource: Error: Could not retrieve catalogue from remote server: Error 400 on SERVER: Duplicate declaration: User[user2] is already declared in file","title":"Resource duplicates"},{"location":"puppet/puppet_study_notes/#resource-refresh","text":"Puppet will refresh a service for each instance of a refresh, e.g if 3 files are being updated and each update triggers a refresh of the same service 3 separate refreshes will be triggered doe each of the file updates. # This is a test to see if puppet will restart a service multiple time or- # just once when multiple refresh events are triggered class loop { file { '/tmp/loop1' : ensure => present, content => \"$::uptime_seconds \\n\", # the \"uptime_seconds\" is used here to ensure that } # the content of the file will change upon each run file { '/tmp/loop2' : ensure => present, content => \"$::uptime_seconds \\n\", } file { '/tmp/loop3' : ensure => present, content => \"$::uptime_seconds \\n\", } service { 'crond' : ensure => running, } File['/tmp/loop1'] ~> Service['crond'] File['/tmp/loop2'] ~> Service['crond'] File['/tmp/loop3'] ~> Service['crond'] }","title":"Resource refresh"},{"location":"puppet/puppet_study_notes/#resource-ordering-and-relationships-with-metaparameters","text":"Via a mechanism called autorequires ; For some of the built in resource types such as users and groups, Puppet is able to automatically determine the dependency relationships among some of the built-in resources such as user and groups without a user/admin having to declare the resource interdependencies. For the majority of resources there is no autorequire and therefore the order of resources in a puppet manifest is generally ignored. If a group of resources have interdependencies you should use one of the metaparameters to specify the relationships between them before - Causes a resource to be applied before the target resource require - Causes a resource to be applied after the target resource notify - Causes a resource to be applied before the target resource. The target resource will refresh if the notifying resource changes. subscribe - Causes a resource to be applied after the target resource. The subscribing resource will refresh if the target resource changes. Note: when creating a resource reference use the syntax Type['title'] Note: Use lower-case for the type name when declaring it but use a capitalised type name when calling it. metaparameter declaration examples...","title":"Resource ordering and relationships with metaparameters"},{"location":"puppet/puppet_study_notes/#before","text":"package { 'openssh-server': ensure => present, before => File['/etc/ssh/sshd_config'], }","title":"before"},{"location":"puppet/puppet_study_notes/#require","text":"file { '/etc/ssh/sshd_config': ensure => file, mode => 600, source => 'puppet:///modules/sshd/sshd_config', require => Package['openssh-server'], }","title":"require"},{"location":"puppet/puppet_study_notes/#ordering-arrows","text":"Package['openssh-server'] -> File['/etc/ssh/sshd_config'] Causes the resource on the left to be applied before the resource on the right.","title":"Ordering Arrows"},{"location":"puppet/puppet_study_notes/#notify","text":"file { '/etc/ssh/sshd_config': ensure => file, mode => 600, source => 'puppet:///modules/sshd/sshd_config', notify => Service['sshd'], }","title":"notify"},{"location":"puppet/puppet_study_notes/#subscribe","text":"service { 'sshd': ensure => running, enable => true, subscribe => File['/etc/ssh/sshd_config'], }","title":"subscribe"},{"location":"puppet/puppet_study_notes/#notification-arrow","text":"File['/etc/ntp.conf'] ~> Service['ntpd'] Causes the resource on the left to be applied first and sends a refresh to the resource on the right if the resource on the left changes. Complete example class sshd { package { 'openssh-server': ensure => present, before => File['/etc/ssh/sshd_config'], } file { '/etc/ssh/sshd_config': ensure => file, mode => '0600', source => 'puppet:///modules/sshd/sshd_config', } } service { 'ssh': ensure => running, enable => true, subscribe => File['/etc/ssh/sshd_config'], }","title":"Notification Arrow"},{"location":"puppet/puppet_study_notes/#extra-metaparameters","text":"Manage dependencies (before, require, subscribe, notify, stage) Manage resources' application policies (audit, noop, schedule, loglevel) Add information to a resource (alias, tag)","title":"extra metaparameters.."},{"location":"puppet/puppet_study_notes/#virtual-resources","text":"are a hack to get around puppets limitation of sigleton resource definitions. They enable an a particular instance of a resource type+title to be used in multiple classes. They are defined as virtual by pre-pending them with the \"@\" symbol @user { seamus: ensure => present } They can then be declared in multiple classes by using one of the following 3 syntaxes using either the realise function or the spaceship collector. User <| title == seamus |> realize User[seamus] realize(User[seamus])","title":"Virtual resources"},{"location":"puppet/puppet_study_notes/#exported-resources","text":"Enable a resource that is defined on a host to be exported to the puppetdb and then be used by 1 or more other hosts. Puppet collects and stores the exported resources during configuration runs. /etc/hosts example [root@master modules]# cat exportedresource/manifests/init.pp class exportedresource { @@host { $hostname: ## sends the local nodes host resource to the puppetdb and ## tags it as exported ip => $::ipaddress, name => $::fqdn, host_aliases => $::hostname, comment => 'addded by the exported resource class', } Host <<| |>> ## retrieves all the host resources from the puppetdb } SSH example class ssh::hostkeys { @@sshkey { \"${::fqdn}_dsa\": host_aliases => [ $::fqdn, $::hostname, $::ipaddress ], type => dsa, key => $::sshdsakey, } @@sshkey { \"${::fqdn}_rsa\": host_aliases => [ $::fqdn, $::hostname, $::ipaddress ], type => rsa, key => $::sshrsakey, } } SSH example of retrieving the exported resources and applying them class ssh::knownhosts { Sshkey <<| |>> { ensure => present } } The ssh::knownhosts class should be included in the catalog for all nodes where Puppet should manage the SSH known_hosts file. Notice that we\u2019ve used double angle braces to collect resources from PuppetDB. define balancermember($url) { file { '/etc/httpd/conf.d.members/worker_${name}.conf': ensure => file, owner => 'root', group => 'root', mode => '0644', content => \"BalancerMember $url \\n\", } } This configuration file fragment contains a single line, the URL to a member of the load balancer pool. Puppet recommends using a defined resource type because all resources declared within the type will be exported when the defined type itself is exported. class loadbalancer_members { Balancermember <<| |>> { notify => Service['apache'] } } This code uses the double angle brace syntax to collect all balancermember resources from the stored configuration database. In addition, it uses a parameter block to notify the Apache service of any changes Puppet makes to the balancermember resources. Just as with virtual resources, a parameter block may be specified to add further parameters to collected resources. Removing retired nodes from PuppetDB # puppet node deactivate mail.example.com Submitted 'deactivate node' for <node_name> with UUID aaaaa-bbbbb-ccccc-ddddd-eeee After you\u2019ve run this on the puppet master, any resources exported by this node will no longer be collected on your Puppet clients. Note: Deactivated node resources will not be removed from the systems that collected them. You will need to clean up those configurations manually; or some resources can be purged using the resource metatype.","title":"Exported resources"},{"location":"puppet/puppet_study_notes/#exec-resource","text":"Exec resources execute external commands, it is important that any commands executed are Idempotent Any command in an exec resource must be able to be run multiple times without casing harm. There are 3 main ways for commands in exec resources to be idempotent 1. The commands them selves can be idempotent 2. You use an (onlyif, unless, or creates) attribute, which will prevent puppet from running a specific command unless a specific condition is met. 3. The exec resource has the ( refreshonly=>true ) value, which only allows puppet to run the command when some other resource is changed syntax exec { 'resource-title': attribute_1 => value_1, attribute_2 => value_2, } example exec { \"updatedb': path => '/usr/sbin', command => 'updatedb', } Note: if you do not set the command value it will default to the exec resource title. You must specifiy the path because the exec resource does not inherit paths.","title":"Exec Resource"},{"location":"puppet/puppet_study_notes/#classes","text":"Classes define a collection of resources that are managed together as a single unit. You can also think of them as named blocks of Puppet code, which are created in one place and invoked elsewhere. Using a Puppet class requires two steps. First, you'l need to define it by writing a class definition and saving it in a manifest file. When Puppet runs, it will parse this manifest and store your class definition. Secondly, The class can then be declared to apply it to nodes in your infrastructure. example class for ssh that contains 3 resources { package, file, service}.. resource class package \\ file >- ssh service / Example class syntax....{with no relationships defined} class ssh { package { 'openssh-clients': ensure => present, } file { '/etc/ssh/ssh_config': ensure => file, owner => 'root', group => 'root', source => 'puppet:///modules/ssh/ssh_config', } service { 'sshd': ensure => running, enable => true, } } Defining a class specifies the contents and behaviour but does not automatically include (apply) it in a configuration Declaring a class, includes the class in the catalogue which will then be applied upon next agent run. To declare a class use either of the following syntaxes in site.pp.. or in /tests/init.pp ** include my_calss ** or ** class { 'my_class': } ** classes are reusable classes are singleton classes can only be used once per node When applying a class make sure you run puppet apply module/tests/init.pp and NOT puppet apply module/manifest/init.pp otherwise no actions will take place and no errors will be logged, this is because you would be defining the class but not declared it anywhere (on a node).","title":"Classes"},{"location":"puppet/puppet_study_notes/#parameterised-classes","text":"Class parameters provide a method to set variables in a class as it's declared. The syntax for parameterised classes is similar to resource declarations. class { 'ntp': servers => ['node1.example.com','node2.example.com','node3.example.com'] } The servers parameter can be populated with a single server or an array of servers. Parameterised class definitions can be set in the site.pp file. node default { # This is where you can declare classes for all nodes. # Example: # class { 'my_class': } class { 'ntp': servers => ['node1.example.com','node2.example.com','node3.example.com'] } } You can also override values with class { '::mysql::server': override_options => { 'mysqld' => { 'max_connections' => '1024' } }, }","title":"Parameterised Classes"},{"location":"puppet/puppet_study_notes/#class-inheritance-derived-classes","text":"By using the inherits keyword classes can be derived from other classes. When a derived class is declared, its base class is automatically declared first and the variable are set as the parent scope. The new class receives a copy of all the base class's variables and resource defaults. Code in the derived class is able to override any resource attributes that were originally set in the base class. Class Inheritance is only useful for overriding resource attributes. For any other use case it is better archive your objective by using some other method. example... We create a new class zsh::developer that will enable us to deploy the zshrc.dev file instead of the zshrc file on certain nodes.. [root@learn modules]# tree zsh/ zsh/ \u251c\u2500\u2500 files \u2502 \u251c\u2500\u2500 zshrc \u2502 \u2514\u2500\u2500 zshrc.dev \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 developer.pp \u2502 \u2514\u2500\u2500 init.pp \u251c\u2500\u2500 Modulefile \u251c\u2500\u2500 README \u251c\u2500\u2500 spec \u2502 \u2514\u2500\u2500 spec_helper.rb \u2514\u2500\u2500 tests \u251c\u2500\u2500 developer.pp \u2514\u2500\u2500 init.pp cat zsh/manifests/init.pp class zsh { package { 'zsh': ensure => present, before => File['/etc/zshrc'], } file { '/etc/zshrc': ensure => file, owner => 'root', group => 'root', source => 'puppet:///modules/zsh/zshrc', } } [root@learn modules]# cat zsh/manifests/developer.pp #Note there is no need for a \"package { 'zsh':\" resource definition here as it will be inherited from the zsh class class zsh::developer inherits zsh { File ['/etc/zshrc'] { source => 'puppet:///modules/zsh/zshrc.dev', } } [root@learn modules]# cat zsh/tests/init.pp include zsh [root@learn modules]# cat zsh/tests/developer.pp include zsh::developer","title":"Class Inheritance / Derived Classes"},{"location":"puppet/puppet_study_notes/#variables-and-class-parameters","text":"Variables are prefixed with the $ sign and assigned with the = operator $mystring = 'this is my string' $mypath = '/tmp/puppet/' Once a variable it is defined it can be used anywhere in the manifest in place of an regular assigned value. Unlike resource declarations, variable assignments are parse-order dependent. This means that you must assign a variable in your manifest before you can use it. If you try to use a variable that has not been defined, the Puppet parser won't complain. Instead, Puppet will treat the variable as having the special undef value. You can only assign a variable once within a single scope. Once it's assigned, the value cannot be changed. (therefore it should be called a constant but hey its not like the puppet people actually follow best practices or common conventions anywhere else so why would this be any different) Variable interpolation enables a string that is stored as a variable to be inserted into another string. eg.. file { \"${mypath}file1.txt\": ... } file { \"${mypath}file2.txt\": ... } Note: A string that includes an interpolated variable must be wrapped in double quotation marks (\"...\"), rather than the single quotation marks that surround an ordinary string Class Parameters provide a method of setting the variables within a class when the class is declared rather than when it is defined. Once defined a Parameterised Class can be declared with a similar syntax to a resource declaration When defining a class, include a list of parameters and optional default values between the class name and the opening curly brace. So a parameterised class is defined as below: class classname ( $parameter = 'default' ) { ... } Once defined, a parameterised class can be declared with a syntax similar to that of resource declarations, including key value pairs for each parameter you want to set. class {'classname': parameter => 'value', } Seamus sample module using defined resource type class with paramaters (as described in the \"defined resource section above\" [root@master modules]# tree definedmod definedmod \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 definedresource.pp \u2502 \u2514\u2500\u2500 init.pp \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp [root@master modules]# cat definedmod/manifests/init.pp class definedmod { definedresource {'mydefinedresource': var2 => 'twovar', var1 => 'variable1', var3 => 'thirdvariable', } } [root@master modules]# cat definedmod/manifests/definedresource.pp define definedresource ($var1, $var2 = default2 , $var3 = default3) { file { \"/tmp/${var1}\" : ensure => present, content => \"this file name is not $var2 or $var3\", } file { \"/tmp/${var2}\" : ensure => present, content => \"this file name is not $var1 or $var3\", } } [root@master modules]# cat definedmod/tests/init.pp include definedmod [root@node4 ~]# cat /tmp/variable1 this file name is not twovar or thirdvariable [root@node4 ~]# cat /tmp/twovar this file name is not variable1 or thirdvariable [root@node4 ~]# cat /tmp/thirdvariable this file name is not variable1 or twovar[root@node4 ~]#","title":"Variables and Class  Parameters"},{"location":"puppet/puppet_study_notes/#puppet-lint","text":"A 3rd party tool used to display common syntax and style errors. Packaged as a ruby gem. gem install puppet-lint puppet-lint /etc/puppetlabs/puppet/modules/ntplint/manifests/init.pp you can ignore certain syntax checks on command line puppet-lint <path>/<file>.pp --no-80chars-check puppet-lint <path>/<file>.pp --no-ensure_first_param-check ~/.puppet-lint.rc --no-80chars-check --no-ensure_first_param-check using a rake file to test multiple manifests in one go by including the following values in your rake file require 'puppet-lint/tasks/puppet-lint' PuppetLint.configuration.send(\"disable_<check>\")","title":"puppet-lint"},{"location":"puppet/puppet_study_notes/#idempotency","text":"By default puppet modules describe the desired final state rather than detail a series of step to follow, this mean that no matter how many times they are run the same end state will occur.","title":"Idempotency"},{"location":"puppet/puppet_study_notes/#puppet-console","text":"The puppet enterprise console is a web-based gui. It can help.. manage node requests to join the puppet deployment assign puppet classes to nodes and groups view reports and activity graphs browse and compare resources on your nodes view inventory data manage console users and their access privileges","title":"Puppet Console"},{"location":"puppet/puppet_study_notes/#event-inspector","text":"The Event Inspector is part of the Puppet Enterprise (PE) Console. It is a reporting tool that provides data for investigating the current state of your infrastructure. Its focus is on correlating information and presenting it from multiple perspectives, in order to reveal common causes behind related events. Event inspector lets you accomplish two important tasks: monitoring a summary of your infrastructure\u2019s activity and analyzing the details of important changes and failures. It displays events from three perspectives Classes Nodes and Resources","title":"Event Inspector"},{"location":"puppet/puppet_study_notes/#resetting-the-admin-user-password","text":"This must be performed from a shell session on the console server and executing a ruby script that was part of the Puppet Enterprise installer. q_puppet_enterpriseconsole_auth_password=newpassword q_puppetagent_certname=$(puppet config print certname) /opt/puppet/bin/ruby update-superuser-password.rb","title":"Resetting the admin user password"},{"location":"puppet/puppet_study_notes/#environments","text":"By default, all nodes are assigned to a default environment named production. There are three ways to assign nodes to a different environment: Via your ENC or node terminus Via each agent node\u2019s puppet.conf Via the PE console to set the environment for each node group. Assigning Environments Via an ENC The interface to set the environment for a node will be different for each ENC. Some ENCs cannot manage environments. When writing an ENC, simply ensure that the environment: key is set in the YAML output that the ENC returns. See the documentation on writing ENCs for details. If the environment key isn\u2019t set in the ENC\u2019s YAML output, the Puppet master will just use the environment requested by the agent. Assigning Environments Via the Agent\u2019s Config File Assigning Environment via agent config In puppet.conf on each agent node, you can set the environment setting in either the agent or main config section. When that node requests a catalog from the Puppet master, it will request that environment. Note: If you are using an ENC and it specifies an environment for that node, it will override whatever is in the config file. Assigning Environments via PE console Click Classification - select node group - click \"edit node group metadata\" - select enviroment","title":"Environments"},{"location":"puppet/puppet_study_notes/#scope","text":"Scope helps to organise classes, telling Puppet where to look within the module directory structure to find each class. It also separates namespaces within the module and your Puppet manifests, preventing conflicts between variables or classes with the same name. The init.pp in the /manifest/ directory must contain a class with the same name as the module name Class name = mysql File location = /modules/mysql/manifests/init.pp ) Class name = mysql::server File location = /modules/mysql/manifests/server.pp Class name = mysql::server::account_security File location = /modules/mysql/manifests/server/account_security.pp","title":"Scope"},{"location":"puppet/puppet_study_notes/#types-and-providers","text":"A type defines the interface for a resource: the set of properties you can use to define a desired state for the resource, and the parameters that don't directly map to things on the system, but tell Puppet how to manage the resource. Both properties and parameters appear in the resource declaration syntax as attribute value pairs. command to show the types available [root@master ~]# puppet resource --type +-----------------------------------------------------------------------------+ | anchor | mcx | puppetdb_conn_validator | | apt_key | mount | resources | | augeas | nagios_command | router | | computer | nagios_contact | schedule | | cron | nagios_contactgroup | scheduled_task | | exec | nagios_host | selboolean | | file | nagios_hostdependency | selmodule | | file_line | nagios_hostescalation | service | | filebucket | nagios_hostextinfo | ssh_authorized_key | | firewall | nagios_hostgroup | sshkey | | firewallchain | nagios_ser^ice | stage | | group | nagios_ser|icedependency | tidy | | host | nagios_ser|iceescalation | user | | ini_setting | nagios_ser|iceextinfo | vlan | | ini_subsetting | nagios_servicegroup | whit | | interface | nagios_timeperiod | yumrepo | | k5login | notify | zfs | | macauthorization | package | zone | | mailalias | postgresql_conf | zpool | | maillist | postgresql_psql | | +-----------------------------------------------------------------------------+","title":"Types and Providers"},{"location":"puppet/puppet_study_notes/#core-types","text":"","title":"Core Types"},{"location":"puppet/puppet_study_notes/#file","text":"manages local files and ensures if a file should exist sets its parameters present / absent file / directory / link source puppet:///modules/","title":"File"},{"location":"puppet/puppet_study_notes/#package","text":"Manages software packages ensures that a package is present / absent / latest /purged / version","title":"Package"},{"location":"puppet/puppet_study_notes/#service","text":"Manages services ensures running / stopped enable true / false","title":"Service"},{"location":"puppet/puppet_study_notes/#notify_1","text":"echo's message to the agent run-time log message => \" hello world ! \",","title":"notify"},{"location":"puppet/puppet_study_notes/#exec","text":"executes an arbitrary command on the agent node.","title":"exec"},{"location":"puppet/puppet_study_notes/#cron","text":"manages cron jobs","title":"cron"},{"location":"puppet/puppet_study_notes/#user","text":"manages user accounts ensure present / absent / role name, uid, gid, groups, home, shell","title":"user"},{"location":"puppet/puppet_study_notes/#group","text":"ensure present / absent name, gid A provider is what does the heavy lifting to bring the system into line with the state defined by a resource declaration. Providers are implemented for a wide variety of supported operating systems. They are a key component of the Resource Abstraction Layer (RAL), translating the universal interface defined by the type into system-specific implementations.","title":"group"},{"location":"puppet/puppet_study_notes/#resource-abstraction-layer_1","text":"+---------------------------------------+ | file | package | service | user | Resources |---------------------------------------| | Ruby | Apt | Redhat | Useradd | Providers | | Yum | Launchd | LDAP | | | Gems | SMF | Netinfo | | | Deb | Debian | | | | RPM | | | +---------------------------------------+","title":"Resource Abstraction Layer"},{"location":"puppet/puppet_study_notes/#conditional-statements","text":"**Note: ** string matches are not case sensitive","title":"Conditional Statements"},{"location":"puppet/puppet_study_notes/#if-optional-elseif-and-else","text":"class accounts ($name) { if $::operatingsystem == 'centos' { $groups = 'wheel' } elsif $::operatingsystem == 'debian' { $groups = 'admin' } else { fail( \"This module doesn't support ${::operatingsystem}.\" ) } notice ( \"Groups for user ${name} set to ${groups}\" ) ... }","title":"if   (optional elseif and else)"},{"location":"puppet/puppet_study_notes/#unless","text":"unless $memorysize > 1024 { $maxclient = 500 }","title":"unless"},{"location":"puppet/puppet_study_notes/#case","text":"case $::operatingsystem { 'CentOS': { $apache_pkg = 'httpd' } 'Redhat': { $apache_pkg = 'httpd' } 'Debian': { $apache_pkg = 'apache2' } 'Ubuntu': { $apache_pkg = 'apache2' } default: { fail(\"Unrecognised operating system for webserver.\") } } package { $apache_pkg : ensure => present, }","title":"case"},{"location":"puppet/puppet_study_notes/#selector","text":"Selector statements are similar to case statements, but return a value instead of executing a code block. Selectors can only be used at places in the code where a plain value is expected an not inside another selector or case statement. $rootgroup = $::osfamily ? { 'Solaris' => 'wheel', 'Darwin' => 'wheel', 'FreeBSD' => 'wheel', 'default' => 'root', }","title":"selector"},{"location":"puppet/puppet_study_notes/#my-sample-class-with-all-the-selectors","text":"[root@pe-puppet modules]# cat noticemod/manifests/init.pp class noticemod { notify { \"Hello World!\": } ## SELECTOR statement notify { \" SELECTOR warning !\": message => $seamus ? { 'mytrue' => \" the value is true.\", 'myfalse' => \" the value is false.\", default => \" the value is not set.\", }, # selector statements warn you if no match is found and there is no default } ## CASE statement case $seamus { 'mytrue': { notify { \" CASE warning ! the value is true.\": } } 'myfalse': { notify { \" CASE warning ! the value is false.\": } } default: { fail(\" CASE warning ! failing now... if the variable is undef the whole catalog compile will fail\") } } # case statements silently fall through the bottom if no match is found ## IF statement if 'rue' in $seamus { notify { \" IF warning ! the value is true.\": } } elsif 'alse' in $seamus { notify { \" IF warning ! the value is false.\": } } else { notify { \" IF warning ! the value does not match.\": } } ## UNLESS statement unless $seamus == 'myfalse' { notify { \" UNLESS statement - \\$seamus does not == myfalse, it is set to $seamus \": } } unless $seamus == 'mytrue' { notify { \" UNLESS statement - \\$seamus does not == mytrue, it is set to $seamus \": } } }","title":"my sample class with all the selectors"},{"location":"puppet/puppet_study_notes/#puppetdb","text":"stores the most recent facts from every node the most recent catalog from every node (optionally) 7 days of event reports from every node it is searchable using either... PuppetDB's query API Puppet's inventory service exported resources example [root@learning ~]# puppet node status learning.puppetlabs.vm learning.puppetlabs.vm Currently active Last catalog: 2015-01-24T13:31:16.291Z Last facts: 2015-01-24T13:31:03.782Z Declaring an exported resource causes that resource to be added to the catalog and marked with an \u201cexported\u201d flag, which prevents the puppet agent from managing the resource. When PuppetDB receives the catalog, it stores a record for each resource with the flag set. Delete reports older then 1 month : sudo -u puppet-dashboard rake RAILS_ENV=production reports:prune upto=1 unit=mon","title":"PuppetDB"},{"location":"puppet/puppet_study_notes/#hiera","text":"https://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/ Hiera is a key/value lookup tool for configuration data, built to make puppet better and let you set node-specific data without repeating yourself. Hiera support is built into puppet 3, and is available as an add-on for puppet 2.7. It keeps site specific data out of the manifest. Puppet classes can query any data they need and hiera will act as a site wide config file. Hiera makes it easier to: 1. separate configuration data from the modules code. 2. configure your own nodes: default data with multiple levels of overrides. 3. re-use public puppet modules: you don't have to edit the code you just have to put the necessary data in hiera. 4. makes it easier to publish your own developed modules to the forge without clashing variable names or having to redact your config values. 5. create common data for most nodes and.. - Override some values for machines located at a particular facility - Override some values for specific machines 6. enables you to only write down the differences that are needed (doesn't make sense) To get started with hiera there are 5 steps.. 1) download and install hiera 2) create a hiera.yaml config file 3) arrange a hierachy that suite your site and data 4) write your data sources 5) configure puppet to use hiera https://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/ hiera - Performs a standard priority lookup and returns the most specific value for a given key. The returned value can be data of any type (strings, arrays, or hashes). hiera_array - Returns all matches throughout the hierarchy \u2014 not just the first match \u2014 as a flattened array of unique values. If any of the matched values are arrays, they\u2019re flattened and included in the results. hiera_hash - Returns a merged hash of matches from throughout the hierarchy. In cases where two or more hashes share keys, the hierarchy order determines which key/value pair will be used in the returned hash, with the pair in the highest priority data source winning. example of using hiera.... [root@learn ~]# vim /etc/puppetlabs/puppet/hieradata/common.yaml --- message: This string is the value that is returned when hiera('message') is called. motd: Hello there Seamus test that the key is set correctly using the puppet apply -e [root@learn hieradata]# puppet apply -e 'notice(hiera(\"motd\"))' Notice: Scope(Class[main]): Hello there Seamus Notice: Compiled catalog for learn.puppetlabs.com in environment production in 0.04 seconds Notice: Finished catalog run in 0.02 seconds Edit the motd module to use the key from hiera instead of the value in the module files [root@learn hieradata]# cd /etc/puppetlabs/puppet/modules/ [root@learn modules]# tree motd/ motd/ \u251c\u2500\u2500 manifests \u2502 \u2514\u2500\u2500 init.pp \u251c\u2500\u2500 Modulefile \u251c\u2500\u2500 README \u251c\u2500\u2500 spec \u2502 \u2514\u2500\u2500 spec_helper.rb \u2514\u2500\u2500 tests \u2514\u2500\u2500 init.pp [root@learn modules]# vim motd/manifests/init.pp Class: motd class motd { file { '/etc/motd': ensure => file, owner => 'root', group => 'root', content => (hiera(\"motd\")), } } [root@learn modules]# puppet parser validate motd/manifests/init.pp [root@learn modules]# puppet apply motd/tests/init.pp Notice: Compiled catalog for learn.puppetlabs.com in environment production in 0.09 seconds Notice: /Stage[main]/Motd/File[/etc/motd]/ensure: defined content as '{md5}13c0016553b5315a8cd66f13b23f14a3' Notice: Finished catalog run in 0.03 seconds [root@learn modules]# cat /etc/motd Hello there Seamus","title":"Hiera"},{"location":"puppet/puppet_study_notes/#facter","text":"Facter stores facts as pairs of keys and values.","title":"Facter"},{"location":"puppet/puppet_study_notes/#facter-command-line-options","text":"-y, --yaml Emit facts in YAML format. -j, --json Emit facts in JSON format. --plaintext Emit facts in plaintext format. --trace Enable backtraces. --external-dir DIR The directory to use for external facts. --no-external-dir Turn off external facts. -d, --debug Enable debugging. -t, --timing Enable timing. -p, --puppet Load the Puppet libraries, thus allowing Facter to load Puppet-specific facts. -v, --version Print the version and exit. -h, --help Print this help message. A shell enviroment variable prepened with the name FACTER_ will be readable by facter as a fact. eg... [root@node1 ~]# export FACTER_seamus=murray [root@node1 ~]# facter seamus murray [root@node1 ~]# facter |grep seamus seamus => murray Facter values can be temporarily overridden by setting a shell variable prepended with FACTER_ and the name of the fact. eg... FACTER_operatingsystem=Debian You can even test the effect that the fake fact will have using puppet apply.. FACTER_operatingsystem=Debian puppet apply --noop accounts/tests/init.p The best way to distribute facts is to include them in your modules using the puppet plug-ins. Puppet will distribute the custom facts, custom types, providers and functions to any host that includes the module. you can either distribute the facts with the modules that need them or you can create a module combining all your custom facts needed by all your other modules. Facter is called by the puppet agent. Facts apear as normal and top scope variables they can be called by $ipaddress or ${::ipaddress} (best practice)","title":"Facter command line options"},{"location":"puppet/puppet_study_notes/#pluginsync","text":"Custom Facts and types can be exported to the the agent nodes from the puppet master using the plugin sync mechanism. Before the client requests a catlog it checks to see if the are any new plugins or if its plugins have been updated. If there has been a change it pulls down the plugins and executes the code in the custom types and facts. This and be disabled from the agent nodes puppet.conf config file [main] pluginsync = false","title":"Pluginsync"},{"location":"puppet/puppet_study_notes/#custom-facts","text":"you can create a custom fact with a bit of ruby code on the puppet master and use plugins and modules to distribute it to the clients/nodes Create a ruby file in <modulename>/lib/facter/<modulename>.rb For simple shell commands, just insert the shell command in between the setcode do and the end . For more complex comands use Facter::Core::Execution.exec(' ') between the setcode do and the end . Facter.add('mycustomfact') do setcode do Facter::Core::Execution.exec('/bin/date') end end on the agents use plugin sync to automatically pull down the facts to /var/lib/puppet/lib/facter/mycustomfact.rb puppet agent -t use the -p facter flag to test the puppet fact facter -p mycustomfact A fact can be confined to run only if another fact is of a certain value e.g. Facter.add(:powerstates) do confine :kernel => 'Linux' setcode do Facter::Core::Execution.exec('cat /sys/power/states') end end","title":"Custom facts"},{"location":"puppet/puppet_study_notes/#external-facts","text":"Can be written in any language as long as they return key/value pairs to stdout. They are stored in .. <MODULEPATH>/<MODULE>/facts.d/ and are distributed to the clients via the pluginsync mechanism. Example fact written in bash... $ cat/etc/puppet/modules/myexternalfact/facts.d/myexternalfacts.sh #!/bin/bash echo \"myexternalfact1=`/bin/df -P / |/usr/bin/tail -1 | /bin/awk '{print $5}'`\" echo \"extfact1=one\" echo \"extfact2=two\" echo \"extfact3=three\" Calling the external fact from the command line on a node that has recieved the external fact by pluginsync... $ facter --external-dir /var/lib/puppet/facts.d/ extfact1 extfact2 extfact3 myexternalfact1 myexternalfact1 => 33% extfact1 => one extfact2 => two extfact3 => three Example fact written in c... #include <stdio.h> int main() { printf(\"my_external_fact_in_c=\\\"Hello World\\\"\\n\"); return 0; } Calling the external fact written in c... # facter --external-dir ~/puppet-study/external-facts/ my_external_fact_in_c \"Hello World\"","title":"External facts"},{"location":"puppet/puppet_study_notes/#distributing-the-external-facts-to-the-puppet-client-nodes","text":"It is best to put the external facts into a module so that they can be distributed to all the agent nodes via plugin sync.. [root@master myexternalfact]# tree /etc/puppet/modules/myexternalfact /etc/puppet/modules/myexternalfact \u2514\u2500\u2500 facts.d \u251c\u2500\u2500 myexternalfactinc \u2514\u2500\u2500 myexternalfact.sh [root@master ~]# ls -l /etc/puppet/modules/myexternalfact/facts.d/ total 16 -rwxr-xr-x. 1 root root 8523 Feb 10 00:27 myexternalfactinc -rwxr-xr-x. 1 root root 235 Feb 10 00:36 myexternalfact.sh Trigger the plugin sync to trasfer the new external facts.. [root@node1 ~]# puppet agent -t Use the puppet apply -e tool on the agent nodes to test the facter values.. [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_c}\" ) ' Notice: Scope(Class[main]): \"Hello World\" Notice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds Notice: Finished catalog run in 0.03 seconds [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_1}\" ) ' Notice: Scope(Class[main]): 32% Notice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds Notice: Finished catalog run in 0.03 seconds [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_2}\" ) ' Notice: Scope(Class[main]): two Notice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds Notice: Finished catalog run in 0.03 seconds","title":"Distributing the external facts to the puppet client nodes"},{"location":"puppet/puppet_study_notes/#structured-facts","text":"Can come straight from YAML,JSON or txt files in the directories.. /etc/facter/facts.d/ or /etc/puppetlabs/facter/facts.d/ $ cat /etc/facter/facts.d/mystructuredfact.yaml --- mystructuredfact1: factone mystructuredfact2: facttwo mystructuredfact3: factthree $ facter mystructuredfact1 mystructuredfact2 mystructuredfact3 mystructuredfact1 => factone mystructuredfact2 => facttwo mystructuredfact3 => factthree","title":"Structured facts"},{"location":"puppet/puppet_study_notes/#roles-and-profiles","text":"Roles and profiles are just modules written in a specific way. roles are assigned to nodes profiles are assigned to roles","title":"Roles and Profiles"},{"location":"puppet/puppet_study_notes/#example-role","text":"class role::weibserver inherits role { include profile:: web } class role::dbserver inherits role { include profile::db } class role::webdbserver inherits role { include profile:web include profile::db } classification = assignment of classes to nodes classification = assignment of roles to nodes","title":"example role"},{"location":"puppet/puppet_study_notes/#example-profile","text":"class profile::web { include apache include php include tomcat include jdk include memcache } __ Tip: classes are singletons, they can be included multiple times on a node, but they will only be evaluated once . A defined class can be declared multiple times, because it takes parameters and each new declaration will be evaluated. You can see we\u2019ve included our two classes but not the definition, apache::vhost. This is because of some module magic called autoloading. Puppet scans your module and loads any .pp file in the manifests directory that is named after the class it contains; for example, the install.pp file contains the apache::install class and so is autoloaded. The same thing happens with definitions: The vhost.pp file contains the definition apache::vhost, and Puppet autoloads it. However, as we declare definitions, for example calling apache::vhost where we need it, we don\u2019t need to do an include apache::vhost because calling it implies inclusion. ............................................... class puppet::params { $puppetserver = hiera('puppetserver') } In Puppet 3, but not Puppet 2.7, there is an automatic lookup of parameters in a parameterized class. We can use this to rewrite the puppet::params class further: class puppet::params ( $puppetserver, ){ } When this is called with no arguments, Puppet 3 will attempt to look up the puppet::params::puppetserver key in Hiera and populate it accordingly. It will not fail unless the Hiera lookup fails.","title":"example profile"},{"location":"puppet/puppet_study_notes/#puppet-templates","text":"Puppet supports templates written in the ERB templating language, which is part of the Ruby standard library. Templates are always evaluated by the parser, not by the client. This means that if you are using a puppet master server, then the templates only need to be on the server, and you never need to download them to the client. The client sees no difference between using a template and specifying all of the text of the file as a string. Note that the template function simply returns a string, which can be used as a value anywhere \u2014 the most common use is to fill file contents, but templates can also provide values for variables: ags The tags available in an ERB file depend on the way the ERB processor is configured. Puppet always uses the same configuration for its templates (see \u201ctrim mode\u201d below), which makes the following tags available: ** <%= Ruby expression %> ** \u2014 This tag will be replaced with the value of the expression it contains. ** <% Ruby code %> ** \u2014 This tag will execute the code it contains, but will not be replaced by a value. Useful for conditional or looping logic, setting variables, and manipulating data before printing it. ** <%# comment %> ** \u2014 Anything in this tag will be suppressed in the final output. ** <%% or %%> ** \u2014 A literal <% or %>, respectively. ** <%- \u2014 Same as <% ** , but suppresses any leading whitespace in the final output. Useful when indenting blocks of code for readability. ** -%> ** \u2014 Same as ** %> ** , but suppresses the subsequent line break in the final output. Useful with many lines of non-printing code in a row, which would otherwise appear as a long stretch of blank lines.","title":"Puppet Templates"},{"location":"puppet/puppet_study_notes/#puppet-parser","text":"Is the part of puppet that pareses the puppet DSL code in manifests files. It can be called from the command line to validate your puppet code [root@pe-puppet]# puppet parser validate /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp Error: Could not parse for environment production: Syntax error at '{'; expected '}' at /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp:3","title":"Puppet parser"},{"location":"puppet/puppet_study_notes/#functions","text":"Functions are extensions of the Puppet Parser. They are located and are executed on the master and only have access to resources and data that is available on the master. During the parsing of the manifest's any function code is executed and the subsequent return value/s are inserted into the resulting compilation.","title":"Functions"},{"location":"puppet/puppet_study_notes/#there-are-two-types-of-functions-statements-and-rvalues","text":"Statements , such as the fail function, which stops the Puppet run with a parser error, perform some action. Rvalues return values and can be used anywhere a normal value is expected. (This includes resource attributes, variable assignments, conditions, selector values, the arguments of other functions, etc.) These values can come from a variety of places; template function reads and evaluates a template to return a string. stdlib\u2019s str2bool and num2bool functions convert values from one data type to another. split function parses a string and returns array elements.","title":"There are two types of functions: statements and rvalues."},{"location":"puppet/puppet_study_notes/#rbac-role-based-access-control","text":"Puppet enterprise ships with 3 default User Roles Administrators Operators Viewers Todo................................. Seamus learn the +> operator Does site.pp override the values in an enc? Does the values in an enc override the values in a class? Yumrepo['custom_packages'] -> Package <| tag == 'custom' |> Will create an order relationship with several package resources? Using puppet node to set password when the nodes cant contact the password server, do you use an exec resource ? Do you have to configure the agents to use heira or just the master? What is a node_terminus where is it set and what overrides it https://docs.puppetlabs.com/guides/external_nodes.html Learn the spaceship operator The puppet db has multiple terminus's where data is stored Facts Catalogs Resources Watch the puppetdb talk by james sweanie puppetny https://www.youtube.com/watch?v=HTr4b02aU7A at 14 mins Do the values set in the console override enc or site.pp (where are the values stored) What is the effect of node default {} in site.pp can this be overridden by ENC or by a specific node hostname {} in site.pp https://docs.puppetlabs.com/guides/external_nodes.html Workout resource interdependency loops ............ sources of information I used to study 1) {book} pro puppet second edition Last update: (made some formatting changes on 13 Feb 2024)","title":"RBAC  Role Based Access Control"},{"location":"scripting/Column_tally/","text":"simple script to tally the numbers in a defined column of text Sample input file.. row1 1 2 3 4 5 6 7 8 9 row2 2 3 4 5 6 7 8 9 10 row3 1 2 3 4 5 6 7 8 9 row4 1 2 3 4 5 6 7 8 9 row5 1 2 3 4 5 6 7 8 9 row6 1 2 3 4 5 6 7 8 9 row7 1 2 3 4 5 6 7 8 9 row8 1 2 3 4 5 6 7 8 9 row9 1 2 3 4 5 6 7 8 9 row10 1 2 3 4 5 6 7 8 9 Sample outputs ./column-tally.sh -f input -c second -v ON input is readable Column is second row1 1 1 row2 2 3 row3 1 4 row4 1 5 row5 1 6 row6 1 7 row7 1 8 row8 1 9 row9 1 10 row10 1 11 The cumulative value of the second column is 11 ./column-tally.sh -f input -c tenth -v ON input is readable Column is tenth row1 9 9 row2 10 19 row3 9 28 row4 9 37 row5 9 46 row6 9 55 row7 9 64 row8 9 73 row9 9 82 row10 9 91 The cumulative value of the tenth column is 91 #!/bin/bash usage() { cat << EOF usage: $0 -f input.txt -c first second $0 -f input.txt -c first second -v ON This script will tally the numerical values in the specified column. OPTIONS: -f input file -c column number {first,second....tenth)etc.. -v verbosity {ON} EOF } if [ $# -lt 4 ] then usage exit 1 fi while getopts \":f:c:v:\" opt; do case $opt in f) InPutFile=$OPTARG if [ -r \"$InPutFile\" ] then echo \"$InPutFile is readable\" else echo \"Unable to read input file --> $InPutFile !!!\" usage exit 1 fi ;; v) Verbose=$OPTARG ;; c) echo \"Column is $OPTARG\" Column=$OPTARG if [[ $Column == \"first\" || $Column == 'second' || $Column == 'third' || $Column == 'fourth' || $Column == 'fifth' || $Column == 'sixth' || $Column == 'seventh' || $Column == 'eighth' || $Column == 'ninth' || $Column == 'tenth' ]] then true else echo\" not a valid column value\" echo\" must be either: first second third fourth fifth sixth seventh eighth ninth tenth\" usage exit 1 fi ;; :) echo \"Option -$OPTARG requires an argument.\" >&2 usage exit 1 ;; ?) echo \"Invalid option: -$OPTARG\" >&2 usage exit 1 ;; * ) echo \"Unimplemented option: -$OPTARG\" >&2 usage exit 1 ;; esac done cumulative=0 while read -r first second third fourth fifth sixth seventh eighth ninth tenth; do let \" cumulative += Column \" let \" value = Column \" if [[ $Verbose == 'ON' || $Verbose == 'OFF' ]] then true else echo \"invalid Verbosity setting please use -v ON or -v OFF\" exit 1 fi if [ $Verbose == 'ON' ] then echo -e \"$first \\t$value \\t$cumulative\" else true fi done < $InPutFile echo \"The cumulative value of the $Column column is $cumulative\"","title":"Column tally"},{"location":"scripting/Convert_kilobytes_to_MB_GB_TB_PB/","text":"#!/bin/bash #converts kilobytes to Mega/Giga/Tera/Peta usage() { cat << EOF usage: $0 {number of Kilobytes} eg # $0 100 cumulative 100 kB # $0 1000 cumulative 1000 kB # $0 1024 cumulative 1024 kB 1 MB # $0 100000 cumulative 100000 kB 97 MB EOF } if [ $# != 1 ] then usage exit 1 fi cumulative=$1 echo cumulative $cumulative kB let cumulativeMB=$cumulative/1024 if [ $cumulativeMB -ge \"1\" ] then echo $cumulativeMB MB fi let cumulativeGB=$cumulativeMB/1024 if [ $cumulativeGB -ge \"1\" ] then echo $cumulativeGB GB let remainderGB=$cumulativeMB-1024 echo $remainderGB MB fi let cumulativeTB=$cumulativeGB/1024 if [ $cumulativeTB -ge \"1\" ] then echo $cumulativeTB TB fi","title":"Convert kilobytes to MB GB TB PB"},{"location":"scripting/DNS_validate_entries/","text":"A a script I created to validate the entries in DNS without having access to the zone files or being able to perform a zone transfer #!/bin/bash #This script will perform.. # 1. forward lookups on a list of dns shortnames # 2. it will then perform reverse lookups on the ips returned # 3. it will then perform the same steps again for various sub domains # Note: replace example with your domain name , input a list of either shortnames or FQDNs for short_name in `head list_systems | awk -F. '{ print $1 }'` do domain_name=\".example.local\" host $short_name$domain_name if [ $? = '0' ] ; then fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then bw_ip=\"`host $fw_ip | awk '{ print $5 }'`\" >> out else echo \"$short_name$domain_name reverse failed $fw_ip\" >> errors bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors fw_ip=\"\" fi domain_name=\".management.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then m_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then m_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$m_fw_ip\",\"$m_bw_ip\",\" >> m_out else echo \"$short_name$domain_name reverse failed $m_fw_ip\" >> m_errors m_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors m_fw_ip=\"\" fi domain_name=\".backupnetwork.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then b_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then b_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$b_fw_ip\",\"$b_bw_ip\",\" >> b_out else echo \"$short_name$domain_name reverse failed $b_fw_ip\" >> b_errors b_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors b_fw_ip=\"\" fi echo $short_name$domain_name\",\"$fw_ip\",\"$bw_ip\",\"\"$m_fw_ip\",\"$m_bw_ip\",\"\"$b_fw_ip\",\"$b_bw_ip\",\" >> out done","title":"DNS validate entries"},{"location":"scripting/DNS_validate_entries/#a-a-script-i-created-to-validate-the-entries-in-dns-without-having-access-to-the-zone-files-or-being-able-to-perform-a-zone-transfer","text":"#!/bin/bash #This script will perform.. # 1. forward lookups on a list of dns shortnames # 2. it will then perform reverse lookups on the ips returned # 3. it will then perform the same steps again for various sub domains # Note: replace example with your domain name , input a list of either shortnames or FQDNs for short_name in `head list_systems | awk -F. '{ print $1 }'` do domain_name=\".example.local\" host $short_name$domain_name if [ $? = '0' ] ; then fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then bw_ip=\"`host $fw_ip | awk '{ print $5 }'`\" >> out else echo \"$short_name$domain_name reverse failed $fw_ip\" >> errors bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors fw_ip=\"\" fi domain_name=\".management.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then m_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then m_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$m_fw_ip\",\"$m_bw_ip\",\" >> m_out else echo \"$short_name$domain_name reverse failed $m_fw_ip\" >> m_errors m_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors m_fw_ip=\"\" fi domain_name=\".backupnetwork.example.local\" host $short_name$domain_name if [ $? = '0' ] ; then b_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" host $fw_ip if [ $? = '0' ] ; then b_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\" echo $short_name$domain_name\",\"$b_fw_ip\",\"$b_bw_ip\",\" >> b_out else echo \"$short_name$domain_name reverse failed $b_fw_ip\" >> b_errors b_bw_ip=\"\" fi else echo \"$short_name$domain_name forward failed\" >> errors b_fw_ip=\"\" fi echo $short_name$domain_name\",\"$fw_ip\",\"$bw_ip\",\"\"$m_fw_ip\",\"$m_bw_ip\",\"\"$b_fw_ip\",\"$b_bw_ip\",\" >> out done","title":"A a script I created to validate the entries in DNS without having access to the zone files or being able to perform a zone transfer"},{"location":"scripting/IP_survey/","text":"#!/bin/bash HOST_NAME=`hostname -s` NICS=`ls /sys/class/net | grep -v lo` for i in $NICS do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://` done #Server_Name eth0 10.10.10.10 Mask:255.255.252.0 #Server_Name eth1 10.11.11.11 Mask:255.255.252.0 #Server_Name eth2 192.168.0.240 Mask:255.255.255.128","title":"IP survey"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/","text":"reverse grep to discover values from one list that are not in another list Sample Input Lists [user@server]# cat list-full [user@server]# cat list-partial 1 1 2 3 3 4 5 5 6 7 7 8 9 9 10 Sample output [user@server]# ./list-diff.sh 2 4 6 8 10 [user@server]# ./list-match.sh 1 3 5 7 9 list-match.sh #!/bin/bash # this will compare two lists and output the lines that appear in both for i in `cat list-partial` do if grep -q -v $i list-full then echo $i else true fi done list-diff.sh #!/bin/bash #this will compare two lists and output the lines that appear in the list-full only for i in `cat list-full` do if grep -q $i list-partial then true else echo $i fi done Example application You have 2 list a list containing all your servers and another list of the servers with a software package installed. for i in `cat systems_list_2014-02-20-sorted` do if grep -q $i package_listinstalledsystems_example-1.2.3-4.x86_6-sorted then true else echo $i fi done > systems_without__example-1.2.3-4.x86_6","title":"Bash and grep script to compare 2 lists"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/#reverse-grep-to-discover-values-from-one-list-that-are-not-in-another-list","text":"","title":"reverse grep to discover values from one list that are not in another list"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/#sample-input-lists","text":"[user@server]# cat list-full [user@server]# cat list-partial 1 1 2 3 3 4 5 5 6 7 7 8 9 9 10","title":"Sample Input Lists"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/#sample-output","text":"[user@server]# ./list-diff.sh 2 4 6 8 10 [user@server]# ./list-match.sh 1 3 5 7 9","title":"Sample output"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/#list-matchsh","text":"#!/bin/bash # this will compare two lists and output the lines that appear in both for i in `cat list-partial` do if grep -q -v $i list-full then echo $i else true fi done","title":"list-match.sh"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/#list-diffsh","text":"#!/bin/bash #this will compare two lists and output the lines that appear in the list-full only for i in `cat list-full` do if grep -q $i list-partial then true else echo $i fi done","title":"list-diff.sh"},{"location":"scripting/bash_and_grep_script_to_compare_2_lists/#example-application","text":"You have 2 list a list containing all your servers and another list of the servers with a software package installed. for i in `cat systems_list_2014-02-20-sorted` do if grep -q $i package_listinstalledsystems_example-1.2.3-4.x86_6-sorted then true else echo $i fi done > systems_without__example-1.2.3-4.x86_6","title":"Example application"},{"location":"scripting/bash_argument_shift_until_loop/","text":"#!/bin/bash #bash script to shift through the arguments one by one using an until loop if [ \"$#\" -le 0 ]; then echo \"You need to enter at least 1 argument\" fi until [ -z \"$1\" ] # until there are no arguments left do echo -n \"$1 \" sleep 1 shift done echo # newline","title":"Bash argument shift until loop"},{"location":"scripting/bash_argument_shift_while_loop/","text":"#!/bin/bash #bash script to shift through the arguments one by one using a while loop if [ \"$#\" -le 0 ]; then echo \"You need to enter at least 1 argument\" fi while (( $# > 0 )) do echo \"parsing the arguments $*\" echo $1 shift done","title":"Bash argument shift while loop"},{"location":"scripting/bash_arguments/","text":"#!/bin/bash if [ -z \"$1\" ]; then echo usage: $0 \"{arg} {arg} etc..\" exit fi if [ $# = 1 ]; then echo you specified a single argument exit fi if [ $# -gt 1 ]; then echo your specified $# arguments exit fi","title":"Bash arguments"},{"location":"scripting/bash_array/","text":"#!/bin/bash # examples on how to use bash arrays declare -A array array[foo]=bar array[bar]=foo echo \"${!array[@]}\" for i in \"${!array[@]}\" do echo \"key :\" $i echo \"value:\" ${array[$i]} done echo ${array[bar]}","title":"Bash array"},{"location":"scripting/bash_capture_keyboard_entry/","text":"#!/bin/bash # Author: Sigurd Solaas, 20 Apr 2011 # Used in ABS Guide with permission. # Requires version 4.2+ of Bash. # handy script to capture keyboard input and take action key=\"no value yet\" while true; do clear echo \"Bash Extra Keys Demo. Keys to try:\" echo echo \"* Insert, Delete, Home, End, Page_Up and Page_Down\" echo \"* The four arrow keys\" echo \"* Tab, enter, escape, and space key\" echo \"* The letter and number keys, etc.\" echo echo \" d = show date/time\" echo \" q = quit\" echo \"================================\" echo # Convert the separate home-key to home-key_num_7: if [ \"$key\" = $'\\x1b\\x4f\\x48' ]; then key=$'\\x1b\\x5b\\x31\\x7e' # Quoted string-expansion construct. fi # Convert the separate end-key to end-key_num_1. if [ \"$key\" = $'\\x1b\\x4f\\x46' ]; then key=$'\\x1b\\x5b\\x34\\x7e' fi case \"$key\" in $'\\x1b\\x5b\\x32\\x7e') # Insert echo Insert Key ;; $'\\x1b\\x5b\\x33\\x7e') # Delete echo Delete Key ;; $'\\x1b\\x5b\\x31\\x7e') # Home_key_num_7 echo Home Key ;; $'\\x1b\\x5b\\x34\\x7e') # End_key_num_1 echo End Key ;; $'\\x1b\\x5b\\x35\\x7e') # Page_Up echo Page_Up ;; $'\\x1b\\x5b\\x36\\x7e') # Page_Down echo Page_Down ;; $'\\x1b\\x5b\\x41') # Up_arrow echo Up arrow ;; $'\\x1b\\x5b\\x42') # Down_arrow echo Down arrow ;; $'\\x1b\\x5b\\x43') # Right_arrow echo Right arrow ;; $'\\x1b\\x5b\\x44') # Left_arrow echo Left arrow ;; $'\\x09') # Tab echo Tab Key ;; $'\\x0a') # Enter echo Enter Key ;; $'\\x1b') # Escape echo Escape Key ;; $'\\x20') # Space echo Space Key ;; d) date ;; q) echo Time to quit... echo exit 0 ;; *) echo You pressed: \\'\"$key\"\\' ;; esac echo echo \"================================\" unset K1 K2 K3 read -s -N1 -p \"Press a key: \" K1=\"$REPLY\" read -s -N2 -t 0.001 K2=\"$REPLY\" read -s -N1 -t 0.001 K3=\"$REPLY\" key=\"$K1$K2$K3\" done exit $?","title":"Bash capture keyboard entry"},{"location":"scripting/bash_case/","text":"#!/bin/bash echo -n what colour \\? read colour echo \"starting an xterm with $colour text\" case $colour in [Bb]lu?) xterm -foreground blue & ;; [Gg]reen) xterm -foreground darkgreen & ;; red) echo red xterm -foreground red & ;; *) xterm & ;; esac","title":"Bash case"},{"location":"scripting/bash_fail_and_exit/","text":"This is a handy function for when you have a script that you need to run interactively you can call it with the || or operator, if any steps fail it will print out the line number and exit #!/bin/bash fail_notify() { echo \"step \"$@\" on line $BASH_LINENO failed <><><><><><><><><><><><><><><\" exit } mount -o rw,remount / || fail_notify \"remount of root\"","title":"Bash fail and exit"},{"location":"scripting/bash_fail_trap/","text":"Using the bash option set -e / set -o errexit and a trap to discontinue the execution of a script and notify the user if any step fails. #!/bin/bash fail_notice () { echo \"Command $BASH_COMMAND failed\" exit } trap fail_notice EXIT set -o errexit #Exit the script if any untested command fails. echo \"before fail\" false # cause non zero exit code and trigger the trap echo \"after fail\" trap Exit # need to reset the Exit trap otherwise the fail_notice() # will be triggered upon script completion (exit).","title":"Bash fail trap"},{"location":"scripting/bash_fail_trap/#using-the-bash-option-set-e-set-o-errexit-and-a-trap-to-discontinue-the-execution-of-a-script-and-notify-the-user-if-any-step-fails","text":"#!/bin/bash fail_notice () { echo \"Command $BASH_COMMAND failed\" exit } trap fail_notice EXIT set -o errexit #Exit the script if any untested command fails. echo \"before fail\" false # cause non zero exit code and trigger the trap echo \"after fail\" trap Exit # need to reset the Exit trap otherwise the fail_notice() # will be triggered upon script completion (exit).","title":"Using the bash option set -e / set -o errexit and a trap to discontinue the execution of a script and notify the user if any step fails."},{"location":"scripting/bash_function/","text":"#!/bin/bash #bash functions function my_fn { echo function has ran with $1 } function exit_fn { echo \"exit function called\" exit } my_fn seamus my_fn seamus2 exit_fn echo \"exit_fn call failed\"","title":"Bash function"},{"location":"scripting/bash_functions/","text":"from http://tldp.org/LDP/abs/html/complexfunct.html #!/bin/bash # Functions and parameters DEFAULT=default # Default param value. func2 () { if [ -z \"$1\" ] # Is parameter #1 zero length? then echo \"-Parameter #1 is zero length.-\" # Or no parameter passed. else echo \"-Parameter #1 is \\\"$1\\\".-\" fi variable=${1-$DEFAULT} # What does echo \"variable = $variable\" #+ parameter substitution show? # --------------------------- # It distinguishes between #+ no param and a null param. if [ \"$2\" ] then echo \"-Parameter #2 is \\\"$2\\\".-\" fi return 0 } func2 $1 $2 Execute the script... scriptname.sh 1 scriptname.sh 2","title":"Bash functions"},{"location":"scripting/bash_local_vs_global_variables/","text":"[user@server]# ./functions-and-variables.sh Local Variable value before calling function1 = Australia Local Variable value during calling function1 = NewZealand Local Variable value after calling function1 = Australia Global Variable value before calling function1 = Earth Global variable value during calling function1 = Mars Global Variable value after calling function1 = Mars [user@server]# cat functions-and-variables.sh #!/bin/bash country=Australia planet=Earth function local_var_function { local country=NewZealand echo \"Local Variable value during calling function1 = $country \" } echo \"Local Variable value before calling function1 = $country \" local_var_function echo \"Local Variable value after calling function1 = $country \" function global_var_function { planet=Mars #changing the global variable echo \"Global variable value during calling function1 = $planet \" } echo \"Global Variable value before calling function1 = $planet \" global_var_function echo \"Global Variable value after calling function1 = $planet \"","title":"Bash local vs global variables"},{"location":"scripting/bash_nested_if_else/","text":"#!/bin/bash #nested if else statement if [ \"$1\" -gt 1 ] then if [ \"$1\" -lt 4 ] then echo is between 2 or 3 else echo is greater than or equal to 4 fi else echo is less than or equal to 1 fi","title":"Bash nested if else"},{"location":"scripting/bash_read_input/","text":"#!/bin/bash #simple example of how to read user input with bash echo \"Please enter a word\" read word echo Please enter your first and last name read Last First echo \"$First $Last, You entered the word $word\"","title":"Bash read input"},{"location":"scripting/bash_scripting/","text":"Bash Scripting Bash case statement example #!/bin/bash echo -n \"what colour ? red green or blue \" read colour case $colour in [Bb]lu?) echo creating a bash subshell with blue foreground env PS1=\"\\e[1;34m[\\u@\\h \\W]\\$ \\e[m \" bash -i ;; [Gg]reen) echo creating a bash subshell with green foreground env PS1=\"\\e[1;32m[\\u@\\h \\W]\\$ \\e[m \" bash -i ;; red) echo creating a bash subshell with red foreground env PS1=\"\\e[1;31m[\\u@\\h \\W]\\$ \\e[m \" bash -i ;; *) echo \"invalid selection\" & ;; esac Bash nested if else statement #!/bin/bash #bash nested if else statement if [ -z \"$1\" ] then echo \"Usage $0 {integer between 1-10}\" exit 1 else true fi if [ \"$1\" -gt 1 ] then if [ \"$1\" -lt 4 ] then echo is between 2 or 3 else echo is greater than or equal to 4 fi else echo is less than or equal to 1 fi Bash wait for user interaction #!/bin/bash #echo are you sure ? #read VALUE until [[ \"$VALUE\" = \"yes\" ]] do echo \"are you sure ?\" read VALUE done Bash select menu example 1 #!/bin/bash OPTIONS=\"Hello Quit extra\" select opt in $OPTIONS; do if [ \"$opt\" = \"Quit\" ]; then echo done exit elif [ \"$opt\" = \"Hello\" ]; then echo Hello World elif [ \"$opt\" = \"extra\" ]; then echo \"Sorry! extra is not implemented yet\" else clear echo bad option fi done Bash select menu example 2 #!/bin/bash PS3=\"Select program: \" select program in ls pwd date top exit do $program done Bash shifting through arguments #!/bin/bash if [ \"$#\" == \"0\" ]; then echo \"Usage: $0 ARG1 ARG2 ARG3 etc....\" exit 1 fi while (( \"$#\" )) do echo -e \"processing argument $1 \\n\" sleep 1 shift done exit Bash compare 2 strings #really bad dont use on production code #!/bin/bash #bash test compare 2 strings #Usage: string-compare.sh string1 string2 S1=$1 S2=$2 if [ \"$S1\" != \"$S2\" ] ;then echo \"S1('$S1') is not equal to S2('$S2')\" fi if [ \"$S1\" = \"$S2\" ] ;then echo \"S1('$S1') is equal to S2('$S2')\" fi if [[ \"$S1\" == *\"$S2\"* ]]; then echo \"String 1 contains String 2\" fi if [[ \"$S2\" == *\"$S1\"* ]]; then #if [[ \"*\"$S1\"*\" == \"$S2\" ]]; then echo \"String 2 contains String 1\" fi Bash script using lock file to control another script #!/bin/bash answer=unset if [ -f /tmp/stop ] then echo -e \"/tmp/stop exists \\n exiting\" exit 1 else while [ ! -f /tmp/stop ] do echo \"go\" && sleep 1 done fi echo stopping Until loop example #!/bin/bash a=0 while [ $a -ne 10 ] do echo $a a=$(( $a +1 )) done While loop example #!/bin/bash a=0 until [ $a -gt 10 ] do echo $a a=$(( $a +1 )) done","title":"Bash scripting"},{"location":"scripting/bash_scripting/#bash-case-statement-example","text":"#!/bin/bash echo -n \"what colour ? red green or blue \" read colour case $colour in [Bb]lu?) echo creating a bash subshell with blue foreground env PS1=\"\\e[1;34m[\\u@\\h \\W]\\$ \\e[m \" bash -i ;; [Gg]reen) echo creating a bash subshell with green foreground env PS1=\"\\e[1;32m[\\u@\\h \\W]\\$ \\e[m \" bash -i ;; red) echo creating a bash subshell with red foreground env PS1=\"\\e[1;31m[\\u@\\h \\W]\\$ \\e[m \" bash -i ;; *) echo \"invalid selection\" & ;; esac","title":"Bash case statement example"},{"location":"scripting/bash_scripting/#bash-nested-if-else-statement","text":"#!/bin/bash #bash nested if else statement if [ -z \"$1\" ] then echo \"Usage $0 {integer between 1-10}\" exit 1 else true fi if [ \"$1\" -gt 1 ] then if [ \"$1\" -lt 4 ] then echo is between 2 or 3 else echo is greater than or equal to 4 fi else echo is less than or equal to 1 fi","title":"Bash nested if else statement"},{"location":"scripting/bash_scripting/#bash-wait-for-user-interaction","text":"#!/bin/bash #echo are you sure ? #read VALUE until [[ \"$VALUE\" = \"yes\" ]] do echo \"are you sure ?\" read VALUE done","title":"Bash wait for user interaction"},{"location":"scripting/bash_scripting/#bash-select-menu-example-1","text":"#!/bin/bash OPTIONS=\"Hello Quit extra\" select opt in $OPTIONS; do if [ \"$opt\" = \"Quit\" ]; then echo done exit elif [ \"$opt\" = \"Hello\" ]; then echo Hello World elif [ \"$opt\" = \"extra\" ]; then echo \"Sorry! extra is not implemented yet\" else clear echo bad option fi done","title":"Bash select menu example 1"},{"location":"scripting/bash_scripting/#bash-select-menu-example-2","text":"#!/bin/bash PS3=\"Select program: \" select program in ls pwd date top exit do $program done","title":"Bash select menu example 2"},{"location":"scripting/bash_scripting/#bash-shifting-through-arguments","text":"#!/bin/bash if [ \"$#\" == \"0\" ]; then echo \"Usage: $0 ARG1 ARG2 ARG3 etc....\" exit 1 fi while (( \"$#\" )) do echo -e \"processing argument $1 \\n\" sleep 1 shift done exit","title":"Bash shifting through arguments"},{"location":"scripting/bash_scripting/#bash-compare-2-strings-really-bad-dont-use-on-production-code","text":"#!/bin/bash #bash test compare 2 strings #Usage: string-compare.sh string1 string2 S1=$1 S2=$2 if [ \"$S1\" != \"$S2\" ] ;then echo \"S1('$S1') is not equal to S2('$S2')\" fi if [ \"$S1\" = \"$S2\" ] ;then echo \"S1('$S1') is equal to S2('$S2')\" fi if [[ \"$S1\" == *\"$S2\"* ]]; then echo \"String 1 contains String 2\" fi if [[ \"$S2\" == *\"$S1\"* ]]; then #if [[ \"*\"$S1\"*\" == \"$S2\" ]]; then echo \"String 2 contains String 1\" fi","title":"Bash compare 2 strings    #really bad dont use on production code"},{"location":"scripting/bash_scripting/#bash-script-using-lock-file-to-control-another-script","text":"#!/bin/bash answer=unset if [ -f /tmp/stop ] then echo -e \"/tmp/stop exists \\n exiting\" exit 1 else while [ ! -f /tmp/stop ] do echo \"go\" && sleep 1 done fi echo stopping","title":"Bash script using lock file to control another script"},{"location":"scripting/bash_scripting/#until-loop-example","text":"#!/bin/bash a=0 while [ $a -ne 10 ] do echo $a a=$(( $a +1 )) done","title":"Until loop example"},{"location":"scripting/bash_scripting/#while-loop-example","text":"#!/bin/bash a=0 until [ $a -gt 10 ] do echo $a a=$(( $a +1 )) done","title":"While loop example"},{"location":"scripting/bash_select/","text":"#!/bin/bash #Using the bash select construct to generate a Menu echo Select an option from the menu... OPTIONS=\"Hello Quit Clear\" select opt in $OPTIONS; do if [ \"$opt\" = \"Quit\" ]; then echo Quitting from Menu exit elif [ \"$opt\" = \"Hello\" ]; then echo Hello World elif [ \"$opt\" = \"Clear\" ]; then echo Clearing Screen..... sleep 2 clear else echo That was not a valid menu option ! sleep 1 fi done","title":"Bash select"},{"location":"scripting/bash_string_comparison/","text":"#!/bin/bash S1=$1 S2=$2 if [ $S1 != $S2 ]; then echo \"S1('$S1') is not equal to S2('$S2')\" fi if [ $S1 = $S2 ]; then echo \"S1('$S1') is equal to S2('$S2')\" fi","title":"Bash string comparison"},{"location":"scripting/bash_test_to_only_run_as_root/","text":"#!/bin/bash if [ \"${UID}\" != \"0\" ] then echo \"Permission denied!!! You need to be root to execute this command\" exit 1 else echo \"You are root\" fi","title":"Bash test to only run as root"},{"location":"scripting/bash_tips/","text":"mv this-is-a-really-long-filename this-is-a-really-long-filename.bak mv this-is-a-really-long-filename{,.bak} if you're typing a multi line command, instead of editing the line by going back and forth use the \"fc\" command to open the line in an editor If you want to use the last argument from your last command press ctrl+. If you want to use the first argument from your last command press alt+ctrl+y You can use !: (word designator) to reference arguments from the previous command line. !:0 is the command, !:1 is the first argument !:2 is the second etc... $ ls -l / ... $ echo !:1 -l","title":"Bash tips"},{"location":"scripting/bash_trap_user_ctrl-c/","text":"#!/bin/bash # Example of how to trap a user pressing ^c during script execution echo \"Starting... Please wait until I have completed\" trap message INT function message() { echo \"Please do not use CTRL^C, I have files open\" sleep 5 } for i in `seq 10 -1 0`; do echo -ne \"\\r $i seconds remaining \" sleep 1 done echo \"I have finished\"","title":"Bash trap user ctrl c"},{"location":"scripting/bash_wait_for_user_input/","text":"#!/bin/bash #bash wait until user enters \"yes\" until [[ \"$VALUE\" = \"yes\" ]] do echo \"are you sure you want to quit ?\" read VALUE done","title":"Bash wait for user input"},{"location":"scripting/diff/","text":"Ocasionaly i have needed to compare parts of the lines in log files or other text files. The diff command has a handy option of ignoring the first X bytes of each line which is especially handy to exclude time stamps. Bellow we will create 2 input files that will illustrate the point. [seamusmurray@example ~]$ cat seamus.before 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 12345678901234567890 [seamusmurray@example ~]$ cat seamus.after 12345678901234567890 2345678901234567890 1 345678901234567890 12 45678901234567890 123 5678901234567890 1234 678901234567890 12345 78901234567890 123456 8901234567890 1234567 901234567890 12345678 01234567890 123456789 1234567890 1234567890 234567890 12345678901 34567890 123456789012 4567890 1234567890123 567890 12345678901234 67890 123456789012345 7890 1234567890123456 890 12345678901234567 90 123456789012345678 0 1234567890123456789 12345678901234567890 Diif of just the characters after column 19 [seamusmurray@example ~]$ diff -y <(cut -b19- seamus.before) <(cut -b19- seamus.after) 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 | 0 90 | 9 90 90 [seamusmurray@example ~]$ diff <(cut -b19- seamus.before) <(cut -b19- seamus.after) 20,21c20,21 < 90 < 90 --- > 0 > 9","title":"Diff"},{"location":"scripting/expect/","text":"###Warning I have no idea when I wrote this and when I last used expect #!/usr/bin/expect set timeout 20 set host [lindex $argv 0] set password \"secret\" set prompt \"$ \" proc dostuff {} { send -- \"put seamus.sh\\r\" return } #spawn scp -q seamus.sh ${host}:/tmp/shell_script.sh spawn sftp -q ${host} while (1) { expect { ;# -- This is the prompt when you first use ;# -- ssh that says \"Are you sure you want to continue ...\" \"no)? \" { send -- \"yes\\r\" } ;# -- the prompt for password \"Password: \" { send -- \"$password\\r\" } ;# -- and finally we got a shell prompt \"$prompt\" { dostuff break } } } ;# -- exit expect \"$prompt\" send -- \"exit\\r\" expect eof ####################################################################### #!/usr/bin/expect set timeout 20 set password [lindex $argv 0] spawn sudo su \\- expect \"seamusmurray:\" send \"$password\\r\";","title":"Expect"},{"location":"scripting/map/","text":". \u251c\u2500\u2500 bash_and_grep_script_to_compare_2_lists.md \u251c\u2500\u2500 bash_and_powershell_failover \u2502 \u251c\u2500\u2500 failover \u2502 \u2502 \u251c\u2500\u2500 BreakLoop.sh \u2502 \u2502 \u251c\u2500\u2500 Break.sh \u2502 \u2502 \u251c\u2500\u2500 check_lagtime \u2502 \u2502 \u2514\u2500\u2500 fake.sh \u2502 \u2514\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 DR \u2502 \u2502 \u251c\u2500\u2500 break_snap_mirror.sh \u2502 \u2502 \u251c\u2500\u2500 initiate_dr.sh \u2502 \u2502 \u251c\u2500\u2500 options.txt \u2502 \u2502 \u2514\u2500\u2500 primary_confirm_fail.sh \u2502 \u251c\u2500\u2500 fence_netapp \u2502 \u2502 \u2514\u2500\u2500 fence.txt.txt \u2502 \u251c\u2500\u2500 Primary \u2502 \u2502 \u2514\u2500\u2500 primary_test.sh \u2502 \u251c\u2500\u2500 steward \u2502 \u2502 \u251c\u2500\u2500 schedule_forwarderA.xml \u2502 \u2502 \u251c\u2500\u2500 slc_steward.ps1 \u2502 \u2502 \u2514\u2500\u2500 SSH_Sessions.PSM1 \u2502 \u2514\u2500\u2500 where_is_the_luns.sh \u251c\u2500\u2500 bash_argument_shift_until_loop.md \u251c\u2500\u2500 bash_argument_shift_while_loop.md \u251c\u2500\u2500 bash_arguments.md \u251c\u2500\u2500 bash_array \u251c\u2500\u2500 bash_capture_keyboard_entry.md \u251c\u2500\u2500 bash_case.md \u251c\u2500\u2500 bash_fail_and_exit.md \u251c\u2500\u2500 bash_fail_trap.md \u251c\u2500\u2500 bash_function.md \u251c\u2500\u2500 bash_functions.md \u251c\u2500\u2500 bash_local_vs_global_variables.md \u251c\u2500\u2500 bash_nested_if_else.md \u251c\u2500\u2500 bash_read_input.md \u251c\u2500\u2500 Bash_scripting.md \u251c\u2500\u2500 bash_select.md \u251c\u2500\u2500 bash_sort.md \u251c\u2500\u2500 bash_string_comparison.md \u251c\u2500\u2500 bash_test_to_only_run_as_root.md \u251c\u2500\u2500 bash_tips.md \u251c\u2500\u2500 bash_trap_user_ctrl-c.md \u251c\u2500\u2500 bash_wait_for_user_input.md \u251c\u2500\u2500 Column_tally.md \u251c\u2500\u2500 Convert_kilobytes_to_MB_GB_TB_PB.md \u251c\u2500\u2500 diff.md \u251c\u2500\u2500 DNS_validate_entries.md \u251c\u2500\u2500 expect.md \u251c\u2500\u2500 IP_survey.md \u251c\u2500\u2500 map.md \u251c\u2500\u2500 script_to_markdown.md \u251c\u2500\u2500 Shuffle_deck_of_cards.md \u251c\u2500\u2500 simulate_site_failure.md \u2514\u2500\u2500 use_a_file_to_control_bash_script_operation.md 7 directories, 47 files","title":"Map"},{"location":"scripting/script_to_markdown/","text":"sed -i s/^/\\ \\ \\ \\ / <file> fix the urls so the files can be served from local filesystem on phone for i in `find . | grep '.html'` ; do sed -i s/\\\\/\\\"\\>/\\\\/index.html\\\"\\>/g $i ; done remove the footer for i in `find . | grep '.html'` ; do sed -i '/Documentation\\ built\\ with/d' $i ; done create the map pages tree | sed s/^/\\ \\ \\ \\ / > map.md for i in find . | grep '.html' ; do sed -i '/icon-home/d' $i ; done","title":"Script to markdown"},{"location":"scripting/simulate_site_failure/","text":"I generated random numbers into a file 0.00 to 9.99 while true ; do echo `echo $RANDOM | cut -c 1`\".\"`echo $RANDOM | cut -c 1-2` ; done > random #!/bin/bash for SLEEP in `cat /root/random` do sleep 10 echo \"ifdown eth0 ; sleep $SLEEP ; ifup eth0\" echo `date` ifdown eth0 ; sleep $SLEEP ; ifup eth0 ; sleep $SLEEP done","title":"Simulate site failure"},{"location":"scripting/use_a_file_to_control_bash_script_operation/","text":"#!/bin/bash #Using a file to control the operation of a bash script #Handy in situations where you are looking for a log file or another process or job to create/update or delete a file control_file=\"/tmp/control\" control_value=\"start\" if [ -e $control_file ] ; then echo \"Terminating due to the presence of the control file $control_file\" echo \"No actions were perfomred\" exit fi echo \"I will continue to run until I detect the control file $control_file\" echo \"You can stop this script by creating the control file e.g.\" echo \"# touch $control_file\" until [ -e $control_file ] do echo \"Performing some arbitrary action.....\" && sleep 1 done echo Program $0 has finished.","title":"Use a file to control bash script operation"},{"location":"time/NTP_Authentication/","text":"To configure symmetric key NTP authentication for the RedHat NTP client, the following 2 files need to be configured. /etc/ntp.conf #symmetric key authentication restrict -6 default kod nomodify nopeer notrap noquery restrict 127.0.0.1 restrict -6 ::1 tinker panic 0 driftfile /var/lib/ntp/drift keys /etc/ntp/keys server 10.10.10.1 version 2 key 2 server 10.0.10.1 version 2 key 2 trustedkey 2 requestkey 2 controlkey 2 /etc/ntp/keys #key_number key_type Key 2 M !@#$%^&*() Unfortunately the default logging level and debug mode of the redhat ntp client does not output any useful messages to syslog regarding symmetric key authentication. Therefore to fault find you will have to use a combination of... ntpq -p ## print list of peers and their state ntpq -c as ## print a list of association identifiers and peer statuses ntpd in debug mode ## probably best to just ignore this option or anything else in syslog besides the message indicating a successful sync Example of outputs when everything is working When using the correct key, after starting the ntpd you should immediately see the output for [root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 16777 f014 yes yes ok reject reachable 1 2 16778 f014 yes yes ok reject reachable 1 reach = yes auth = ok condition = reject last_event = reachable The ntp -q wont show an asterix (sys.peer) until the client has had enough time to validate the stability of the ntp server and sync its clock [root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== router1. 10.10.0.1 2 u 17 64 1 0.622 0.502 0.001 router2. 10.11.0.1 2 u 16 64 1 16.264 -0.395 0.001 After a few minutes (depending on the time disparity) the clocks should sync and the outputs should be... [root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 16777 f414 yes yes ok candidat reachable 1 2 16778 f614 yes yes ok sys.peer reachable 1 [root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== +router1. 10.10.0.1 2 u 56 64 377 0.615 0.767 0.127 *router2. 10.11.0.1 2 u 59 64 377 15.865 -0.095 0.134 When the client has been synchronized you should see a message similar to the following syslog message Aug 5 12:22:53 linuxserver1 ntpd[32065]: synchronized to 10.10.10.1, stratum 2 Possible error scenarios 1. Specifying a key number that doesn't match on the ntp server, eg... client specifies key number 1 server has key number 2 [root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== router1. .AUTH. 16 u - 64 0 0.000 0.000 0.000 router2. .AUTH. 16 u - 64 0 0.000 0.000 0.000 [root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 62393 c000 yes yes bad reject 2 62394 c000 yes yes bad reject syslog message Aug 8 11:30:30 linuxserver1 ntpd[20387]: transmit: 10.10.0.1 key 1 not found 2. When using an incorrect or corrupted key (eg...if using a script to deploy the key have you escaped the extended ascii values in the key) [root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== router1. .AUTH. 16 u - 64 0 0.000 0.000 0.001 router2. .AUTH. 16 u - 64 0 0.000 0.000 0.001 [root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 62393 c000 yes yes bad reject 2 62394 c000 yes yes bad reject ~~Note there is no indication in /var/log/messages that the key is bad~~ 3. Specifying an NTP server device IP when that device dose not have symmetric authentication turned on ~~ the client authenticates even though it should not ~~ Aug 9 13:55:17 linuxserver1 ntpd[4101]: synchronized to router3, stratum 2 remote refid st t when poll reach delay offset jitter ============================================================================== +router3. 10.10.0.1 2 u 2 64 377 0.566 -1.045 1.085 *router4. 10.11.0.1 2 u 6 64 377 15.751 1.584 1.122 ind assID status conf reach auth condition last_event cnt =========================================================== 1 61738 f414 yes yes ok candidat reachable 1 2 61739 f614 yes yes ok sys.peer reachable 1 ~~ it appears that the device may authenticate to the local router that is using symmetric authentication to its upstream device 4. When specifying a windows domain controller that does not support symmetric key authentication remote refid st t when poll reach delay offset jitter ============================================================================== router1. .INIT. 16 u - 64 0 0.000 0.000 0.000 router2. .INIT. 16 u - 64 0 0.000 0.000 0.000 ind assID status conf reach auth condition last_event cnt =========================================================== 1 30681 c000 yes yes bad reject 2 30682 c000 yes yes bad reject 5. Specifying the incorrect IP or the IP of a non reachable NTP device remote refid st t when poll reach delay offset jitter ============================================================================== 100.80.88.255 .INIT. 16 u - 64 0 0.000 0.000 0.000 100.80.88.254 .INIT. 16 u - 64 0 0.000 0.000 0.000 ind assID status conf reach auth condition last_event cnt =========================================================== 1 47581 c000 yes yes bad reject 2 47582 c000 yes yes bad reject","title":"NTP Authentication"},{"location":"time/NTP_Authentication/#etcntpconf","text":"#symmetric key authentication restrict -6 default kod nomodify nopeer notrap noquery restrict 127.0.0.1 restrict -6 ::1 tinker panic 0 driftfile /var/lib/ntp/drift keys /etc/ntp/keys server 10.10.10.1 version 2 key 2 server 10.0.10.1 version 2 key 2 trustedkey 2 requestkey 2 controlkey 2","title":"/etc/ntp.conf"},{"location":"time/NTP_Authentication/#etcntpkeys","text":"#key_number key_type Key 2 M !@#$%^&*() Unfortunately the default logging level and debug mode of the redhat ntp client does not output any useful messages to syslog regarding symmetric key authentication. Therefore to fault find you will have to use a combination of... ntpq -p ## print list of peers and their state ntpq -c as ## print a list of association identifiers and peer statuses ntpd in debug mode ## probably best to just ignore this option or anything else in syslog besides the message indicating a successful sync","title":"/etc/ntp/keys"},{"location":"time/NTP_Authentication/#example-of-outputs-when-everything-is-working","text":"","title":"Example of outputs when everything is working"},{"location":"time/NTP_Authentication/#when-using-the-correct-key-after-starting-the-ntpd-you-should-immediately-see-the-output-for","text":"[root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 16777 f014 yes yes ok reject reachable 1 2 16778 f014 yes yes ok reject reachable 1 reach = yes auth = ok condition = reject last_event = reachable","title":"When using the correct key, after starting the ntpd you should immediately see the output for"},{"location":"time/NTP_Authentication/#the-ntp-q-wont-show-an-asterix-syspeer-until-the-client-has-had-enough-time-to-validate-the-stability-of-the-ntp-server-and-sync-its-clock","text":"[root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== router1. 10.10.0.1 2 u 17 64 1 0.622 0.502 0.001 router2. 10.11.0.1 2 u 16 64 1 16.264 -0.395 0.001","title":"The ntp -q wont show an asterix (sys.peer) until the client has had enough time to validate the stability of the ntp server and sync its clock"},{"location":"time/NTP_Authentication/#after-a-few-minutes-depending-on-the-time-disparity-the-clocks-should-sync-and-the-outputs-should-be","text":"[root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 16777 f414 yes yes ok candidat reachable 1 2 16778 f614 yes yes ok sys.peer reachable 1 [root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== +router1. 10.10.0.1 2 u 56 64 377 0.615 0.767 0.127 *router2. 10.11.0.1 2 u 59 64 377 15.865 -0.095 0.134","title":"After a few minutes (depending on the time disparity) the clocks should sync and the outputs should be..."},{"location":"time/NTP_Authentication/#when-the-client-has-been-synchronized-you-should-see-a-message-similar-to-the-following","text":"syslog message Aug 5 12:22:53 linuxserver1 ntpd[32065]: synchronized to 10.10.10.1, stratum 2","title":"When the client has been synchronized you should see a message similar to the following"},{"location":"time/NTP_Authentication/#possible-error-scenarios","text":"","title":"Possible error scenarios"},{"location":"time/NTP_Authentication/#1-specifying-a-key-number-that-doesnt-match-on-the-ntp-server","text":"","title":"1. Specifying a key number that doesn't match on the ntp server,"},{"location":"time/NTP_Authentication/#eg-client-specifies-key-number-1-server-has-key-number-2","text":"[root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== router1. .AUTH. 16 u - 64 0 0.000 0.000 0.000 router2. .AUTH. 16 u - 64 0 0.000 0.000 0.000 [root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 62393 c000 yes yes bad reject 2 62394 c000 yes yes bad reject syslog message Aug 8 11:30:30 linuxserver1 ntpd[20387]: transmit: 10.10.0.1 key 1 not found","title":"eg... client specifies key number 1  server has key number 2"},{"location":"time/NTP_Authentication/#2-when-using-an-incorrect-or-corrupted-key-egif-using-a-script-to-deploy-the-key-have-you-escaped-the-extended-ascii-values-in-the-key","text":"[root@linuxserver1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== router1. .AUTH. 16 u - 64 0 0.000 0.000 0.001 router2. .AUTH. 16 u - 64 0 0.000 0.000 0.001 [root@linuxserver1 ~]# ntpq -c as ind assID status conf reach auth condition last_event cnt =========================================================== 1 62393 c000 yes yes bad reject 2 62394 c000 yes yes bad reject ~~Note there is no indication in /var/log/messages that the key is bad~~","title":"2. When using an incorrect or corrupted key (eg...if using a script to deploy the key have you escaped the extended ascii values in the key)"},{"location":"time/NTP_Authentication/#3-specifying-an-ntp-server-device-ip-when-that-device-dose-not-have-symmetric-authentication-turned-on","text":"~~ the client authenticates even though it should not ~~ Aug 9 13:55:17 linuxserver1 ntpd[4101]: synchronized to router3, stratum 2 remote refid st t when poll reach delay offset jitter ============================================================================== +router3. 10.10.0.1 2 u 2 64 377 0.566 -1.045 1.085 *router4. 10.11.0.1 2 u 6 64 377 15.751 1.584 1.122 ind assID status conf reach auth condition last_event cnt =========================================================== 1 61738 f414 yes yes ok candidat reachable 1 2 61739 f614 yes yes ok sys.peer reachable 1 ~~ it appears that the device may authenticate to the local router that is using symmetric authentication to its upstream device","title":"3. Specifying an NTP server device IP when that device dose not have symmetric authentication turned on"},{"location":"time/NTP_Authentication/#4-when-specifying-a-windows-domain-controller-that-does-not-support-symmetric-key-authentication","text":"remote refid st t when poll reach delay offset jitter ============================================================================== router1. .INIT. 16 u - 64 0 0.000 0.000 0.000 router2. .INIT. 16 u - 64 0 0.000 0.000 0.000 ind assID status conf reach auth condition last_event cnt =========================================================== 1 30681 c000 yes yes bad reject 2 30682 c000 yes yes bad reject","title":"4. When specifying a windows domain controller that does not support symmetric key authentication"},{"location":"time/NTP_Authentication/#5-specifying-the-incorrect-ip-or-the-ip-of-a-non-reachable-ntp-device","text":"remote refid st t when poll reach delay offset jitter ============================================================================== 100.80.88.255 .INIT. 16 u - 64 0 0.000 0.000 0.000 100.80.88.254 .INIT. 16 u - 64 0 0.000 0.000 0.000 ind assID status conf reach auth condition last_event cnt =========================================================== 1 47581 c000 yes yes bad reject 2 47582 c000 yes yes bad reject","title":"5. Specifying the incorrect IP or the IP of a non reachable NTP device"},{"location":"time/Setting_time/","text":"If you need to quickly jump the system time to the correct time. Before you do this, you should shut down any databases, etc... running on the server. ntpdate time.nist.gov set the CMOS/BIOS/TRC clock hwclock -w If you don't have an NTP service available, you can set the clock based on the current time on another server date +%Y%m%d%T -s \"`ssh user@remoteserver 'date \"+%Y%m%d%T\"'`\"","title":"Setting time"},{"location":"time/Setting_time/#if-you-need-to-quickly-jump-the-system-time-to-the-correct-time-before-you-do-this-you-should-shut-down-any-databases-etc-running-on-the-server","text":"ntpdate time.nist.gov","title":"If you need to quickly jump the system time to the correct time. Before you do this, you should shut down any databases, etc... running on the server."},{"location":"time/Setting_time/#set-the-cmosbiostrc-clock","text":"hwclock -w","title":"set the CMOS/BIOS/TRC clock"},{"location":"time/Setting_time/#if-you-dont-have-an-ntp-service-available-you-can-set-the-clock-based-on-the-current-time-on-another-server","text":"date +%Y%m%d%T -s \"`ssh user@remoteserver 'date \"+%Y%m%d%T\"'`\"","title":"If you don't have an NTP service available, you can set  the clock based on the current time on another server"},{"location":"time/System_Timing_and_NTP/","text":"Last updated by Seamus Murray on 20240215 Purpose of this Document: Act as a repository for separate but related system timing information. Index: Definitions of the terms used in this document Description of the 4 main clock sources referred to in this document Maintaining System Time on a single core fixed frequency x86 system Advancements of x86 CPU's and the effects upon the MonoTonic TSC CPU Bugs and Clock Sources TimeLines, OffSets and Clock Synchronization Examples of Clock Drift Calculating the RoundTrip Delay and Clock OffSet between 2 Clock Sources NTP Client / NTP Server polling mechanism. NTP Precision and Accuracy Example C programs to poll the TSC, HPET,and ACPI_PM clock sources Sources for this document Definitions of the terms used in this document: Term Definition Wall Clock Time Time displayed in a format understandable to humans, such as \"12:00 am, Jan 1st, 1970.\" Real-Time Clock A hardware-based clock found on system motherboards, functions independent of a computer's power state. System Time A software-based clock or time line initialized during system boot from the real-time clock Reference Clock A highly precise and stable time source used to synchronize and calibrate other clocks. Clock Cycles A measure of the transitions in an electronic circuit, akin to the movement of a production line. Counter A device that increments based on specific events or time periods, independent of CPU-executed code. Timer A device that tracks the number of increments occurring between two points in time. Clock Source A device queried by CPU-executed code to obtain the current value of a counter. Monotonic Counter A counter that increments at regular intervals, such as once per second, analogous to a metronome. System A self-contained computing entity, such as a Linux server, Windows desktop, or Mac laptop. Time Line A graphical or numerical depiction of the passage of time. Time Scale A representation of time, either graphical or numerical, adjusted to illustrate drift or skew Description of the 4 main clock sources referred to in this document RTC (Real Time Clock) A hardware device used to keep track of time even when a system is powered off. Similar to a battery-operated digital Wrist Watch. A quartz crystal is used as a stable oscillator (32Khz). When an x86 computer is powered off, the Real Time Clock on the x86 computer's motherboard is powered by a battery and is able to maintain an approximation of WallClock time. These RTCs tend to be fairly inaccurate, the amount by which they drift from the true WallClock time can depend on several factors such as the quality of the electronic components and temperature variations. When an x86 computer is powered off there is no System Time, Clock Cycles, Interrupts, Counters, or Timers. Problems with RTC Subject to temperature changes. Subject to manufacturing tolerances. Modern CPU clock rates are much higher than the 32khz (clock) rate of RTC, therefor the resolution that they provide is not sufficiently high enough for modern CPUs. It is very computationally expensive and slow for the OS to poll the RTC. TSC (Time Stamp Counter) The TSC was introduced by Intel with the original Pentium CPU in 1993. The original TSC was very simple to use, it provided a counter that could be read by code being executed on the CPU. Located within the CPU. When an Intel Pentium computer is powered up the TSC register is set to zero. The value in the TSC register was incremented by 1 for every CPU ClockCycle. If the CPU ClockCycle was 60MHz then the TSC would also be incremented at a rate of 60MHz. Any code running on the system could read this register to determine what the current value was. Because both the CPU frequency and the TSC frequency were fixed this type of counter was known as a Monotonic Counter. The TSC is often preferred over many of the modern alternative Clock Sources because it is the least computationally expensive to use. For a running process (application) to query the current value of the TSC, it only requires a single X86 instruction call. Retrieving the TSC counter value in this this CPU register can be achieved by calling either of the following nonprivileged x86 instructions rdtsc/rdtscp. Interestingly; these two x86 instructions rdtsc/rdtscp can be called from any privilege level and they do not require a CPU ring transition. Over time, the reliability of the Time Stamp Counter (TSC) has been compromised on numerous occasions, necessitating the implementation of various hardware, firmware, and software solutions to ensure its dependability. Other clocksources with more advanced features have become available, but the TSC has remained as the ubiquitous clocksource. Reference time / Wall Clock time A high precision Reference clock is used to keep track of the agreed-upon time standard (such as UTC) Used to synchronise and calibrate other clocks. Measured in the time units that us humans use Hours, Minutes seconds Micro seconds, etc...for example 12:00 am, Jan 1st, 1970 System Time The time displayed when you run the date command, or view the GUI desktop clock. A clock / Time Line maintained in software. System time is initialised and set during system boot up from the Real Time Clock. System time is often kept in sync with the WallClockTime /reference time by using an NTP client to poll a reference NTP time source. When a System is powered off, the OS is not running therefore there is no SystemTime. Maintaining System Time on a single core fixed frequency x86 system Below is a simplified timeline of what happens when an Intel Pentium I single-core fixed Clock Frequency x86 computer is powered up and running a standard Linux Kernel 2.6. RTC continues working independently of the state of the computer. (Battery backed) CPU is powered on and initialised. The ClockSource Counters built into the CPU are initialized to Zero. The OS is booted. The OS System Time is set to the WallClock time based on the value in the RTC. (Generally, the RealTime clock is not read again until the next boot) The OS queries to see what ClockSources are available. The OS determines which ClockSource to use, based on manual configuration overrides and or the calculated stability of the available ClockSources. For simplicity's sake let's say that the OS has decided to use the TSC (Time Stamp Counter) as the ClockSource. The OS determines the ratio of the ClockSource frequency vs the frequency of the CPU Clock Cycles. ( on this computer is 1:1 ) For simplicity's sake let's assume that the CPU Clock Frequency is 10Hz and the ClockSource Counter frequency is also 10Hz. The OS takes note of the value of the ClockSource counter and the System WallClock time. The Computer has been powered on for 10 seconds, the ClouckSource counter = 10 (seconds) x 10hz = 100 The SystemTime & WallClock time is 12:00:10 Jan 1 2016 The OS runs the various process and applications. The OS-maintained SystemTime can now be kept in sync with the estimated WallClock time by moving it forward by an offset of the value maintained by the ClockSource Counter. For simplicity's sake let's say that the OS has been busy running processes and applications and the SystemTime has not been updated for 10 seconds in WallClock time. The OS queries the ClockSource Counter (which continues independently of CPU load) and notices that the counter has increased from 100 to 200. The OS calculates the current SystemTime by... TSC Counter difference (current) 200 - (previous) 100 = 100 The OS knows that the TSC frequency is 10Hz 100 / 10Hz = 10 seconds (previous) SystemTime + 10 seconds = 12:00:10 Jan 1 2016 There are 2 major problems with this method of maintaining system time. There is no way to determine the initial accuracy of the RTC's approximation of the True Wall Clock time.. You cannot determine what time zone it is set to. You cannot determine if it is even set to the correct year. There is no way to validate the accuracy or the stability of the ClockSource.. Is it exactly 10hz or is it 10.01hz ? Is it varying between 9hz and 11hz ? NTP solves problem [1] by polling a remote time source across the network to get an accurate reading for the True WallClock time in reference to the prime epoch. NTP goes a great way to mitigate problem [2] by comparing the apparent time drift between the multiple values returned by the local ClockSource with multiple values returned by the more accurate remote NTP time source. If it is determined that the local ClockSource is drifting from its stated speed of 10Hz, then the local NTP daemon calculates the drift offset and adjusts the system clock. Advancements of x86 CPU's and the effects upon the MonoTonic TSC Brief overview of the advances in X86 system architecture that affected the previously used method of using a simple monotonic counter (TSC) to maintain time. For most purposes, the methodology for maintaining system time described above (Intel Pentium I - III) worked fairly well. However times have changed and as x86 systems have evolved they have become far more complicated, as a result, the old method of determining the WallClock time needed to be changed to work on modern CPUs. The following 3 advances in X86 CPUs nullified the previously used method of using a simple monotonic counter (TSC) to maintain time. x86 systems evolved from having a single CPU core to multiple CPU cores. x86 CPUs could change Clock frequency whilst a system was running, to both save power and turbo boost. x86 CPU cores could change power states whilst a system was running, Individual CPU cores could be turned off and then be turned back on. A more detailed overview of the advances in X86 system architecture that affected the previously used method of using a simple monotonic counter (TSC) to maintain time. Below I have detailed 5 points in the evolution of X86 CPUs that I believe had the most effect on system timekeeping. Single CPU fixed clock frequency. (tsc) Historically back when x86 CPU frequencies were fixed, the TSC incremented in synchronization with the CPU cycles and everything was fine. SMP (symmetric multi-processing) fixed clock frequency. (tsc) When SMP (symmetric multi-processing) x86-based systems were released, each CPU had its own TSC and there was no mechanism to keep them in sync. Having separate TSC's on the same x86 system that could be out of sync with each other sometimes caused issues when applications/processes were moved between CPUs + TSC's, as the time could appear to jump either forwards or backward. As a fix for the out-of-sync TSCs on SMP systems, the TSCs were kept in sync by the BIOS/firmware during CPU resets and the situation was better again. SMP (symmetric multi-processing) variable clock frequency (SpeedStep). (constant_tsc) When x86 CPUs were released with new features such as variable clock speed once again the multiple TSC's within a single x86 system were out of sync. To solve this issue the variable clock/tick/rate/frequency of the CPU, the TSC frequency was Decoupled from the CPU clock frequencies. The TSCs would be incremented at a constant rate regardless of the variable CPU frequency. To communicate to the OS that the TSCs are in sync the CPU cpuid bit (flag) constant_tsc is presented to the OS. SMP (symmetric multi-processing), variable clock frequency, and Power States (ACPI P-, C-. and T-states, etc..) (nonstop_tsc) Now individual CPU cores can change their clock frequency independently of the other cores and even the uncore can be shutdown. When the uncore is shutdown the associated TSC is frozen and is now out of sync with the other TSC's To solve the issue of the TSC being shutdown with the CPUs and then started again, Intel introduced the \"nonstop_tsc\" Sleep x86 (nonstop_tsc_s3) x86 systems can be put to sleep where all system context is lost except system memory. So we now have 4 TSC-related CPUID flags for the different TSC feature sets. tsc : TSC is present constant_tsc: TSC ticks at a constant rate nonstop_tsc: TSC does not stop in C states nonstop_tsc_s3: TSC doesn't stop in S3 state Plus 1 more TSC CPUID flag that is only set if the kernel determines that the TSC is reliable. tsc_reliable: TSC is known to be reliable CPU Bugs and Clock Sources There have been a few different CPU, Firmware, and Operating System bugs related to CPU clock sources. One example is an issue with the Intel Xeon E5 v2 CPUs which results in the TSC not being cleared upon a CPU warm reset. This can result in the various TSC's on a system getting out of sync with each other. The bug is referenced in the following Intel Xeon E5 v2 spec update as \"CA105 TSC is Not Affected by warm reset\" http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/xeon-e5-v2-spec-update.pdf If a CPUs with this bug are warm reset or the uncores are powered down, the multiple TSC's within a system can get out of sync with each other. This problem can be mitigated by motherboard firmware/BIOS keeping the various TSC's in sync. If the firmware does not reset/sync the TSCs, then they may stay out of sync until a cold reset (power off/power on)is performed. The Linux Operating system running detect if the TSC's are out of sync and flag the TSC clocksource/s as being unstable. If this occurs, the Linux Operating system should then failover and utilise one of the alternative clock sources and continue functioning normally. Unfortunately, certain kernel builds in certain Linux distributions do not successfully fail over to alternate clock sources. For example, kernel-2.6.32-504.30.3.el6.x86_64, continues to use the unstable TSC as a clock source, doesnt fail over to a stable source, and can result in timewarping of the Syste, Time so severere that it may drift too much to be corrected by NTP. TimeLines, OffSets and Clock Synchronization \"A man with a watch knows what time it is. A man with two watches is never sure.\" Segal's Law If we have a Wrist Watch, we can be reasonably sure of what the time is. ,--.-----.--. |-----------| +--+ +--+ | +-----+ | __+--+ +--+__ / | +-----+ | \\ / \\__|-----|__/ \\ / _______________ \\/\\ / / \\ \\/ { / _ _ _ \\ } | { | | . | | | | } |-, | | |_| . |_| |_| | | | | { } |-' { \\ / } \\ `---------------' /\\ \\ __|-----|__ /\\/ \\ / +-----+ \\ / \\ |--+ +--| / --+ +-----+ +-- +--+ +--+ |-----------| `--'-----`--' Wrist Watch 1 Wrist Watch Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| Problems with using 1 Wrist Watch to determine the time. There is no way that we can confirm that the Wrist Watch is currently displaying the correct time. It could be correct, it could be 1 second behind, it could be 1 hour behind. There is no way that we can determine if the Wrist Watch is drifting too fast or too slow. There is no way that we can determine if the Wrist Watch is wandering between being too fast and too slow. Problem (1) Solution (a)... We can purchase a 2nd Wrist Watch. We can now compare the time on Wrist Watch 1 and Wrist Watch 2. If they are different, we can approximate the time by averaging the times on both of the watches. If the time between the 2 watches differs, there is no way to determine if there is a problem with either watch or both watches. If you have 2 Wrist Watches and they both display the same time you can you be even more sure of what the time is ? ,--.-----.--. ,--.-----.--. |-----------| |-----------| +--+ +--+ +--+ +--+ | +-----+ | | +-----+ | __+--+ +--+__ __+--+ +--+__ / | +-----+ | \\ / | +-----+ | \\ / \\__|-----|__/ \\ / \\__|-----|__/ \\ / _______________ \\/\\ / _______________ \\/\\ / / \\ \\/ / / \\ \\/ { / _ _ _ \\ } { / _ _ _ \\ } | { | | . | | | | } |-, | { | | . | | | | } |-, | | |_| . |_| |_| | | | | | |_| . |_| |_| | | | | { } |-' | { } |-' { \\ / } { \\ / } \\ `---------------' /\\ \\ `---------------' /\\ \\ __|-----|__ /\\/ \\ __|-----|__ /\\/ \\ / +-----+ \\ / \\ / +-----+ \\ / \\ |--+ +--| / \\ |--+ +--| / --+ +-----+ +-- --+ +-----+ +-- +--+ +--+ +--+ +--+ |-----------| |-----------| `--'-----`--' `--'-----`--' Wrist Watch 1 Wrist Watch 2 Wrist Watch 1 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| Wrist Watch 2 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| If you have 2 Wrist Watches and they both display different times you can never be sure of what the time is. In the example below the time on Wrist Watch 2 is 5 minutes ahead of Wrist Watch 1... ,--.-----.--. ,--.-----.--. |-----------| |-----------| +--+ +--+ +--+ +--+ | +-----+ | | +-----+ | __+--+ +--+__ __+--+ +--+__ / | +-----+ | \\ / | +-----+ | \\ / \\__|-----|__/ \\ / \\__|-----|__/ \\ / _______________ \\/\\ / _______________ \\/\\ / / \\ \\/ / / \\ \\/ { / _ _ _ \\ } { / _ _ _ \\ } | { | | . | | | | } |-, | { | | . | | |_ } |-, | | |_| . |_| |_| | | | | | |_| . |_| _| | | | | { } |-' | { } |-' { \\ / } { \\ / } \\ `---------------' /\\ \\ `---------------' /\\ \\ __|-----|__ /\\/ \\ __|-----|__ /\\/ \\ / +-----+ \\ / \\ / +-----+ \\ / \\ |--+ +--| / \\ |--+ +--| / --+ +-----+ +-- --+ +-----+ +-- +--+ +--+ +--+ +--+ |-----------| |-----------| `--'-----`--' `--'-----`--' Wrist Watch 1 Wrist Watch 2 Wrist Watch 1 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . Wrist Watch 2 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| With just 2 Wrist Watches it is impossible to determine if the time is correct on either of the Wrist Watches. Side Note... Describe atomic visibility, you can only view one of the timelines in a single action, Therefore to view both time lines you need to perform 2 actions. Examples of Clock Skew and Drift Watch 2 is running too fast Wrist Watch 2 FASTER than Wrist Watch 1 0 5 10 15 20 25 30 Wrist Watch 1 Time Line |----|----|----|----|----|----| . . . . . . . . . 0 5 10 15 20 25 30 Wrist Watch 2 Time Line |---|---|---|---|---|---| Watch 2 is running too slow Wrist Watch 2 SLOWER than Wrist Watch 1 0 5 10 15 20 25 30 Wrist Watch 1 Time Line |----|----|----|----|----|----| . . . . . . . . . . . . Wrist Watch 2 Time Line |-------|-------|-------|-------|-------|-------| 0 5 10 15 20 25 30 Watch 2 is unstable, sometimes running too fast and sometimes running too slow Wrist Watch 2 initially in sync with Wrist Watch 1 Wrist Watch 2 SLOWER than Wrist Watch 1 (out of sync) Wrist Watch 2 FASTER than Wrist Watch 1 (out of sync) Wrist Watch 2 in sync with Wrist Watch 1 0 5 10 15 20 25 30 Wrist Watch 1 Time Line |----|----|----|----|----|----| . . . . . . . . . 0 5 10 15 20 25 30 Wrist Watch 2 Time Line |------|-------|-----|--|--|--| #ascii-art { font-family: monospace; white-space: pre; font-size: 75%; } const frames = [ ` 1 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . . . . . . . . . . |----|----|----|----|----|----| 0 5 10 15 20 25 30 `, ` 2 0 5 10 15 20 25 30 *----|----|----|----|----|----| . . . . . . . . . . . . *----|----|----|----|----|----| 0 5 10 15 20 25 30 `, ` 3 0 5 10 15 20 25 30 |-*--|----|----|----|----|----| . . . . . . . . . . . . |-*---|---|----|----|----|----| 0 5 10 15 20 25 30 `, ` 4 0 5 10 15 20 25 30 |---*|----|----|----|----|----| . . . . . . . . . . . . |---*--|--|----|----|----|----| 0 5 10 15 20 25 30 `, ` 5 0 5 10 15 20 25 30 |----|*---|----|----|----|----| . . . . . . . . . . . . |-----*-|-|----|----|----|----| 0 5 10 15 20 25 30 `, ` 6 0 5 10 15 20 25 30 |----|---*|----|----|----|----| . . . . . . . . . . . . |-------|*-|---|----|----|----| 0 5 10 15 20 25 30 `, ` 7 0 5 10 15 20 25 30 |----|----|-*--|----|----|----| . . . . . . . . . . . . |-------|---*---|---|----|----| 0 5 10 15 20 25 30 `, ` 8 0 5 10 15 20 25 30 |----|----|----*----|----|----| . . . . . . . . . . . . |-------|----|-*-|---|----|---| 0 5 10 15 20 25 30 `, ` 9 0 5 10 15 20 25 30 |----|----|----|--*-|----|----| . . . . . . . . . . . . |-------|----|---|*--|----|---| 0 5 10 15 20 25 30 `, ` 10 0 5 10 15 20 25 30 |----|----|----|----|*---|----| . . . . . . . . . Slow . . . |-------|-----|---|--*|---|---| 0 5 10 15 20 25 30 `, ` 11 0 5 10 15 20 25 30 |----|----|----|----|---*|----| . . . . . . . . . Slow . . . |-------|-----|---|---|-*-|---| 0 5 10 15 20 25 30 `, ` 12 0 5 10 15 20 25 30 |----|----|----|----|----|*---| . . . . . . Same . . . Slow . Rate . . |-------|-----|----|----|-*|--| 0 5 10 15 20 25 30 `, ` 13 0 5 10 15 20 25 30 |----|----|----|----|----|---*| . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|-*| 0 5 10 15 20 25 30 `, ` 14 0 5 10 15 20 25 30 |----|----|----|----|----|----* . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|--* 0 5 10 15 20 25 30 `, ` 15 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|--| 0 5 10 15 20 25 30 `, ` 16 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|--| 0 5 10 15 20 25 30 ` ]; let currentFrameIndex = 0; const asciiArtElement = document.getElementById(\"ascii-art\"); function renderFrame() { asciiArtElement.textContent = frames[currentFrameIndex]; currentFrameIndex = (currentFrameIndex + 1) % frames.length; } // Initial render renderFrame(); // Start animation at 2 frames per second (500ms delay) setInterval(renderFrame, 1000); Problem (1) (2) and (3) Solution (b)... We can gain access to an accurate time source. We can synchronise the Wrist Watch 1 to the accurate time source. We can repeatedly compare the time displayed by the Wrist Watch 1 with the accurate time source. If the Wrist Watch 1 repeatedly shows the same time as the accurate time source, then the accuracy and reliability of the Wrist Watch can be confirmed. If the Wrist Watch 1 is seen to consistently drift either faster or slower than the accurate time source. Then we can compensate for the drift by regularly adjusting the time on the Wrist Watch 1. e.g. every minute we could move the time on the Wrist Watch 1 second forward or 1 second backward. Problem 3 Solution (b+)... We can detect if the Wrist Watch 1 is wandering between too fast and too slow, by repeatedly comparing the Wrist Watch time to the accurate time source. Calculating the RoundTrip Delay and Clock OffSet between 2 remote Clock Sources Definitions of terms.. Reference Time (WallClockTime), is the term used to describe the universally agreed upon time. System Time (Linux Computer) same units in Wall Clock time, maintained in software, initialised during boot up from the Real Time Clock. Scenario 1... Using the time read from a Wristwatch to set Linux SystemTime Definition of SystemTime: Similar to Wall Clock time. It is maintained in software. Initialised during boot up from the Real Time Clock. Often kept in sync with WallClockTime with the aid of NTP. Let's say that we manually set the Linux system time using a value read from our Wrist Watch time. The steps would be: Read time from Wrist Watch. Type the \"date\" command and the \"Wrist Watch time\" on the command prompt, and press enter. Code is executed. System Time is set. The problem, there is a delay between step 1 and step 4. Therefore both time lines (Wrist Watch and SystemTime will never be exactly in Sync. WristWatch time = 10 SystemTime = 15 OffSet = 05 Problem 1, Offset between the time taken to read the Wrist Watch Time and set the SystemTime. Problem 2, No way to determine the accuracy of the Wrist Watch time, maybe the SystemTime was closer to the real time? Scenario 2... Purchase an Accurate clock to use as a TimeLine reference Problem 1, Offset between the time taken to read the Accurate clock Time and set the SystemTime. Problem 2, Expensive, an accurate clock costs money. Scenario 3... Use a shared Accurate clock as a TimeLine reference Problem 1, Delay between the time taken to read the shared Accurate clock Time and set the SystemTime. Description of Problem 1... Let's say that you have a friend working at a science laboratory who has access to a very accurate clock. You could make a phone call to the friend and ask what the time is. ____ ____ / 12 \\ / 12 \\ / \\ / \\ ( 9 +->3) /+------------------------+/ ( 9 +->3) \\ \\ / / / \\ \\ / \\__6_/ /~~~~~~~/ / \\__6_/ / / /+\\ ,'\"\". / /++-----------------------+/ | \\ ,'\"\". ) ,+ / | | \\ ) ,+ / /,'+. / | | \\ / /,'+. / // /.`. | | \\ / // /.`. ,' // / `.`. | | \\ ,' // / `.`. ( )++.`. //+ | | \\ ( )++.`. //+ +`++'+ `.`.// + + + \\+`++'+ `.`.// + `++' `./ / `++' `./ / | + / | + / +_________+/ +_________+/ There are many problems with this solution including the following... The delay for the person on the other end to read the clock and to say the time. The delay for the electrical signal to travel through the long distance phone line. The delay between when you have heard the complete time and the time taken to set the time. Assuming that the current time is read out when we press a button on the phone. How can we determine the Round Trip Delay between when we press the button on the phone and when we receive the voice recording of the time? How can we determine the delay between when the remote system receives our query and when it sends the response? NTP Client to NTP Server Polling Mechanism NTP Packet Format +--------------------------------------------+ | LI | VN | Mode | Stratum | Poll |Precision | +--------------------------------------------+ | Root Delay | +--------------------------------------------+ | Root Dispersion | +--------------------------------------------+ | Reference Identifier | +--------------------------------------------+ | | | Reference TimeStamp | | | +--------------------------------------------+ | | | Origin TimeStamp | | | +--------------------------------------------+ | | | Recieve TimeStamp | | | +--------------------------------------------+ | | | Transmit TimeStamp | | | +--------------------------------------------+ | | | Optional Fields / Digest | | | +--------------------------------------------+ LI (Leap Indicator) 2-bits VN (NTP Version Number) 3-bits Mode (Work Mode) 3-bits code that indicates the work mode, can be set to either of these values: 0 - reserved 1 - symmetric active 2 - symmetric passive 3 - client 4 - server 5 - broadcast or multicast 6 - NTP control message 7 - reserved for private use. Stratum (Statum Level) 8-bits. An integer ranging from 1 to 16. (Stratum 1 clock has the highest precision, and a stratum 16 clock is not synchronized and cannot be used as a reference clock. Poll ( Poll Interval ) - 8-bits maximum interval between successive messages. Precision (precision of the local clock) - 8-bit signed integer. Root Delay - Roundtrip delay to the primary reference source. Root Dispersion - The maximum error of the local clock relative to the primary reference source. Reference Identifier - Identifier of the particular reference source. Reference Timestamp - Local time at which the local clock was last set or corrected. Originate Timestamp - Local time at which the request departed from the client for the service host. Receive Timestamp - Local time at which the request arrived at the service host. Transmit Timestamp - Local time at which the reply departed from the service host for the client. Authenticator - Authentication information. How an NTP Client synchronises with an NTP server Various delays and offsets need to be taken into consideration when an NTP client is determining the correct time. T1 the client timestamp on the request packet T2 the server timestamp upon arrival T3 the server timestamp on the departure of the reply packet T4 the client timestamp upon arrival #ascii-art { font-family: monospace; white-space: pre; font-size: 60%; } const frames = [ ` NTP CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | T1 | | | | T1 | | | | | T2 | <--------------------------<-| T2 | <--+ | | | T3 | | | | T3 | | | | T4 | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | | | | | | | | | | ---------------------------->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | | | | | | ---------------------------->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | | | | | | ->------>------->------>---->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ->------>------->------>---->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <<-+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | T1 | | | | | | <----------------------------| T2 | <<-+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | T1 | | | | | | <----------------------------| T2 | <--+ | | | | | | | T3 | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | T1 | | | | | | <------<------<------<-----<-| T2 | <--+ | | | | | | | T3 | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | T1 | | | | T1 | | | | | T2 | <------<------<------<-----<-| T2 | <--+ | | | T3 | | | | T3 | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | T1 | | | | T1 | | | | | T2 | <--------------------------<-| T2 | <--+ | | | T3 | | | | T3 | | | | T4 | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ ` ]; let currentFrameIndex = 0; const asciiArtElement = document.getElementById(\"ascii-art\"); function renderFrame() { asciiArtElement.textContent = frames[currentFrameIndex]; currentFrameIndex = (currentFrameIndex + 1) % frames.length; } // Initial render renderFrame(); // Start animation at 2 frames per second (500ms delay) setInterval(renderFrame, 2000); Formulas for calculating the RTD and OffSet Round Trip Delay = (T4 - T1) - (T3 - T2) Clock OffSet = [(T2 - T1) + (T3 - T4)] / 2 3 example calculations for RTD and OffSet. Example 1 +--------------+ RTD = ( T4 + T1 ) + ( T3 + T2 ) | T1 0 | ( 14 + 0 ) | ( 12 + 2 ) | T2 2 | ( 14 ) + ( 10 ) | T3 12 | 4 | T4 14 | | | | | OffSet = [( T2 + T1 ) + ( T3 + T4 )] /2 | | [( 2 + 0 ) | ( 12 | 14 )] /2 | | [( 2 ) + ( +2 )] /2 | RTD = 4 | [ 0 ] /2 | OffSet = 0 | 0 +--------------+ Example 2 +--------------+ RTD = ( T4 + T1 ) + ( T3 + T2 ) | | ( 10 + 0 ) | ( 6 + 4 ) | T1 0 | ( 10 ) + ( 2 ) | T2 4 | 8 | T3 6 | | T4 10 | | | OffSet = [( T2 + T1 ) + ( T3 + T4 )] /2 | | [( 4 + 0 ) | ( 6 | 10 )] /2 | | [( 4 ) + ( +4 )] /2 | RTD = 8 | [ 0 ] /2 | OffSet = 0 | 0 +--------------+ Example 3 +---------------+ RTD = ( T4 + T1 ) + ( T3 + T2 ) | | ( 24 + 10 ) | ( 12 + 2 ) | T1 10 | ( 14 ) + ( 10 ) | T2 2 | 4 | T3 12 | | T4 24 | | | OffSet = [( T2 + T1 ) + ( T3 + T4 )] /2 | | [( 2 | 10 ) | ( 12 | 24 )] /2 | | [( +8 ) | ( +12 )] /2 | RTD = 4 | [ |20 ] /2 | OffSet = +10 | +10 +---------------+ .container { float: left; width: 50%; } .form-container { border: 2px solid #ccc; padding: 20px; margin-bottom: 20px; } .form-container h3 { margin-top: 0; } input[type=\"text\"] { width: 100px; } Round Trip Delay and Offset Calculator Enter the values for T1, T2, T3, and T4 (in the format hh:mm:ss): NTP Client T1: T4: NTP Server T2: T3: Calculate function calculate() { // Getting input values for ntp Client var T1 = document.getElementById(\"T1\").value; var T4 = document.getElementById(\"T4\").value; // Getting input values for ntp Server var T2 = document.getElementById(\"T2\").value; var T3 = document.getElementById(\"T3\").value; // Validation check for T3 being after T2 if (timeToSeconds(T3) <= timeToSeconds(T2)) { alert(\"Time travel is not possible. Please ensure that T3 is after T2.\"); return; } // Calculating Round Trip Delay var roundTripDelay = (timeToSeconds(T4) - timeToSeconds(T1)) - (timeToSeconds(T3) - timeToSeconds(T2)); // Calculating Clock Offset var clockOffset = ((timeToSeconds(T2) - timeToSeconds(T1)) + (timeToSeconds(T3) - timeToSeconds(T4))) / 2; // Displaying results var resultDiv = document.getElementById(\"results\"); // Validation check for roundTripDelay if (roundTripDelay <= 0) { alert(\"The timestamp for the NTP packet receive time is earlier than the send time. The local clock is unstable.\"); return; } resultDiv.innerHTML = \"<h3>Results:</h3>\" + \"<p>Round Trip Delay: \" + roundTripDelay + \" seconds</p>\" + \"<p>Clock Offset: \" + clockOffset + \" seconds</p>\"; } function timeToSeconds(timeString) { var parts = timeString.split(\":\"); return (+parts[0]) * 3600 + (+parts[1]) * 60 + (+parts[2]); } Networks and NTP Precision and Accuracy The precision and accuracy of system time achievable and maintainable by an NTP (Network Time Protocol) client depend significantly on the network infrastructure between the NTP client and the NTP servers. * In a Local Area Network (LAN), the accuracy of NTP synchronization can typically reach the order of 100 microseconds (us). Within a Wide Area Network (WAN), the accuracy may decrease slightly, with synchronization achievable in the order of several tens of milliseconds. Over the Internet, the accuracy of NTP synchronization depends heavily on many different factors. Typically, selecting NTP servers closer to the client location rather than those farther away results in more precise time synchronization. However, it's important to note that accuracy in all these scenarios is significantly influenced by factors such as jitter and asymmetric network paths. Example C programs to poll the TSC, HPET,and ACPI_PM clock sources Example C Program to read the TSC #include <stdio.h> #include <stdint.h> int main() { unsigned int tsc_val_lo, tsc_val_hi; uint64_t timestamp; // Execute the rdtsc instruction __asm__ __volatile__ (\"rdtsc\" : \"=a\"(tsc_val_lo), \"=d\"(tsc_val_hi)); // Combine the low and high parts to form the full 64bit timestamp timestamp = ((uint64_t)tsc_val_lo) | (((uint64_t)tsc_val_hi) << 32); printf(\"TSC value: %lu\\n\", timestamp); return 0; } Example C program to read the HPET #include <stdio.h> #include <stdint.h> #include <fcntl.h> #include <sys/mman.h> #include <unistd.h> #define HPET_BASE_ADDRESS 0xFED00000 // Base address of HPET on most x86 machine #define HPET_SIZE 0x400 // HPET size in bytes int main() { int hpet_fd; volatile void *hpet_base; uint64_t hpet_counter; // Open the /dev/mem device to access physical memory hpet_fd = open(\"/dev/mem\", O_RDONLY); if (hpet_fd < 0) { perror(\"Error opening /dev/mem\"); return 1; } // Memory-map the HPET region hpet_base = mmap(NULL, HPET_SIZE, PROT_READ, MAP_SHARED, hpet_fd, HPET_BASE_ADDRESS); if (hpet_base == MAP_FAILED) { perror(\"Error mapping HPET region\"); close(hpet_fd); return 1; } // Read the HPET counter value hpet_counter = *((volatile uint64_t *)(hpet_base + 0xFEE)); // Unmap the HPET region and close /dev/mem munmap((void *)hpet_base, HPET_SIZE); close(hpet_fd); printf(\"HPET Counter: %lu\\n\", hpet_counter); return 0; } Example C program to read the ACPI_PM #include <stdio.h> #include <stdint.h> #include <fcntl.h> #include <sys/io.h> #include <unistd.h> #define PM_BASE_ADDRESS 0x40 // Base address of ACPI PM timer on most x86 machine int main() { int pm_fd; uint32_t pm_counter; // Open /dev/port to access I/O ports pm_fd = open(\"/dev/port\", O_RDONLY); if (pm_fd < 0) { perror(\"Error opening /dev/port\"); return 1; } // Enable I/O port access if (iopl(3) < 0) { perror(\"Error enabling I/O port access\"); close(pm_fd); return 1; } // Read the ACPI PM timer counter value outl(PM_BASE_ADDRESS, 0x43); // Select counter 0 and latch its value pm_counter = inl(PM_BASE_ADDRESS); // Read the latched value // Close /dev/port close(pm_fd); printf(\"ACPI PM Timer Counter: %u\\n\", pm_counter); return 0; } Sources for this document: Intel CPU specifications and developer guides Intel 64-ia-32-architectures-software-developer-vol-2a-manual.pdf Intel 64-ia-32-architectures-software-developer-manual-325462.pdf Intel 64-ia-32-architectures-software-developer-vol-3b-part-2-manual.html Intel timestamp-counter-scaling-virtualization-white-paper.pdf Intel xeon-e5-v2-spec-update.pdf Linux Kernel and Linux Distribution documentation https://www.kernel.org/doc/Documentation/virtual/kvm/timekeeping.txt https://access.redhat.com/solutions/18627 kb.vmware.com RedHat - Virtualization-KVM_guest_timing_management https://bugzilla.redhat.com/show_bug.cgi?id=698842 http://blog.cr0.org/2009/05/time-stamp-counter-disabling-oddities.html http://www.cs.virginia.edu/~evans/cs216/guides/x86.html http://man7.org/linux/man-pages/man2/prctl.2.html https://www.chromium.org/chromium-os/how-tos-and-troubleshooting/tsc-resynchronization https://wiki.archlinux.org/index.php/CPU_frequency_scaling http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/ https://lkml.org/lkml/2005/11/4/173 [Manual page clock_gettime] NTP (Official Documentation) https://www.eecis.udel.edu/~mills/ntp/html/index.html https://www.amazon.com/Computer-Network-Time-Synchronization-Protocol/dp/0849358051/ https://www.amazon.com/Computer-Network-Time-Synchronization-Protocol/dp/1439814635/ http://what-if.xkcd.com/26/ Leap second (XKCD always counts as official) Wikipedia links Segals Law List_of_Intel_microprocessors P5 microarchitecture Intel SpeedStep Wall-clock_time Time_Stamp_Counter Linux_kernel Allan_variance ASCII Art http://www.ascii-code.com/ascii-art/electronics/clocks.php https://www.asciiart.eu/electronics/phones Extra Info Measuring Wander and Jitter","title":"System Timing and NTP"},{"location":"time/System_Timing_and_NTP/#purpose-of-this-document","text":"Act as a repository for separate but related system timing information.","title":"Purpose of this Document:"},{"location":"time/System_Timing_and_NTP/#index","text":"Definitions of the terms used in this document Description of the 4 main clock sources referred to in this document Maintaining System Time on a single core fixed frequency x86 system Advancements of x86 CPU's and the effects upon the MonoTonic TSC CPU Bugs and Clock Sources TimeLines, OffSets and Clock Synchronization Examples of Clock Drift Calculating the RoundTrip Delay and Clock OffSet between 2 Clock Sources NTP Client / NTP Server polling mechanism. NTP Precision and Accuracy Example C programs to poll the TSC, HPET,and ACPI_PM clock sources Sources for this document","title":"Index:"},{"location":"time/System_Timing_and_NTP/#definitions-of-the-terms-used-in-this-document","text":"Term Definition Wall Clock Time Time displayed in a format understandable to humans, such as \"12:00 am, Jan 1st, 1970.\" Real-Time Clock A hardware-based clock found on system motherboards, functions independent of a computer's power state. System Time A software-based clock or time line initialized during system boot from the real-time clock Reference Clock A highly precise and stable time source used to synchronize and calibrate other clocks. Clock Cycles A measure of the transitions in an electronic circuit, akin to the movement of a production line. Counter A device that increments based on specific events or time periods, independent of CPU-executed code. Timer A device that tracks the number of increments occurring between two points in time. Clock Source A device queried by CPU-executed code to obtain the current value of a counter. Monotonic Counter A counter that increments at regular intervals, such as once per second, analogous to a metronome. System A self-contained computing entity, such as a Linux server, Windows desktop, or Mac laptop. Time Line A graphical or numerical depiction of the passage of time. Time Scale A representation of time, either graphical or numerical, adjusted to illustrate drift or skew","title":"Definitions of the terms used in this document: "},{"location":"time/System_Timing_and_NTP/#description-of-the-4-main-clock-sources-referred-to-in-this-document","text":"","title":"Description of the 4 main clock sources referred to in this document"},{"location":"time/System_Timing_and_NTP/#rtc-real-time-clock","text":"A hardware device used to keep track of time even when a system is powered off. Similar to a battery-operated digital Wrist Watch. A quartz crystal is used as a stable oscillator (32Khz). When an x86 computer is powered off, the Real Time Clock on the x86 computer's motherboard is powered by a battery and is able to maintain an approximation of WallClock time. These RTCs tend to be fairly inaccurate, the amount by which they drift from the true WallClock time can depend on several factors such as the quality of the electronic components and temperature variations. When an x86 computer is powered off there is no System Time, Clock Cycles, Interrupts, Counters, or Timers.","title":"RTC (Real Time Clock)"},{"location":"time/System_Timing_and_NTP/#problems-with-rtc","text":"Subject to temperature changes. Subject to manufacturing tolerances. Modern CPU clock rates are much higher than the 32khz (clock) rate of RTC, therefor the resolution that they provide is not sufficiently high enough for modern CPUs. It is very computationally expensive and slow for the OS to poll the RTC.","title":"Problems with RTC"},{"location":"time/System_Timing_and_NTP/#tsc-time-stamp-counter","text":"The TSC was introduced by Intel with the original Pentium CPU in 1993. The original TSC was very simple to use, it provided a counter that could be read by code being executed on the CPU. Located within the CPU. When an Intel Pentium computer is powered up the TSC register is set to zero. The value in the TSC register was incremented by 1 for every CPU ClockCycle. If the CPU ClockCycle was 60MHz then the TSC would also be incremented at a rate of 60MHz. Any code running on the system could read this register to determine what the current value was. Because both the CPU frequency and the TSC frequency were fixed this type of counter was known as a Monotonic Counter. The TSC is often preferred over many of the modern alternative Clock Sources because it is the least computationally expensive to use. For a running process (application) to query the current value of the TSC, it only requires a single X86 instruction call. Retrieving the TSC counter value in this this CPU register can be achieved by calling either of the following nonprivileged x86 instructions rdtsc/rdtscp. Interestingly; these two x86 instructions rdtsc/rdtscp can be called from any privilege level and they do not require a CPU ring transition. Over time, the reliability of the Time Stamp Counter (TSC) has been compromised on numerous occasions, necessitating the implementation of various hardware, firmware, and software solutions to ensure its dependability. Other clocksources with more advanced features have become available, but the TSC has remained as the ubiquitous clocksource.","title":"TSC (Time Stamp Counter)"},{"location":"time/System_Timing_and_NTP/#reference-time-wall-clock-time","text":"A high precision Reference clock is used to keep track of the agreed-upon time standard (such as UTC) Used to synchronise and calibrate other clocks. Measured in the time units that us humans use Hours, Minutes seconds Micro seconds, etc...for example 12:00 am, Jan 1st, 1970","title":"Reference time / Wall Clock time"},{"location":"time/System_Timing_and_NTP/#system-time","text":"The time displayed when you run the date command, or view the GUI desktop clock. A clock / Time Line maintained in software. System time is initialised and set during system boot up from the Real Time Clock. System time is often kept in sync with the WallClockTime /reference time by using an NTP client to poll a reference NTP time source. When a System is powered off, the OS is not running therefore there is no SystemTime.","title":"System Time"},{"location":"time/System_Timing_and_NTP/#maintaining-system-time-on-a-single-core-fixed-frequency-x86-system","text":"Below is a simplified timeline of what happens when an Intel Pentium I single-core fixed Clock Frequency x86 computer is powered up and running a standard Linux Kernel 2.6. RTC continues working independently of the state of the computer. (Battery backed) CPU is powered on and initialised. The ClockSource Counters built into the CPU are initialized to Zero. The OS is booted. The OS System Time is set to the WallClock time based on the value in the RTC. (Generally, the RealTime clock is not read again until the next boot) The OS queries to see what ClockSources are available. The OS determines which ClockSource to use, based on manual configuration overrides and or the calculated stability of the available ClockSources. For simplicity's sake let's say that the OS has decided to use the TSC (Time Stamp Counter) as the ClockSource. The OS determines the ratio of the ClockSource frequency vs the frequency of the CPU Clock Cycles. ( on this computer is 1:1 ) For simplicity's sake let's assume that the CPU Clock Frequency is 10Hz and the ClockSource Counter frequency is also 10Hz. The OS takes note of the value of the ClockSource counter and the System WallClock time. The Computer has been powered on for 10 seconds, the ClouckSource counter = 10 (seconds) x 10hz = 100 The SystemTime & WallClock time is 12:00:10 Jan 1 2016 The OS runs the various process and applications. The OS-maintained SystemTime can now be kept in sync with the estimated WallClock time by moving it forward by an offset of the value maintained by the ClockSource Counter. For simplicity's sake let's say that the OS has been busy running processes and applications and the SystemTime has not been updated for 10 seconds in WallClock time. The OS queries the ClockSource Counter (which continues independently of CPU load) and notices that the counter has increased from 100 to 200. The OS calculates the current SystemTime by... TSC Counter difference (current) 200 - (previous) 100 = 100 The OS knows that the TSC frequency is 10Hz 100 / 10Hz = 10 seconds (previous) SystemTime + 10 seconds = 12:00:10 Jan 1 2016 There are 2 major problems with this method of maintaining system time. There is no way to determine the initial accuracy of the RTC's approximation of the True Wall Clock time.. You cannot determine what time zone it is set to. You cannot determine if it is even set to the correct year. There is no way to validate the accuracy or the stability of the ClockSource.. Is it exactly 10hz or is it 10.01hz ? Is it varying between 9hz and 11hz ? NTP solves problem [1] by polling a remote time source across the network to get an accurate reading for the True WallClock time in reference to the prime epoch. NTP goes a great way to mitigate problem [2] by comparing the apparent time drift between the multiple values returned by the local ClockSource with multiple values returned by the more accurate remote NTP time source. If it is determined that the local ClockSource is drifting from its stated speed of 10Hz, then the local NTP daemon calculates the drift offset and adjusts the system clock.","title":"Maintaining System Time on a single core fixed frequency x86 system  "},{"location":"time/System_Timing_and_NTP/#advancements-of-x86-cpus-and-the-effects-upon-the-monotonic-tsc","text":"","title":"Advancements of x86 CPU's and the effects upon the MonoTonic TSC "},{"location":"time/System_Timing_and_NTP/#brief-overview-of-the-advances-in-x86-system-architecture-that-affected-the-previously-used-method-of-using-a-simple-monotonic-counter-tsc-to-maintain-time","text":"For most purposes, the methodology for maintaining system time described above (Intel Pentium I - III) worked fairly well. However times have changed and as x86 systems have evolved they have become far more complicated, as a result, the old method of determining the WallClock time needed to be changed to work on modern CPUs. The following 3 advances in X86 CPUs nullified the previously used method of using a simple monotonic counter (TSC) to maintain time. x86 systems evolved from having a single CPU core to multiple CPU cores. x86 CPUs could change Clock frequency whilst a system was running, to both save power and turbo boost. x86 CPU cores could change power states whilst a system was running, Individual CPU cores could be turned off and then be turned back on.","title":"Brief overview of the advances in X86 system architecture that affected the previously used method of using a simple monotonic counter (TSC) to maintain time."},{"location":"time/System_Timing_and_NTP/#a-more-detailed-overview-of-the-advances-in-x86-system-architecture-that-affected-the-previously-used-method-of-using-a-simple-monotonic-counter-tsc-to-maintain-time","text":"Below I have detailed 5 points in the evolution of X86 CPUs that I believe had the most effect on system timekeeping. Single CPU fixed clock frequency. (tsc) Historically back when x86 CPU frequencies were fixed, the TSC incremented in synchronization with the CPU cycles and everything was fine. SMP (symmetric multi-processing) fixed clock frequency. (tsc) When SMP (symmetric multi-processing) x86-based systems were released, each CPU had its own TSC and there was no mechanism to keep them in sync. Having separate TSC's on the same x86 system that could be out of sync with each other sometimes caused issues when applications/processes were moved between CPUs + TSC's, as the time could appear to jump either forwards or backward. As a fix for the out-of-sync TSCs on SMP systems, the TSCs were kept in sync by the BIOS/firmware during CPU resets and the situation was better again. SMP (symmetric multi-processing) variable clock frequency (SpeedStep). (constant_tsc) When x86 CPUs were released with new features such as variable clock speed once again the multiple TSC's within a single x86 system were out of sync. To solve this issue the variable clock/tick/rate/frequency of the CPU, the TSC frequency was Decoupled from the CPU clock frequencies. The TSCs would be incremented at a constant rate regardless of the variable CPU frequency. To communicate to the OS that the TSCs are in sync the CPU cpuid bit (flag) constant_tsc is presented to the OS. SMP (symmetric multi-processing), variable clock frequency, and Power States (ACPI P-, C-. and T-states, etc..) (nonstop_tsc) Now individual CPU cores can change their clock frequency independently of the other cores and even the uncore can be shutdown. When the uncore is shutdown the associated TSC is frozen and is now out of sync with the other TSC's To solve the issue of the TSC being shutdown with the CPUs and then started again, Intel introduced the \"nonstop_tsc\" Sleep x86 (nonstop_tsc_s3) x86 systems can be put to sleep where all system context is lost except system memory.","title":"A more detailed overview of the advances in X86 system architecture that affected the previously used method of using a simple monotonic counter (TSC) to maintain time."},{"location":"time/System_Timing_and_NTP/#so-we-now-have-4-tsc-related-cpuid-flags-for-the-different-tsc-feature-sets","text":"tsc : TSC is present constant_tsc: TSC ticks at a constant rate nonstop_tsc: TSC does not stop in C states nonstop_tsc_s3: TSC doesn't stop in S3 state Plus 1 more TSC CPUID flag that is only set if the kernel determines that the TSC is reliable. tsc_reliable: TSC is known to be reliable","title":"So we now have 4 TSC-related CPUID flags for the different TSC feature sets."},{"location":"time/System_Timing_and_NTP/#cpu-bugs-and-clock-sources","text":"There have been a few different CPU, Firmware, and Operating System bugs related to CPU clock sources. One example is an issue with the Intel Xeon E5 v2 CPUs which results in the TSC not being cleared upon a CPU warm reset. This can result in the various TSC's on a system getting out of sync with each other. The bug is referenced in the following Intel Xeon E5 v2 spec update as \"CA105 TSC is Not Affected by warm reset\" http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/xeon-e5-v2-spec-update.pdf If a CPUs with this bug are warm reset or the uncores are powered down, the multiple TSC's within a system can get out of sync with each other. This problem can be mitigated by motherboard firmware/BIOS keeping the various TSC's in sync. If the firmware does not reset/sync the TSCs, then they may stay out of sync until a cold reset (power off/power on)is performed. The Linux Operating system running detect if the TSC's are out of sync and flag the TSC clocksource/s as being unstable. If this occurs, the Linux Operating system should then failover and utilise one of the alternative clock sources and continue functioning normally. Unfortunately, certain kernel builds in certain Linux distributions do not successfully fail over to alternate clock sources. For example, kernel-2.6.32-504.30.3.el6.x86_64, continues to use the unstable TSC as a clock source, doesnt fail over to a stable source, and can result in timewarping of the Syste, Time so severere that it may drift too much to be corrected by NTP.","title":"CPU Bugs and Clock Sources"},{"location":"time/System_Timing_and_NTP/#timelines-offsets-and-clock-synchronization","text":"\"A man with a watch knows what time it is. A man with two watches is never sure.\" Segal's Law If we have a Wrist Watch, we can be reasonably sure of what the time is. ,--.-----.--. |-----------| +--+ +--+ | +-----+ | __+--+ +--+__ / | +-----+ | \\ / \\__|-----|__/ \\ / _______________ \\/\\ / / \\ \\/ { / _ _ _ \\ } | { | | . | | | | } |-, | | |_| . |_| |_| | | | | { } |-' { \\ / } \\ `---------------' /\\ \\ __|-----|__ /\\/ \\ / +-----+ \\ / \\ |--+ +--| / --+ +-----+ +-- +--+ +--+ |-----------| `--'-----`--' Wrist Watch 1 Wrist Watch Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| Problems with using 1 Wrist Watch to determine the time. There is no way that we can confirm that the Wrist Watch is currently displaying the correct time. It could be correct, it could be 1 second behind, it could be 1 hour behind. There is no way that we can determine if the Wrist Watch is drifting too fast or too slow. There is no way that we can determine if the Wrist Watch is wandering between being too fast and too slow. Problem (1) Solution (a)... We can purchase a 2nd Wrist Watch. We can now compare the time on Wrist Watch 1 and Wrist Watch 2. If they are different, we can approximate the time by averaging the times on both of the watches. If the time between the 2 watches differs, there is no way to determine if there is a problem with either watch or both watches. If you have 2 Wrist Watches and they both display the same time you can you be even more sure of what the time is ? ,--.-----.--. ,--.-----.--. |-----------| |-----------| +--+ +--+ +--+ +--+ | +-----+ | | +-----+ | __+--+ +--+__ __+--+ +--+__ / | +-----+ | \\ / | +-----+ | \\ / \\__|-----|__/ \\ / \\__|-----|__/ \\ / _______________ \\/\\ / _______________ \\/\\ / / \\ \\/ / / \\ \\/ { / _ _ _ \\ } { / _ _ _ \\ } | { | | . | | | | } |-, | { | | . | | | | } |-, | | |_| . |_| |_| | | | | | |_| . |_| |_| | | | | { } |-' | { } |-' { \\ / } { \\ / } \\ `---------------' /\\ \\ `---------------' /\\ \\ __|-----|__ /\\/ \\ __|-----|__ /\\/ \\ / +-----+ \\ / \\ / +-----+ \\ / \\ |--+ +--| / \\ |--+ +--| / --+ +-----+ +-- --+ +-----+ +-- +--+ +--+ +--+ +--+ |-----------| |-----------| `--'-----`--' `--'-----`--' Wrist Watch 1 Wrist Watch 2 Wrist Watch 1 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| Wrist Watch 2 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| If you have 2 Wrist Watches and they both display different times you can never be sure of what the time is. In the example below the time on Wrist Watch 2 is 5 minutes ahead of Wrist Watch 1... ,--.-----.--. ,--.-----.--. |-----------| |-----------| +--+ +--+ +--+ +--+ | +-----+ | | +-----+ | __+--+ +--+__ __+--+ +--+__ / | +-----+ | \\ / | +-----+ | \\ / \\__|-----|__/ \\ / \\__|-----|__/ \\ / _______________ \\/\\ / _______________ \\/\\ / / \\ \\/ / / \\ \\/ { / _ _ _ \\ } { / _ _ _ \\ } | { | | . | | | | } |-, | { | | . | | |_ } |-, | | |_| . |_| |_| | | | | | |_| . |_| _| | | | | { } |-' | { } |-' { \\ / } { \\ / } \\ `---------------' /\\ \\ `---------------' /\\ \\ __|-----|__ /\\/ \\ __|-----|__ /\\/ \\ / +-----+ \\ / \\ / +-----+ \\ / \\ |--+ +--| / \\ |--+ +--| / --+ +-----+ +-- --+ +-----+ +-- +--+ +--+ +--+ +--+ |-----------| |-----------| `--'-----`--' `--'-----`--' Wrist Watch 1 Wrist Watch 2 Wrist Watch 1 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . Wrist Watch 2 Time Line 0 5 10 15 20 25 30 |----|----|----|----|----|----| With just 2 Wrist Watches it is impossible to determine if the time is correct on either of the Wrist Watches. Side Note... Describe atomic visibility, you can only view one of the timelines in a single action, Therefore to view both time lines you need to perform 2 actions.","title":"TimeLines, OffSets and Clock Synchronization"},{"location":"time/System_Timing_and_NTP/#examples-of-clock-skew-and-drift","text":"","title":"Examples of Clock Skew and Drift"},{"location":"time/System_Timing_and_NTP/#watch-2-is-running-too-fast","text":"Wrist Watch 2 FASTER than Wrist Watch 1 0 5 10 15 20 25 30 Wrist Watch 1 Time Line |----|----|----|----|----|----| . . . . . . . . . 0 5 10 15 20 25 30 Wrist Watch 2 Time Line |---|---|---|---|---|---|","title":"Watch 2 is running too fast"},{"location":"time/System_Timing_and_NTP/#watch-2-is-running-too-slow","text":"Wrist Watch 2 SLOWER than Wrist Watch 1 0 5 10 15 20 25 30 Wrist Watch 1 Time Line |----|----|----|----|----|----| . . . . . . . . . . . . Wrist Watch 2 Time Line |-------|-------|-------|-------|-------|-------| 0 5 10 15 20 25 30","title":"Watch 2 is running too slow"},{"location":"time/System_Timing_and_NTP/#watch-2-is-unstable-sometimes-running-too-fast-and-sometimes-running-too-slow","text":"Wrist Watch 2 initially in sync with Wrist Watch 1 Wrist Watch 2 SLOWER than Wrist Watch 1 (out of sync) Wrist Watch 2 FASTER than Wrist Watch 1 (out of sync) Wrist Watch 2 in sync with Wrist Watch 1 0 5 10 15 20 25 30 Wrist Watch 1 Time Line |----|----|----|----|----|----| . . . . . . . . . 0 5 10 15 20 25 30 Wrist Watch 2 Time Line |------|-------|-----|--|--|--| #ascii-art { font-family: monospace; white-space: pre; font-size: 75%; } const frames = [ ` 1 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . . . . . . . . . . |----|----|----|----|----|----| 0 5 10 15 20 25 30 `, ` 2 0 5 10 15 20 25 30 *----|----|----|----|----|----| . . . . . . . . . . . . *----|----|----|----|----|----| 0 5 10 15 20 25 30 `, ` 3 0 5 10 15 20 25 30 |-*--|----|----|----|----|----| . . . . . . . . . . . . |-*---|---|----|----|----|----| 0 5 10 15 20 25 30 `, ` 4 0 5 10 15 20 25 30 |---*|----|----|----|----|----| . . . . . . . . . . . . |---*--|--|----|----|----|----| 0 5 10 15 20 25 30 `, ` 5 0 5 10 15 20 25 30 |----|*---|----|----|----|----| . . . . . . . . . . . . |-----*-|-|----|----|----|----| 0 5 10 15 20 25 30 `, ` 6 0 5 10 15 20 25 30 |----|---*|----|----|----|----| . . . . . . . . . . . . |-------|*-|---|----|----|----| 0 5 10 15 20 25 30 `, ` 7 0 5 10 15 20 25 30 |----|----|-*--|----|----|----| . . . . . . . . . . . . |-------|---*---|---|----|----| 0 5 10 15 20 25 30 `, ` 8 0 5 10 15 20 25 30 |----|----|----*----|----|----| . . . . . . . . . . . . |-------|----|-*-|---|----|---| 0 5 10 15 20 25 30 `, ` 9 0 5 10 15 20 25 30 |----|----|----|--*-|----|----| . . . . . . . . . . . . |-------|----|---|*--|----|---| 0 5 10 15 20 25 30 `, ` 10 0 5 10 15 20 25 30 |----|----|----|----|*---|----| . . . . . . . . . Slow . . . |-------|-----|---|--*|---|---| 0 5 10 15 20 25 30 `, ` 11 0 5 10 15 20 25 30 |----|----|----|----|---*|----| . . . . . . . . . Slow . . . |-------|-----|---|---|-*-|---| 0 5 10 15 20 25 30 `, ` 12 0 5 10 15 20 25 30 |----|----|----|----|----|*---| . . . . . . Same . . . Slow . Rate . . |-------|-----|----|----|-*|--| 0 5 10 15 20 25 30 `, ` 13 0 5 10 15 20 25 30 |----|----|----|----|----|---*| . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|-*| 0 5 10 15 20 25 30 `, ` 14 0 5 10 15 20 25 30 |----|----|----|----|----|----* . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|--* 0 5 10 15 20 25 30 `, ` 15 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|--| 0 5 10 15 20 25 30 `, ` 16 0 5 10 15 20 25 30 |----|----|----|----|----|----| . . . . . . Same . . . Slow . Rate .Fast. |-------|-----|----|----|--|--| 0 5 10 15 20 25 30 ` ]; let currentFrameIndex = 0; const asciiArtElement = document.getElementById(\"ascii-art\"); function renderFrame() { asciiArtElement.textContent = frames[currentFrameIndex]; currentFrameIndex = (currentFrameIndex + 1) % frames.length; } // Initial render renderFrame(); // Start animation at 2 frames per second (500ms delay) setInterval(renderFrame, 1000); Problem (1) (2) and (3) Solution (b)... We can gain access to an accurate time source. We can synchronise the Wrist Watch 1 to the accurate time source. We can repeatedly compare the time displayed by the Wrist Watch 1 with the accurate time source. If the Wrist Watch 1 repeatedly shows the same time as the accurate time source, then the accuracy and reliability of the Wrist Watch can be confirmed. If the Wrist Watch 1 is seen to consistently drift either faster or slower than the accurate time source. Then we can compensate for the drift by regularly adjusting the time on the Wrist Watch 1. e.g. every minute we could move the time on the Wrist Watch 1 second forward or 1 second backward. Problem 3 Solution (b+)... We can detect if the Wrist Watch 1 is wandering between too fast and too slow, by repeatedly comparing the Wrist Watch time to the accurate time source.","title":"Watch 2 is unstable, sometimes running too fast and sometimes running too slow"},{"location":"time/System_Timing_and_NTP/#calculating-the-roundtrip-delay-and-clock-offset-between-2-remote-clock-sources","text":"Definitions of terms.. Reference Time (WallClockTime), is the term used to describe the universally agreed upon time. System Time (Linux Computer) same units in Wall Clock time, maintained in software, initialised during boot up from the Real Time Clock.","title":"Calculating the RoundTrip Delay and Clock OffSet between 2 remote Clock Sources"},{"location":"time/System_Timing_and_NTP/#scenario-1-using-the-time-read-from-a-wristwatch-to-set-linux-systemtime","text":"Definition of SystemTime: Similar to Wall Clock time. It is maintained in software. Initialised during boot up from the Real Time Clock. Often kept in sync with WallClockTime with the aid of NTP. Let's say that we manually set the Linux system time using a value read from our Wrist Watch time. The steps would be: Read time from Wrist Watch. Type the \"date\" command and the \"Wrist Watch time\" on the command prompt, and press enter. Code is executed. System Time is set. The problem, there is a delay between step 1 and step 4. Therefore both time lines (Wrist Watch and SystemTime will never be exactly in Sync. WristWatch time = 10 SystemTime = 15 OffSet = 05 Problem 1, Offset between the time taken to read the Wrist Watch Time and set the SystemTime. Problem 2, No way to determine the accuracy of the Wrist Watch time, maybe the SystemTime was closer to the real time?","title":"Scenario 1... Using the time read from a Wristwatch to set Linux SystemTime"},{"location":"time/System_Timing_and_NTP/#scenario-2-purchase-an-accurate-clock-to-use-as-a-timeline-reference","text":"Problem 1, Offset between the time taken to read the Accurate clock Time and set the SystemTime. Problem 2, Expensive, an accurate clock costs money.","title":"Scenario 2... Purchase an Accurate clock to use as a TimeLine reference"},{"location":"time/System_Timing_and_NTP/#scenario-3-use-a-shared-accurate-clock-as-a-timeline-reference","text":"Problem 1, Delay between the time taken to read the shared Accurate clock Time and set the SystemTime.","title":"Scenario 3... Use a shared Accurate clock as a TimeLine reference"},{"location":"time/System_Timing_and_NTP/#description-of-problem-1","text":"Let's say that you have a friend working at a science laboratory who has access to a very accurate clock. You could make a phone call to the friend and ask what the time is. ____ ____ / 12 \\ / 12 \\ / \\ / \\ ( 9 +->3) /+------------------------+/ ( 9 +->3) \\ \\ / / / \\ \\ / \\__6_/ /~~~~~~~/ / \\__6_/ / / /+\\ ,'\"\". / /++-----------------------+/ | \\ ,'\"\". ) ,+ / | | \\ ) ,+ / /,'+. / | | \\ / /,'+. / // /.`. | | \\ / // /.`. ,' // / `.`. | | \\ ,' // / `.`. ( )++.`. //+ | | \\ ( )++.`. //+ +`++'+ `.`.// + + + \\+`++'+ `.`.// + `++' `./ / `++' `./ / | + / | + / +_________+/ +_________+/ There are many problems with this solution including the following... The delay for the person on the other end to read the clock and to say the time. The delay for the electrical signal to travel through the long distance phone line. The delay between when you have heard the complete time and the time taken to set the time. Assuming that the current time is read out when we press a button on the phone. How can we determine the Round Trip Delay between when we press the button on the phone and when we receive the voice recording of the time? How can we determine the delay between when the remote system receives our query and when it sends the response?","title":"Description of Problem 1..."},{"location":"time/System_Timing_and_NTP/#ntp-client-to-ntp-server-polling-mechanism","text":"","title":"NTP Client to NTP Server Polling Mechanism"},{"location":"time/System_Timing_and_NTP/#ntp-packet-format","text":"+--------------------------------------------+ | LI | VN | Mode | Stratum | Poll |Precision | +--------------------------------------------+ | Root Delay | +--------------------------------------------+ | Root Dispersion | +--------------------------------------------+ | Reference Identifier | +--------------------------------------------+ | | | Reference TimeStamp | | | +--------------------------------------------+ | | | Origin TimeStamp | | | +--------------------------------------------+ | | | Recieve TimeStamp | | | +--------------------------------------------+ | | | Transmit TimeStamp | | | +--------------------------------------------+ | | | Optional Fields / Digest | | | +--------------------------------------------+ LI (Leap Indicator) 2-bits VN (NTP Version Number) 3-bits Mode (Work Mode) 3-bits code that indicates the work mode, can be set to either of these values: 0 - reserved 1 - symmetric active 2 - symmetric passive 3 - client 4 - server 5 - broadcast or multicast 6 - NTP control message 7 - reserved for private use. Stratum (Statum Level) 8-bits. An integer ranging from 1 to 16. (Stratum 1 clock has the highest precision, and a stratum 16 clock is not synchronized and cannot be used as a reference clock. Poll ( Poll Interval ) - 8-bits maximum interval between successive messages. Precision (precision of the local clock) - 8-bit signed integer. Root Delay - Roundtrip delay to the primary reference source. Root Dispersion - The maximum error of the local clock relative to the primary reference source. Reference Identifier - Identifier of the particular reference source. Reference Timestamp - Local time at which the local clock was last set or corrected. Originate Timestamp - Local time at which the request departed from the client for the service host. Receive Timestamp - Local time at which the request arrived at the service host. Transmit Timestamp - Local time at which the reply departed from the service host for the client. Authenticator - Authentication information.","title":"NTP Packet Format"},{"location":"time/System_Timing_and_NTP/#how-an-ntp-client-synchronises-with-an-ntp-server","text":"Various delays and offsets need to be taken into consideration when an NTP client is determining the correct time. T1 the client timestamp on the request packet T2 the server timestamp upon arrival T3 the server timestamp on the departure of the reply packet T4 the client timestamp upon arrival #ascii-art { font-family: monospace; white-space: pre; font-size: 60%; } const frames = [ ` NTP CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | T1 | | | | T1 | | | | | T2 | <--------------------------<-| T2 | <--+ | | | T3 | | | | T3 | | | | T4 | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | | | | | | | | | | ---------------------------->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | | | | | | ---------------------------->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | | | | | | ->------>------->------>---->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ->------>------->------>---->| | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <--+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | | <----------------------------| | <<-+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | T1 | | | | | | <----------------------------| T2 | <<-+ | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP_SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | T1 | | | | | | <----------------------------| T2 | <--+ | | | | | | | T3 | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | T1 | | | | | | <------<------<------<-----<-| T2 | <--+ | | | | | | | T3 | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | T1 | | | | T1 | | | | | T2 | <------<------<------<-----<-| T2 | <--+ | | | T3 | | | | T3 | | | | | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ `, ` NTP_CLIENT NTP SERVER +---------------+ +----------------+ | | | | | +----+ | | +----+ | | | T1 | | | | T1 | | | | | ---------------------------->| T2 | +--+ | | | | | | | | | | | | | | | | | | | | +----+ | | +----+ | | | | | | | | | | | | | +----+ | | +----+ | | | | T1 | | | | T1 | | | | | T2 | <--------------------------<-| T2 | <--+ | | | T3 | | | | T3 | | | | T4 | | | | | | | +----+ | | +----+ | | | | | +---------------+ +----------------+ ` ]; let currentFrameIndex = 0; const asciiArtElement = document.getElementById(\"ascii-art\"); function renderFrame() { asciiArtElement.textContent = frames[currentFrameIndex]; currentFrameIndex = (currentFrameIndex + 1) % frames.length; } // Initial render renderFrame(); // Start animation at 2 frames per second (500ms delay) setInterval(renderFrame, 2000);","title":"How an NTP Client synchronises with an NTP server"},{"location":"time/System_Timing_and_NTP/#formulas-for-calculating-the-rtd-and-offset","text":"Round Trip Delay = (T4 - T1) - (T3 - T2) Clock OffSet = [(T2 - T1) + (T3 - T4)] / 2","title":"Formulas for calculating the RTD and OffSet"},{"location":"time/System_Timing_and_NTP/#3-example-calculations-for-rtd-and-offset","text":"Example 1 +--------------+ RTD = ( T4 + T1 ) + ( T3 + T2 ) | T1 0 | ( 14 + 0 ) | ( 12 + 2 ) | T2 2 | ( 14 ) + ( 10 ) | T3 12 | 4 | T4 14 | | | | | OffSet = [( T2 + T1 ) + ( T3 + T4 )] /2 | | [( 2 + 0 ) | ( 12 | 14 )] /2 | | [( 2 ) + ( +2 )] /2 | RTD = 4 | [ 0 ] /2 | OffSet = 0 | 0 +--------------+ Example 2 +--------------+ RTD = ( T4 + T1 ) + ( T3 + T2 ) | | ( 10 + 0 ) | ( 6 + 4 ) | T1 0 | ( 10 ) + ( 2 ) | T2 4 | 8 | T3 6 | | T4 10 | | | OffSet = [( T2 + T1 ) + ( T3 + T4 )] /2 | | [( 4 + 0 ) | ( 6 | 10 )] /2 | | [( 4 ) + ( +4 )] /2 | RTD = 8 | [ 0 ] /2 | OffSet = 0 | 0 +--------------+ Example 3 +---------------+ RTD = ( T4 + T1 ) + ( T3 + T2 ) | | ( 24 + 10 ) | ( 12 + 2 ) | T1 10 | ( 14 ) + ( 10 ) | T2 2 | 4 | T3 12 | | T4 24 | | | OffSet = [( T2 + T1 ) + ( T3 + T4 )] /2 | | [( 2 | 10 ) | ( 12 | 24 )] /2 | | [( +8 ) | ( +12 )] /2 | RTD = 4 | [ |20 ] /2 | OffSet = +10 | +10 +---------------+ .container { float: left; width: 50%; } .form-container { border: 2px solid #ccc; padding: 20px; margin-bottom: 20px; } .form-container h3 { margin-top: 0; } input[type=\"text\"] { width: 100px; }","title":"3 example calculations for RTD and OffSet."},{"location":"time/System_Timing_and_NTP/#networks-and-ntp-precision-and-accuracy","text":"The precision and accuracy of system time achievable and maintainable by an NTP (Network Time Protocol) client depend significantly on the network infrastructure between the NTP client and the NTP servers. * In a Local Area Network (LAN), the accuracy of NTP synchronization can typically reach the order of 100 microseconds (us). Within a Wide Area Network (WAN), the accuracy may decrease slightly, with synchronization achievable in the order of several tens of milliseconds. Over the Internet, the accuracy of NTP synchronization depends heavily on many different factors. Typically, selecting NTP servers closer to the client location rather than those farther away results in more precise time synchronization. However, it's important to note that accuracy in all these scenarios is significantly influenced by factors such as jitter and asymmetric network paths.","title":"Networks and NTP Precision and Accuracy"},{"location":"time/System_Timing_and_NTP/#example-c-programs-to-poll-the-tsc-hpetand-acpi_pm-clock-sources","text":"","title":"Example C programs to poll the TSC, HPET,and ACPI_PM clock sources"},{"location":"time/System_Timing_and_NTP/#example-c-program-to-read-the-tsc","text":"#include <stdio.h> #include <stdint.h> int main() { unsigned int tsc_val_lo, tsc_val_hi; uint64_t timestamp; // Execute the rdtsc instruction __asm__ __volatile__ (\"rdtsc\" : \"=a\"(tsc_val_lo), \"=d\"(tsc_val_hi)); // Combine the low and high parts to form the full 64bit timestamp timestamp = ((uint64_t)tsc_val_lo) | (((uint64_t)tsc_val_hi) << 32); printf(\"TSC value: %lu\\n\", timestamp); return 0; }","title":"Example C Program to read the TSC"},{"location":"time/System_Timing_and_NTP/#example-c-program-to-read-the-hpet","text":"#include <stdio.h> #include <stdint.h> #include <fcntl.h> #include <sys/mman.h> #include <unistd.h> #define HPET_BASE_ADDRESS 0xFED00000 // Base address of HPET on most x86 machine #define HPET_SIZE 0x400 // HPET size in bytes int main() { int hpet_fd; volatile void *hpet_base; uint64_t hpet_counter; // Open the /dev/mem device to access physical memory hpet_fd = open(\"/dev/mem\", O_RDONLY); if (hpet_fd < 0) { perror(\"Error opening /dev/mem\"); return 1; } // Memory-map the HPET region hpet_base = mmap(NULL, HPET_SIZE, PROT_READ, MAP_SHARED, hpet_fd, HPET_BASE_ADDRESS); if (hpet_base == MAP_FAILED) { perror(\"Error mapping HPET region\"); close(hpet_fd); return 1; } // Read the HPET counter value hpet_counter = *((volatile uint64_t *)(hpet_base + 0xFEE)); // Unmap the HPET region and close /dev/mem munmap((void *)hpet_base, HPET_SIZE); close(hpet_fd); printf(\"HPET Counter: %lu\\n\", hpet_counter); return 0; }","title":"Example C program to read the HPET"},{"location":"time/System_Timing_and_NTP/#example-c-program-to-read-the-acpi_pm","text":"#include <stdio.h> #include <stdint.h> #include <fcntl.h> #include <sys/io.h> #include <unistd.h> #define PM_BASE_ADDRESS 0x40 // Base address of ACPI PM timer on most x86 machine int main() { int pm_fd; uint32_t pm_counter; // Open /dev/port to access I/O ports pm_fd = open(\"/dev/port\", O_RDONLY); if (pm_fd < 0) { perror(\"Error opening /dev/port\"); return 1; } // Enable I/O port access if (iopl(3) < 0) { perror(\"Error enabling I/O port access\"); close(pm_fd); return 1; } // Read the ACPI PM timer counter value outl(PM_BASE_ADDRESS, 0x43); // Select counter 0 and latch its value pm_counter = inl(PM_BASE_ADDRESS); // Read the latched value // Close /dev/port close(pm_fd); printf(\"ACPI PM Timer Counter: %u\\n\", pm_counter); return 0; }","title":"Example C program to read the ACPI_PM"},{"location":"time/System_Timing_and_NTP/#sources-for-this-document","text":"Intel CPU specifications and developer guides Intel 64-ia-32-architectures-software-developer-vol-2a-manual.pdf Intel 64-ia-32-architectures-software-developer-manual-325462.pdf Intel 64-ia-32-architectures-software-developer-vol-3b-part-2-manual.html Intel timestamp-counter-scaling-virtualization-white-paper.pdf Intel xeon-e5-v2-spec-update.pdf Linux Kernel and Linux Distribution documentation https://www.kernel.org/doc/Documentation/virtual/kvm/timekeeping.txt https://access.redhat.com/solutions/18627 kb.vmware.com RedHat - Virtualization-KVM_guest_timing_management https://bugzilla.redhat.com/show_bug.cgi?id=698842 http://blog.cr0.org/2009/05/time-stamp-counter-disabling-oddities.html http://www.cs.virginia.edu/~evans/cs216/guides/x86.html http://man7.org/linux/man-pages/man2/prctl.2.html https://www.chromium.org/chromium-os/how-tos-and-troubleshooting/tsc-resynchronization https://wiki.archlinux.org/index.php/CPU_frequency_scaling http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/ https://lkml.org/lkml/2005/11/4/173 [Manual page clock_gettime] NTP (Official Documentation) https://www.eecis.udel.edu/~mills/ntp/html/index.html https://www.amazon.com/Computer-Network-Time-Synchronization-Protocol/dp/0849358051/ https://www.amazon.com/Computer-Network-Time-Synchronization-Protocol/dp/1439814635/ http://what-if.xkcd.com/26/ Leap second (XKCD always counts as official) Wikipedia links Segals Law List_of_Intel_microprocessors P5 microarchitecture Intel SpeedStep Wall-clock_time Time_Stamp_Counter Linux_kernel Allan_variance ASCII Art http://www.ascii-code.com/ascii-art/electronics/clocks.php https://www.asciiart.eu/electronics/phones Extra Info Measuring Wander and Jitter","title":"Sources for this document: "},{"location":"time/linux_clocksource_benchmark-2.c/","text":"A short program written in C and in-line X86 assembler for experimenting the the various clocksources available on an Intel X86 system. Sample execution.. Inital ClockSource setting is : tsc Available ClockSources : tsc hpet acpi_pm +--------------------------------------------------------------------------------------------------+ |ClockSrc|Exec time | Start | Finish |RDSTC start |RDTSC finish| RDTSCP start | RDTSCP finish | +--------+----------+--------+---------+------------+------------+----------------+----------------+ |tsc | 0.032044 | 1032 | 33076 | 1159606212 | 1269023424 | 1856585477990 | 1856694895364 | |hpet | 0.596099 | 33218 | 629317 | 1269654044 | 3304412637 | 1856695525864 | 1858730284571 | |acpi_pm | 0.734672 | 629399 | 1364071 | 3304881771 | 1517682755 | 1858730753603 | 1861238521977 | +--------------------------------------------------------------------------------------------------+ Source Code.. #include <stdio.h> #include <sys/time.h> #include <time.h> #include <stdlib.h> #include <stdint.h> unsigned long get_rdtscp(int *chip, int *core); static unsigned long long get_rdtsc(); int main() { //array of strings for the clocksource names char clocksource[4][15]; clocksource[0][0] = '\\0'; clocksource[1][0] = '\\0'; clocksource[2][0] = '\\0'; clocksource[3][0] = '\\0'; clocksource[4][0] = '\\0'; char initial_clocksource[15] = {'\\0'}; struct timeval mytime; struct timezone mytimezone; clock_t start, end; int x = 0; int y = 0; //special numbers to store the values retrieved from the TSC calls uint64_t starting_tsc = 0; uint64_t finishing_tsc = 0; uint64_t starting_tscp = 0; uint64_t finishing_tscp = 0; //cpu values to send the rdtscp command to the same cpu core int my_chip = 0; int my_core = 3; int * chip = &my_chip; int * core = &my_core; //saving the initial clocksource value so we can set it back when we are finished FILE *get_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"r\"); fscanf(get_clocksource, \"%s\", initial_clocksource); //saving initial value fclose(get_clocksource); printf(\"Initial ClockSource setting is : %s \\n\",initial_clocksource); //test if we have the permissions to change the current ClockSource FILE *current_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\"); if (current_clocksource == NULL) { perror(\"failed to open /sys/devices/system/clocksource/clocksource0/current_clocksource for writing\" \"\\n are you running as root or via sudo ?\\n\"); exit(1); } fclose(current_clocksource); //reading in all the available clocksource options, this seems to vary between 2 and 4 values FILE *available_clocksources = fopen(\"/sys/devices/system/clocksource/clocksource0/available_clocksource\", \"r\"); if (available_clocksources == NULL) { perror(\"failed to open /sys/devices/system/clocksource/clocksource0/available_clocksource for reading\" \"\\n are you running as root ?\\n\"); exit(1); } else { fscanf(available_clocksources, \"%s %s %s %s %s\", clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]); printf(\"Available ClockSources : %s %s %s %s %s\\n\",clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]); } printf(\"+----------------------------------------------------------------------------------------------------------+\\n\"); printf(\"|ClockSrc|Exec time | Start | Finish | RDSTC start | RDTSC finish | RDTSCP start | RDTSCP finish |\\n\"); printf(\"+--------+----------+--------+---------+----------------+----------------+----------------+----------------+\\n\"); while ( (x <= 4 ) && (clocksource[x][0] != '\\0') ){ //iterate through the available clocksource values and set them as the current value FILE *current_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\"); if (current_clocksource == NULL) { perror(\"failed to open /sys/devices/system/clocksource/clocksource0/current_clocksource for writing\" \"\\n are you running as root ?\\n\"); exit(1); } else { fprintf(current_clocksource,\"%s\", clocksource[x]); fclose(current_clocksource); } y = 0 ;//execute the gettimeofday call starting_tscp = get_rdtscp(chip,core); starting_tsc = get_rdtsc(); start = clock(); while ( y < 1000000 ) { gettimeofday(&mytime, &mytimezone); y++; } end = clock(); finishing_tsc = get_rdtsc(); finishing_tscp = get_rdtscp(chip,core); printf(\"|%-7s | %-7f | %-6ld | %-7ld | %ld | %ld | %ld | %ld |\\n\",clocksource[x],((double) (end - start)) / CLOCKS_PER_SEC,start,end,starting_tsc,finishing_tsc,starting_tscp,finishing_tscp); x++; } //setting the clocksource back to the initial value FILE *set_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\"); if (set_clocksource == NULL) { perror(\"failed to open /sys/devices/system/clocksource/clocksource0/current_clocksource for writing\" \"\\n are you running as root ?\\n\"); exit(1); } else { fprintf(set_clocksource,\"%s\", initial_clocksource); //setting clocksource back to initial value fclose(set_clocksource); } printf(\"+----------------------------------------------------------------------------------------------------------+\\n\"); return 0; } //2 functions to retrieve the counter values from the TSC //this function just calls which ever TSC that the kernel or cpu assigns, i.e it varies static __inline__ unsigned long long get_rdtsc(void) { unsigned tsc_val_lo, tsc_val_hi; __asm__ __volatile__ (\"rdtsc\" : \"=a\"(tsc_val_lo), \"=d\"(tsc_val_hi)); return ( (unsigned long long)tsc_val_lo)|( ((unsigned long long)tsc_val_hi)<<32 ); } //with this function we can hard set the CPU and core unsigned long get_rdtscp(int *chip, int *core) { unsigned a, d, c; __asm__ volatile(\"rdtscp\" : \"=a\" (a), \"=d\" (d), \"=c\" (c)); *chip = (c & 0xFFF000)>>12; *core = c & 0xFFF; return ((unsigned long)a) | (((unsigned long)d) << 32);; }","title":"Linux clocksource benchmark 2.c"}]}