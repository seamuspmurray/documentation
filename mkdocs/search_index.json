{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to my linux notes site\n\n\nThis is were I put notes about stuff that I may need to use again in the future.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-my-linux-notes-site", 
            "text": "This is were I put notes about stuff that I may need to use again in the future.", 
            "title": "Welcome to my linux notes site"
        }, 
        {
            "location": "/map/", 
            "text": ".\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 linux\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Backups\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Clustering\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 cluster2_arch.txt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_automatic_failover_manual_fence_override.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_automatic_failover.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_cluster_build_guide.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_cluster_build.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_manual_failover.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_manual_failover.txt.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homemade_cluster.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mkclusterconf.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 RHEL4_cluster_installation.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 simple.cluster.conf.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Commands\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 email_on_linux.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hot_add_memory.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Core_Utils\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 awk.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bash_shortcuts.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spacecmd_breaking_up_large_groups_of_hosts_with_awk.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test.log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 user_creation.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 vim_sudo.mk\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 watch_and_timeout.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Disks_and_File_Systems\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 boot_cdrom_dvdrom.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk_filesystem_partition_imaging_using_squashfs_and_dd.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Extend_lvm_filesystem.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 filesystem_shuffle.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Linux_LVM_Logical_Volume_Manager.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Listing_Oracle_ASM_disks.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Multipath_and_NetApp_fixes.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Online_lun_resize.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 RHEL_floppy_kickstart.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rsync.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 shrink_lvm_filesystem.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Firewalls_and_Security\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auditd_rules.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 iptables_notes.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hot_add_memory_and_cpus.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Installing_Linux\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 linux_proccess_signaling.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Monitoring\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 nagios_2.6_install.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Networking\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 check_dns_records.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 determine_the_routes_a_packet_will_take.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 linux_networking.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 local_IP_address_survey.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 RedHat\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kickstart.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 satellite_spacewalk\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 perl_script_to_calulate_uptime_of_satellite_clients.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replacing_your_satellite_certificate.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 satellite_certificate_licence_install.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 using_spacecmd.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yum_rpm\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 create_local_repos_from_redhat_install_media.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 working_with_yum_and_rpm.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 yum_and_rpm.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Software_Packaging\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 apt_get.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Time\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 Adventures_in_the_land_of_symmetric_ntp_authentication.md\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 Setting_time.md\n\u251c\u2500\u2500 map.md\n\u251c\u2500\u2500 programming\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c_programming\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fork_and_maloc_demo.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fork_and_maloc_demo.md~\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fork_demo_menu.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 linux_clocksource_benchmark.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 programmable_keyboard.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 python\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 python_study\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-01\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_bigdigits.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_calculator.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_poem_generator.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 05_calculator.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 bigdigits.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 even-or-odd.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 seamus.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 sort.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-02\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_print_unicode.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_quadratic.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_csv2html.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 04_csv2html.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 input.csv\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-03\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_external_sites.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_uniquewords2.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_generate_usernames.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 bbc.html\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 external_sites.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 generate_usernames.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 mark-bio.txt\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 slashdot.html\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 smh.html\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 uniquewords2.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 users2.txt\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 users.txt\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 wordlist.text\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-04\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_interactive.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 new.lst\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-05\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 big.file\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 last.txt\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 ls.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 ls.py3\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 ls.py.bak\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 opt.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 testdir\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 child-test.file\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 z.text\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-06\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 1234.acc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 1.acc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 Account.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 create-image.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_small.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 Image.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 Image.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 large.save\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 large.xpm\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 meduim.save\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 MY_Shape.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 object.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 __pycache__\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Account.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Image.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 MY_Shape.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Shape_ans.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Shape.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 transaction.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Transaction.cpython-34.pyc\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 pycallgraph.png\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 resize_dict.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 seamus.xpm\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 Shape.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 shrink.xpm\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 smaller.save\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 smaller.xpm\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 small.save\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 small.xpm\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 Transaction.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 chapter-07\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 convert-incidents.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 incidents.aix\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 generate_usernames.py\n\u251c\u2500\u2500 puppet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 puppet_study_notes.md\n\u2514\u2500\u2500 scripting\n    \u251c\u2500\u2500 bash_and_grep_script_to_compare_2_lists.md\n    \u251c\u2500\u2500 bash_and_powershell_failover\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 failover\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 BreakLoop.sh\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Break.sh\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 check_lagtime\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake.sh\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 scripts\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 DR\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 break_snap_mirror.sh\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 initiate_dr.sh\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 options.txt\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 primary_confirm_fail.sh\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 fence_netapp\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 fence.txt.txt\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Primary\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 primary_test.sh\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 steward\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 schedule_forwarderA.xml\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 slc_steward.ps1\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 SSH_Sessions.PSM1\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 where_is_the_luns.sh\n    \u251c\u2500\u2500 bash_argument_shift_until_loop.sh.md\n    \u251c\u2500\u2500 bash_argument_shift_while_loop.sh.md\n    \u251c\u2500\u2500 bash_arguments.sh.md\n    \u251c\u2500\u2500 bash_array.sh\n    \u251c\u2500\u2500 bash_capture_keyboard_entry.sh.md\n    \u251c\u2500\u2500 bash_case.sh.md\n    \u251c\u2500\u2500 bash_fail_and_exit.sh.md\n    \u251c\u2500\u2500 bash_fail_trap.sh.md\n    \u251c\u2500\u2500 bash_function.sh.md\n    \u251c\u2500\u2500 bash_functions.md\n    \u251c\u2500\u2500 bash_local_vs_global_variables.md\n    \u251c\u2500\u2500 bash_nested_if_else.sh.md\n    \u251c\u2500\u2500 bash_read_input.sh.md\n    \u251c\u2500\u2500 Bash_scripting.md\n    \u251c\u2500\u2500 bash_select.sh.md\n    \u251c\u2500\u2500 bash_sort.md\n    \u251c\u2500\u2500 bash_string_comparison.sh.md\n    \u251c\u2500\u2500 bash_test_to_only_run_as_root.md\n    \u251c\u2500\u2500 bash_tips.md\n    \u251c\u2500\u2500 bash_trap_user_ctrl-c.sh.md\n    \u251c\u2500\u2500 bash_wait_for_user_input.sh.md\n    \u251c\u2500\u2500 Column_tally.md\n    \u251c\u2500\u2500 Convert_kilobytes_to_MB_GB_TB_PB.md\n    \u251c\u2500\u2500 diff.md\n    \u251c\u2500\u2500 DNS_validate_entries.md\n    \u251c\u2500\u2500 expect.md\n    \u251c\u2500\u2500 IP_survey.md\n    \u251c\u2500\u2500 map.md\n    \u251c\u2500\u2500 script_to_markdown.md\n    \u251c\u2500\u2500 Shuffle_deck_of_cards.md\n    \u251c\u2500\u2500 simulate_site_failure.md\n    \u2514\u2500\u2500 use_a_file_to_control_bash_script_operation.sh.md\n\n37 directories, 176 files", 
            "title": "Map"
        }, 
        {
            "location": "/programming/map/", 
            "text": ".\n\u251c\u2500\u2500 c_programming\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fork_and_maloc_demo.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fork_and_maloc_demo.md~\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 fork_demo_menu.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 linux_clocksource_benchmark.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 programmable_keyboard.md\n\u251c\u2500\u2500 map.md\n\u2514\u2500\u2500 python\n    \u2514\u2500\u2500 python_study\n        \u251c\u2500\u2500 chapter-01\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_bigdigits.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_calculator.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_poem_generator.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 05_calculator.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 bigdigits.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 even-or-odd.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 seamus.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 sort.py\n        \u251c\u2500\u2500 chapter-02\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_print_unicode.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_quadratic.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_csv2html.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 04_csv2html.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 input.csv\n        \u251c\u2500\u2500 chapter-03\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_external_sites.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 02_uniquewords2.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 03_generate_usernames.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 bbc.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 external_sites.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 generate_usernames.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 mark-bio.txt\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 slashdot.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 smh.html\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 uniquewords2.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 users2.txt\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 users.txt\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 wordlist.text\n        \u251c\u2500\u2500 chapter-04\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 01_interactive.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 new.lst\n        \u251c\u2500\u2500 chapter-05\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 big.file\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 last.txt\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 ls.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 ls.py3\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 ls.py.bak\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 opt.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 testdir\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 child-test.file\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 z.text\n        \u251c\u2500\u2500 chapter-06\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 1234.acc\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 1.acc\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 Account.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 create-image.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_small.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 Image.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 Image.pyc\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 large.save\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 large.xpm\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 meduim.save\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 MY_Shape.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 object.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 __pycache__\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Account.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Image.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 MY_Shape.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Shape_ans.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Shape.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 transaction.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Transaction.cpython-34.pyc\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 pycallgraph.png\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 resize_dict.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 seamus.xpm\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 Shape.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 shrink.xpm\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 smaller.save\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 smaller.xpm\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 small.save\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 small.xpm\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 Transaction.py\n        \u251c\u2500\u2500 chapter-07\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 convert-incidents.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 incidents.aix\n        \u2514\u2500\u2500 generate_usernames.py\n\n12 directories, 74 files", 
            "title": "Map"
        }, 
        {
            "location": "/programming/c_programming/fork_and_maloc_demo/", 
            "text": "simple program that I created to demonstrate the fork system call\n\n\nand how memory gets copied on write from one process to another\n\n\n#include \nstdio.h\n\n#include \nunistd.h\n\n#include \nstdlib.h\n\n#include \nstring.h\n\n#include \nsys/wait.h\n\n\nint calc_pi() //I copied this function from the net somewhere\n{\n    int r[2800 + 1];\n    int i, k;\n    int b, d;\n    int c = 0;\n\n    for (i = 0; i \n 2800; i++) {\n        r[i] = 2000;\n    }\n\n    for (k = 2800; k \n 0; k -= 14) {\n        d = 0;\n\n        i = k;\n        for (;;) {\n            d += r[i] * 10000;\n            b = 2 * i - 1;\n\n            r[i] = d % b;\n            d /= b;\n            i--;\n            if (i == 0) break;\n            d *= i;\n        }\n      //printf(\"%.4d\", c + d / 10000);\n        c = d % 10000;\n    }\n\n    return 0;\n}\n\nint main(int argc, char ** argv)\n{\n    int pid;\n\n    printf(\"I am the parent proccess \\n\");\n    int counter = 0;\n    while ( counter \n= 2000 ) {\n        calc_pi(); \n        counter += 1;\n       }\n\n    int * myparentmemory = malloc(10000000 * sizeof(int));\n    pid = fork();\n    sleep(10);\n    if (pid == 0){\n        sleep(1);\n        printf(\"I am the child proccess\\n\");\n        argv[0][1] = 'c';\n        argv[0][2] = 'h';\n        argv[0][3] = 'i';\n        argv[0][4] = 'l';\n        argv[0][5] = 'd';\n        int * mychildmemory = malloc(10000000 * sizeof(int));\n        memset (myparentmemory,'x',5000);\n       }\n    else {\n        int exitstatus = 0;\n        sleep(1);\n        printf(\"I am the parent of the child proccess ID %d\\n\", pid);\n        memset (myparentmemory,'x',50000);\n        printf(\"child exited with status of = %d\\n\",wait(\nexitstatus));\n       }\n    sleep(1000);\n    return 0;\n}", 
            "title": "Fork and maloc demo"
        }, 
        {
            "location": "/programming/c_programming/fork_and_maloc_demo/#simple-program-that-i-created-to-demonstrate-the-fork-system-call", 
            "text": "and how memory gets copied on write from one process to another  #include  stdio.h \n#include  unistd.h \n#include  stdlib.h \n#include  string.h \n#include  sys/wait.h \n\nint calc_pi() //I copied this function from the net somewhere\n{\n    int r[2800 + 1];\n    int i, k;\n    int b, d;\n    int c = 0;\n\n    for (i = 0; i   2800; i++) {\n        r[i] = 2000;\n    }\n\n    for (k = 2800; k   0; k -= 14) {\n        d = 0;\n\n        i = k;\n        for (;;) {\n            d += r[i] * 10000;\n            b = 2 * i - 1;\n\n            r[i] = d % b;\n            d /= b;\n            i--;\n            if (i == 0) break;\n            d *= i;\n        }\n      //printf(\"%.4d\", c + d / 10000);\n        c = d % 10000;\n    }\n\n    return 0;\n}\n\nint main(int argc, char ** argv)\n{\n    int pid;\n\n    printf(\"I am the parent proccess \\n\");\n    int counter = 0;\n    while ( counter  = 2000 ) {\n        calc_pi(); \n        counter += 1;\n       }\n\n    int * myparentmemory = malloc(10000000 * sizeof(int));\n    pid = fork();\n    sleep(10);\n    if (pid == 0){\n        sleep(1);\n        printf(\"I am the child proccess\\n\");\n        argv[0][1] = 'c';\n        argv[0][2] = 'h';\n        argv[0][3] = 'i';\n        argv[0][4] = 'l';\n        argv[0][5] = 'd';\n        int * mychildmemory = malloc(10000000 * sizeof(int));\n        memset (myparentmemory,'x',5000);\n       }\n    else {\n        int exitstatus = 0;\n        sleep(1);\n        printf(\"I am the parent of the child proccess ID %d\\n\", pid);\n        memset (myparentmemory,'x',50000);\n        printf(\"child exited with status of = %d\\n\",wait( exitstatus));\n       }\n    sleep(1000);\n    return 0;\n}", 
            "title": "simple program that I created to demonstrate the fork system call"
        }, 
        {
            "location": "/programming/c_programming/fork_demo_menu/", 
            "text": "#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nstring.h\n\n#include \nunistd.h\n\n#include \nsys/types.h\n\n\n\nint main_menu(void)\n{\nint main_choice = 0;\nint child_choice = 0;\n    while (!(main_choice == 1 || main_choice == 2 || main_choice == 3))\n        {\n        printf(\"choose a menu option..1,2 or 3:\\n\");\n        printf(\"1) fork children\\n\");\n        printf(\"2) list children\\n\");\n        printf(\"3) kill children\\n\");\n        printf(\"\n \n\\b\\b\\b\\b\");\n        scanf(\"%1d\", \nmain_choice);\n        while (getchar() != '\\n');\n        }\n    printf(\"%d\\n\", main_choice);\n\n    if (main_choice == 2) {\n        printf(\"listing all child process id's\\n\");\n        }\n\n    if (main_choice == 1) {\n        while (!(child_choice \n= 1 \n  child_choice \n= 9))\n            {\n            printf(\"how many children... 1 - 9 :\\n\");\n            printf(\"\n \n\\b\\b\\b\\b\");\n            scanf(\"%1d\", \nchild_choice);\n            while (getchar() != '\\n');\n            }\n        }\n\n    if (main_choice == 3)\n        {\n        printf(\"killing all child proccess's's's's's\\n\");\n        }\n\n    main_choice *= 10; \n    // multiplying by 10 so i can return 2 values with one integer\n    // 12 = fork 2 children\n    // 19 = fork 9 children\n    // 20 = list children\n    // 30 = kill children\n    return main_choice + child_choice;\n\n\n}\n\nint main(void)\n{\nint i;\nint user_choice;\nint child_proccess_array[10] = {0};\nuser_choice = main_menu();\nint parent_pid = getpid();\nprintf(\"i am %d\\n\",getpid());\n\n\n\nif ((user_choice \n 10) \n (user_choice \n 20)) {\n    int num_of_children = user_choice % 10; \n    printf(\"forking %d child proccesses.....\\n\",num_of_children);\n    for ( i = 1; i \n= num_of_children ; i++) {\n        pid_t child_pid = fork();\n        child_proccess_array[i] = child_pid;\n        printf(\"i am %d\\n\",getpid());\n        printf(\"I %d forked my %d child proccess with a pid of %d\\n\",getpid(),i, child_pid);\n        if (child_pid == 0) \n           printf(\"%d says child pid is 0 and parent_pid = %d \\n\",getpid(),parent_pid);\n        if (getpid() != parent_pid)\n            break;\n    }      \n}\n\nif (user_choice == 20){\n    for ( i = 1; i \n10 ; i++) {\n          if ( child_proccess_array[i] \n= 1 ) {\n              printf(\"child proccess %d = %d\\n\", i, child_proccess_array[i]);\n          }\n    }\n}\n\nif (user_choice == 30){\n    for ( i = 1; i \n10 ; i++) {\n          if ( child_proccess_array[i] \n= 1 ) {\n              printf(\"killing child proccess %d = %d\\n\", i, child_proccess_array[i]);\n          }\n    }\n}\n//printf(\"my global_pid = %ld\\n\",global_pid);\nsleep(60);\nreturn 0;\n}", 
            "title": "Fork demo menu"
        }, 
        {
            "location": "/programming/c_programming/linux_clocksource_benchmark/", 
            "text": "#include \nstdio.h\n\n#include \nsys/time.h\n\n#include \ntime.h\n\n\n\n\nint main()\n\n{\nchar clocksource[4][15];\nclocksource[0][0] = '\\0';\nclocksource[1][0] = '\\0';\nclocksource[2][0] = '\\0';\nclocksource[3][0] = '\\0';\nclocksource[4][0] = '\\0';\nchar initial_clocksource[15] = {'\\0'};\nstruct timeval mytime;\nstruct timezone mytimezone;\nclock_t start, end;\nint x = 0;\nint y = 0;\n\n\n//saving the initial clocksource value so we can set it back when we are finished\nFILE *get_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"r\");\nfscanf(get_clocksource, \"%s\", initial_clocksource); //saving initial value \nfclose(get_clocksource);\nprintf(\"Inital ClockSource setting is : %s \\n\",initial_clocksource);\n\n//reading in all the available clocksource oprions, this seems to vary between 2 and 4 values \nFILE *in_file  = fopen(\"/sys/devices/system/clocksource/clocksource0/available_clocksource\", \"r\");\nfscanf(in_file, \"%s %s %s %s %s\", clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]);\nprintf(\"Available ClockSources : %s %s %s %s %s\\n\",clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]);\n\nwhile ( (x \n= 4 ) \n (clocksource[x][0] \n= 1) ){ //itterate through the available clocksource values and set them as the current value\n      FILE *current_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\");\n      printf(\"Testing %s \",clocksource[x]);\n      fprintf(current_clocksource,\"%s\", clocksource[x]);\n      x++;\n      fclose(current_clocksource);\n\n         y = 0 ;//execute the gettimeofday call \n         start = clock();\n         while ( y \n 1000000 ) {\n             gettimeofday(\nmytime, \nmytimezone);\n             y++;\n             }\n\n         end = clock();\n         printf(\"Execution took %f \\n\",((double) (end - start)) / CLOCKS_PER_SEC);\n\n     }\n\n//setting the clocksource back to the initial value\nFILE *set_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\");\nfprintf(set_clocksource,\"%s\", initial_clocksource); //setting clocksource back to  initial value\nfclose(set_clocksource);\n\nreturn 0;\n\n}", 
            "title": "Linux clocksource benchmark"
        }, 
        {
            "location": "/programming/c_programming/programmable_keyboard/", 
            "text": "#include \nstdio.h\n\n#include \nstrings.h\n\n\n/*\nthis program will generate a keyboard macro file that can be uploaded to the maxkeybaord nighthawk x8.\nThe file can be uploaded to the keyboard  by importing via the maxkeyboard \"Gaming keyboard config\" software.\n *\n *\nusage: binaryname \n inputfile of text you want to type\nMaxkeyboard nighthawk x8 Macro generator\n\n\n\nTodo..\n* figure out the upload protocol and validation\n* write a linux program to upload the macro\n * figure out how to upload data to the flash memory in the 8 bit micro controller Freescale MC9S08JM16 \n     http://cache.freescale.com/files/microcontrollers/doc/data_sheet/MC9S08JM16.pdf\n     USB-LITE Stack by CMX and the USB-MINI Stack by Freescale\n\n  * capture and analyse the usbdata using usbpcap http://desowin.org/usbpcap/ and wireshark //done\n  * write a http://desowin.org/usbpcap/dissectors.html\n\n\nhttp://www.maxkeyboard.com/download.html\n\nDetails regarding the file format that the Maykeyboard software uses.\n\nThe config file is a plain text file\n\n\neach macro is stored as a series of bytes\n\nsample blank config with the 11202 zeros redacted...\n[Setting]\nMacroLength=0\nDiableWinKey=0\n[BUTTONCONFIG0]\nFunID=255\nMappinkey=255\n[MACROG0]\nRepeatOption=0\nMacro={hex values for the macro + a checksum}\nMacroLength=0\n[BUTTONCONFIG1]\nFunID=255\nMappinkey=255\n[MACROG1]\nRepeatOption=0\nMacro={hex values for the macro + a checksum}\nMacroLength=0\n[BUTTONCONFIG2]\nFunID=255\nMappinkey=255\n[MACROG2]\nRepeatOption=0\nMacro={hex values for the macro + a checksum}\nMacroLength=0\n\n\n\n[BUTTONCONFIG0] the macro number/id\nFunID=32 (seems to indicate keyboard macro)\nMappinkey= this is the physical key on the keyboard that is assigned to this macro, 1 = F1 , 2 = F2 etc\n[MACROG0] the macro group this macro belongs to ( I haven't used this yet)\nRepeatOption=1   this needs to be set to 1 or the macro will not run\nMacro= a single line with 11204 chars, seems to be divided up into 700 x 16 byte words..  the last 2 chars on this line seem to be a checksum that isn't validated (it changes when anything else changes)\nMacroLength= a count of the 16 char words,this needs to be set based on the line above\n\n\n# The Macro format.........\n\neach char that requires a single key press takes 4 x 16 byte words\n\n.1 key value press\n.2 key time down\n.3 key value release\n.4 key time release\n\n## if the inputed key requires 2 key presses eg.. upper case or special chars !@#...... then 8 x 16 byte words are needed\n.1 shift key value press \n.2 shift time down\n.3 key value press \n.4 key time down\n.5 key value release\n.6 key time release\n.7 shift key release \n.8 shift time release\n\nthe timing is based on milliseconds and stored in hex\nthe largest timing value seems to be 6375 milliseconds or 18E7\neg \n       50 milliseconds = 32\n      100 milliseconds = 64\n      125 milliseconds = 7D\n\nthe values are stored in the file in little endian notation ( least significant byte first )\neg... the hex value 18E7  is stored as E718\n\n      Decimal =         6375\n      Hex     =         18E7       \n      In File = FF000000E718000000\n\n\n\nexample press of 'a' for 100 milliseconds with a 100 millisecond gap at end   \nX = value that can be changed\n                ____________    ____________    ____________    ____________  \n               /            \\  /            \\  /            \\  /            \\ \n              | Key to press || Time to press||Key to release|| Time after   |\n              1400000001000000FF000000640000001400000000000000FF00000064000000\nKey Value     XX                              XX                              \nStatic value    0000000 000000                  0000000 000000                \nPress/Release          X                               X\n\nStatic value                  FF000000                        FF000000        \nTime Value                            XXXX                            XXXX\n\nexample press of 'a' for 1000 milliseconds with a 100 millisecond gap at end  \n                ____________    ____________    ____________    ____________  \n               /            \\  /            \\  /            \\  /            \\ \n              | Key to press || Time to press||Key to release|| Time after   |\n              1400000001000000FF000000E60300001400000000000000FF00000064000000\nKey Value     XX                              XX                              \nStatic value    0000000 000000                  0000000 000000                \nPress/Release          X                               X                      \nStatic value                  FF000000                        FF000000        \nTime Value                            XXXX                            XXXX\n\n\n100mSec       FF00000064000000\nKey_press       00000001000000\nkey_release     00000000000000\n\nto enter the exclamation mark ! on the US keyboard...requires that the shift key is pressed whilst the 1 key is pressed\n\nshift key on,     8300000001000000FF00000012020000    //this time has to extend past the next key\n1 key on and off, 1100000001000000FF0000005E0000001100000000000000FF00000048010000\nshiftkey  off,    8300000000000000FF00000000000000\n\n\n\n\nbelow is a list of the key values for the US keyboard layout\n\nkey value\n\n\n\nIncomplete list of the control keys\nshift  83\ncapslk 1B\nspace  57\nTAB    13\nL CTRL 88\nL ALT  7D\nEnter  56\nBKSPC  53\n\n\nList of the keys that only require one key to the pressed\na 14\nb 2F\nc 26\nd 24\ne 22\nf 2C\ng 2D\nh 35\ni 3A\nj 34\nk 3C\nl 44\nm 36\nn 37\no 42\np 4A\nq 12\nr 2A\ns 1C\nt 2B\nu 32\nv 2E\nw 1A\nx 1E\ny 33\nz 16\n1 11\n2 19\n3 21\n4 29\n5 28\n6 30\n7 31\n8 39\n9 41\n0 49\n- 48\n= 38\n[ 4B\n] 3B\n\\ 54\n; 4C\n' 4D\n, 3E\n. 46\n/ 4F\n` 10\n  57\n\n\nKeys that require the shift key to be pressed simultaneously \n~ 10\n! 11\n@ 19\n# 21\n$ 29\n% 28\n^ 30\n\n 31\n* 39\n( 41\n) 49\n_ 48\n+ 38\n{ 4B\n} 3B\n| 54\n: 4C\n\" 4D\n\n 3E\n\n 46\n? 4F\n\nA 14\nB 2F\nC 26\nD 24\nE 22\nF 2C\nG 2D\nH 35\nI 3A\nJ 34\nK 3C\nL 44\nM 36\nN 37\nO 42\nP 4A\nQ 12\nR 2A\nS 1C\nT 2B\nU 32\nV 2E\nW 1A\nX 1E\nY 33\nZ 16\n\n each macro will be an array of 11204 chars, initialized with zeros, \n monitoring the usb bus whilst uploading a config that contains mostly zeros \n not seems to be transferred to the keyboard i assume that are only used my the maxkeyboard software\n\n\n Macro length limit... \n we are limited to 700 16Byte words in each macro ,  \n  #1 key value press = 16 Byte word\n  #2 key time down = 16 Byte word\n  #3 key value release = 16 Byte word\n  #4 key time release = 16 Byte word\n\nthis equates to 175 key press and key release actions.\nNote if you require multiple keys to be pressed simultaneously it will use more bytes eg the shift + key stated above\n\nmake sure you do not go over the 700 limit, leave a few bytes free. \nIf you enter a invalid macro config...sometimes all the keyboard macros stop working\nperforming a reset from the app or uploading an empty config does not reset the key board.\nStrangely overwriting/applying a valid (previously exported) config containing  multiple macros will restore the keyboards macro functions\n\n*/\n\n\n\nint main()\n{\n    int c, char_tally, MacroLength;\n    unsigned char key_value;\n    int shift_status; //for future use if i decided to extend the shift press for sequential upper keys\n    char* key_press = \"00000001000000\";\n    char* key_release = \"00000000000000\";\n    // there are 4 different timings 25 milliseconds doesn't seem to work\n    // 50 milliseconds seems like a good mix between speed and consistency\n    // 100 milliseconds may be needed when working through multiple remote nested RDP citrix vnx etc..\n    // 150 milliseconds is to hold the shift down past the timing of the upper keys\n    char* key_time_down_25 = \"FF00000019000000\";\n    char* key_time_down_50 = \"FF00000032000000\";\n    char* key_time_down_100 = \"FF00000064000000\";\n    char* key_time_down_150 = \"FF00000096000000\";\n    char* key_time_release_25 = \"FF00000019000000\";\n    char* key_time_release_50 = \"FF00000032000000\";\n    char* key_time_release_100 = \"FF00000064000000\";\n    char* key_time_release_150 = \"FF00000096000000\";\n    char* shift_key_press = \"8300000001000000\";\n    char* shift_key_release = \"8300000000000000\";\n    char* zero_fill = \"0000000000000000\";\n    key_value = 0;\n    char_tally = 0;\n    MacroLength = 0;\n    shift_status = 0;\n\n\n    char single_lowwer(char c)\n           {\n           //printf(\"%hhX%s%s%hhX%s%s\",key_value, key_press, key_time_down_100, key_value,key_release, key_time_release_100 );\n           printf(\"%hhX%s%s%hhX%s%s\",key_value, key_press, key_time_down_50, key_value,key_release, key_time_release_50 );\n           MacroLength = MacroLength + 4;\n             }\n\n    char single_upper(char c)\n           {\n           printf(\"%s%s\",shift_key_press, key_time_down_150  );\n           printf(\"%hhX%s%s%hhX%s%s\", key_value, key_press, key_time_down_100, key_value,key_release, key_time_release_100 );\n           printf(\"%s%s\", shift_key_release, key_time_release_50 );\n           MacroLength = MacroLength + 8;\n             }\n\n\n   while ((c= getchar()) != EOF) {\n      ++char_tally;\n      if (c == 'a' ){\n             key_value = 0x14;\n              single_lowwer(c);\n             }\n      else if ( c == 'b' ){\n             key_value = 0x2F;\n             single_lowwer(c);\n             }\n      else if ( c == 'c' ){\n             key_value = 0x26;\n             single_lowwer(c);\n             }\n      else if ( c == 'd' ){\n             key_value = 0x24;\n             single_lowwer(c);\n             }\n      else if ( c == 'e' ){\n             key_value = 0x22;\n             single_lowwer(c);\n             }\n      else if ( c == 'f' ){\n             key_value = 0x2C;\n             single_lowwer(c);\n             }\n      else if ( c == 'g' ){\n             key_value = 0x2D;\n             single_lowwer(c);\n             }\n      else if ( c == 'h' ){\n             key_value = 0x35;\n             single_lowwer(c);\n             }\n      else if ( c == 'i' ){\n             key_value = 0x3A;\n             single_lowwer(c);\n             }\n      else if ( c == 'j' ){\n             key_value = 0x34;\n             single_lowwer(c);\n             }\n      else if ( c == 'k' ){\n             key_value = 0x3C;\n             single_lowwer(c);\n             }\n      else if ( c == 'l' ){\n             key_value = 0x44;\n             single_lowwer(c);\n             }\n      else if ( c == 'm' ){\n             key_value = 0x36;\n             single_lowwer(c);\n             }\n      else if ( c == 'n' ){\n             key_value = 0x37;\n             single_lowwer(c);\n             }\n      else if ( c == 'o' ){\n             key_value = 0x42;\n             single_lowwer(c);\n             }\n      else if ( c == 'p' ){\n             key_value = 0x4A;\n             single_lowwer(c);\n             }\n      else if ( c == 'q' ){\n             key_value = 0x12;\n             single_lowwer(c);\n             }\n      else if ( c == 'r' ){\n             key_value = 0x2A;\n             single_lowwer(c);\n             }\n      else if ( c == 's' ){\n             key_value = 0x1C;\n             single_lowwer(c);\n             }\n      else if ( c == 't' ){\n             key_value = 0x2B;\n             single_lowwer(c);\n             }\n      else if ( c == 'u' ){\n             key_value = 0x32;\n             single_lowwer(c);\n             }\n      else if ( c == 'v' ){\n             key_value = 0x2E;\n             single_lowwer(c);\n             }\n      else if ( c == 'w' ){\n             key_value = 0x1A;\n             single_lowwer(c);\n             }\n      else if ( c == 'x' ){\n             key_value = 0x1E;\n             single_lowwer(c);\n             }\n      else if ( c == 'y' ){\n             key_value = 0x33;\n             single_lowwer(c);\n             }\n      else if ( c == 'z' ){\n             key_value = 0x16;\n             single_lowwer(c);\n             }\n      else if ( c == '1' ){\n             key_value = 0x11;\n             single_lowwer(c);\n             }\n      else if ( c == '2' ){\n             key_value = 0x19;\n             single_lowwer(c);\n             }\n      else if ( c == '3' ){\n             key_value = 0x21;\n             single_lowwer(c);\n             }\n      else if ( c == '4' ){\n             key_value = 0x29;\n             single_lowwer(c);\n             }\n      else if ( c == '5' ){\n             key_value = 0x28;\n             single_lowwer(c);\n             }\n      else if ( c == '6' ){\n             key_value = 0x30;\n             single_lowwer(c);\n             }\n      else if ( c == '7' ){\n             key_value = 0x31;\n             single_lowwer(c);\n             }\n      else if ( c == '8' ){\n             key_value = 0x39;\n             single_lowwer(c);\n             }\n      else if ( c == '9' ){\n             key_value = 0x41;\n             single_lowwer(c);\n             }\n      else if ( c == '0' ){\n             key_value = 0x49;\n             single_lowwer(c);\n             }\n      else if ( c == '-' ){\n             key_value = 0x48;\n             single_lowwer(c);\n             }\n      else if ( c == '=' ){\n             key_value = 0x38;\n             single_lowwer(c);\n             }\n      else if ( c == '[' ){\n             key_value = 0x4B;\n             single_lowwer(c);\n             }\n      else if ( c == ']' ){\n             key_value = 0x3B;\n             single_lowwer(c);\n             }\n      else if ( c == '\\\\' ){\n             key_value = 0x54;\n             single_lowwer(c);\n             }\n      else if ( c == ';' ){\n             key_value = 0x4C;\n             single_lowwer(c);\n             }\n      else if ( c == '8' ){\n             key_value = 0x39;\n             single_lowwer(c);\n             }\n      else if ( c == '9' ){\n             key_value = 0x41;\n             single_lowwer(c);\n             }\n      else if ( c == '0' ){\n             key_value = 0x49;\n             single_lowwer(c);\n             }\n      else if ( c == '-' ){\n             key_value = 0x48;\n             single_lowwer(c);\n             }\n      else if ( c == '=' ){\n             key_value = 0x38;\n             single_lowwer(c);\n             }\n      else if ( c == '[' ){\n             key_value = 0x4B;\n             single_lowwer(c);\n             }\n      else if ( c == ']' ){\n             key_value = 0x3B;\n             single_lowwer(c);\n             }\n      else if ( c == '\\\\' ){\n             key_value = 0x54;\n             single_lowwer(c);\n             }\n      else if ( c == '\\'' ){\n             key_value = 0x4D;\n             single_lowwer(c);\n             }\n      else if ( c == ',' ){\n             key_value = 0x3E;\n             single_lowwer(c);\n             }\n      else if ( c == '.' ){\n             key_value = 0x46;\n             single_lowwer(c);\n             }\n      else if ( c == '/' ){\n             key_value = 0x4F;\n             single_lowwer(c);\n             }\n      else if ( c == '`' ){\n             key_value = 0x10;\n             single_lowwer(c);\n             }\n      else if ( c == ' ' ){\n             key_value = 0x57;\n             single_lowwer(c);\n             }\n      //start of shift key\n      else if ( c == '~' ){\n             key_value = 0x10;\n             single_upper(c);\n             }\n      else if ( c == '!' ){\n             key_value = 0x11;\n             single_upper(c);\n             }\n      else if ( c == '@' ){\n             key_value = 0x19;\n             single_upper(c);\n             }\n      else if ( c == '#' ){\n             key_value = 0x21;\n             single_upper(c);\n             }\n      else if ( c == '$' ){\n             key_value = 0x29;\n             single_upper(c);\n             }\n      else if ( c == '%' ){\n             key_value = 0x28;\n             single_upper(c);\n             }\n      else if ( c == '^' ){\n             key_value = 0x30;\n             single_upper(c);\n             }\n      else if ( c == '\n' ){\n             key_value = 0x31;\n             single_upper(c);\n             }\n      else if ( c == '*' ){\n             key_value = 0x39;\n             single_upper(c);\n             }\n      else if ( c == '(' ){\n             key_value = 0x41;\n             single_upper(c);\n             }\n      else if ( c == ')' ){\n             key_value = 0x49;\n             single_upper(c);\n             }\n      else if ( c == '_' ){\n             key_value = 0x48;\n             single_upper(c);\n             }\n      else if ( c == '+' ){\n             key_value = 0x38;\n             single_upper(c);\n             }\n      else if ( c == '{' ){\n             key_value = 0x4B;\n             single_upper(c);\n             }\n      else if ( c == '}' ){\n             key_value = 0x3B;\n             single_upper(c);\n             }\n      else if ( c == '|' ){\n             key_value = 0x54;\n             single_upper(c);\n             }\n      else if ( c == ':' ){\n             key_value = 0x4C;\n             single_upper(c);\n             }\n      else if ( c == '\"' ){\n             key_value = 0x4D;\n             single_upper(c);\n             }\n      else if ( c == '\n' ){\n             key_value = 0x3E;\n             single_upper(c);\n             }\n      else if ( c == '\n' ){\n             key_value = 0x46;\n             single_upper(c);\n             }\n      else if ( c == '?' ){\n             key_value = 0x4F;\n             single_upper(c);\n             }\n      else if ( c == 'A' ){\n             key_value = 0x14;\n             single_upper(c);\n             }\n      else if ( c == 'B' ){\n             key_value = 0x2F;\n             single_upper(c);\n             }\n      else if ( c == 'C' ){\n             key_value = 0x26;\n             single_upper(c);\n             }\n      else if ( c == 'D' ){\n             key_value = 0x24;\n             single_upper(c);\n             }\n      else if ( c == 'E' ){\n             key_value = 0x22;\n             single_upper(c);\n             }\n      else if ( c == 'F' ){\n             key_value = 0x2C;\n             single_upper(c);\n             }\n      else if ( c == 'G' ){\n             key_value = 0x2D;\n             single_upper(c);\n             }\n      else if ( c == 'H' ){\n             key_value = 0x35;\n             single_upper(c);\n             }\n      else if ( c == 'I' ){\n             key_value = 0x3A;\n             single_upper(c);\n             }\n      else if ( c == 'J' ){\n             key_value = 0x34;\n             single_upper(c);\n             }\n      else if ( c == 'K' ){\n             key_value = 0x3C;\n             single_upper(c);\n             }\n      else if ( c == 'L' ){\n             key_value = 0x44;\n             single_upper(c);\n             }\n      else if ( c == 'M' ){\n             key_value = 0x36;\n             single_upper(c);\n             }\n      else if ( c == 'N' ){\n             key_value = 0x37;\n             single_upper(c);\n             }\n      else if ( c == 'O' ){\n             key_value = 0x42;\n             single_upper(c);\n             }\n      else if ( c == 'P' ){\n             key_value = 0x4A;\n             single_upper(c);\n             }\n      else if ( c == 'Q' ){\n             key_value = 0x12;\n             single_upper(c);\n             }\n      else if ( c == 'R' ){\n             key_value = 0x2A;\n             single_upper(c);\n             }\n      else if ( c == 'S' ){\n             key_value = 0x1C;\n             single_upper(c);\n             }\n      else if ( c == 'T' ){\n             key_value = 0x2B;\n             single_upper(c);\n             }\n      else if ( c == 'U' ){\n             key_value = 0x32;\n             single_upper(c);\n             }\n      else if ( c == 'V' ){\n             key_value = 0x2E;\n             single_upper(c);\n             }\n      else if ( c == 'W' ){\n             key_value = 0x1A;\n             single_upper(c);\n             }\n      else if ( c == 'X' ){\n             key_value = 0x1E;\n             single_upper(c);\n             }\n      else if ( c == 'Y' ){\n             key_value = 0x33;\n             single_upper(c);\n             }\n      else if ( c == 'Z' ){\n             key_value = 0x16;\n             single_upper(c);\n             }\n      else if ( c == '\\n' ){\n//            printf(\"new line\\n\"); \n//write something so that an empty line equates to an enter maybe use get line\n             }\n\n      else\n           printf(\"Invalid input char\\n\");\n\n\n}\n\n\n    if ( MacroLength \n 700 )\n        printf(\"\\n\\n\\nWARNING #######  Macro length too long\\n\");\n    int i;\n    for(i = (700 - MacroLength) ; i \n 0 ; i--)\n        printf(\"%s\",zero_fill);\n     printf(\"AB\\n\");//fake checksum\n     printf(\"MacroLength=%d\\n\" , MacroLength);\n     printf(\"\\nzero fill bytes %d\\n\", (700 - MacroLength ));\n     printf(\"\\n\"); \nreturn 0;\n}", 
            "title": "Programmable keyboard"
        }, 
        {
            "location": "/puppet/puppet_study_notes/", 
            "text": "Puppet study notes January 2015\nSeamus Murray\n\n\nTodo.................................\n\n\nSeamus learn the +\n operator\n\n\ndoes site.pp override the values in an enc\ndoes the values in an enc override the values in a class\n\n\nYumrepo['custom_packages'] -\n Package \n| tag == 'custom' |\n\nWill create an order relationship with several package resources\n using puppet node to set password when the nodes cant contact the password server, do you use an exec resource ?\n\n\ndo you have to configure the agents to use heira or just the master\n\n\nwhat is a node_terminus where is it set and what overrides it\n  https://docs.puppetlabs.com/guides/external_nodes.html\n\n\nSeamus learn the spaceship operator\n\n\nThe puppet db has multiple terminus's where data is stored\n\n\n\n\nFacts\n\n\nCatalogs\n\n\nResources\n\n\n\n\nwatch the puppetdb talk by james sweanie puppetny\nhttps://www.youtube.com/watch?v=HTr4b02aU7A  at 14 mins\n\n\ndoes the values set in the console override enc or site.pp  (where are the values stored)\nwhat is the effect of node default {} in site.pp can this be overridden by ENC or by a specific node hostname {} in site.pp\n\n\nhttps://docs.puppetlabs.com/guides/external_nodes.html\n\n\nworkout resource interdependency loops\n\n\n............\n\n\nsources of information I used to study\n\n\n1) {book} pro puppet second edition \n\n\nWhilst it is quite possibly the worst technical book I've read, is it is still a worth while read. Many of the chapters seem to have been cut and pasted together in a rush. Don't bother trying to replicate the examples as many of them just don't work. Ignore the sections on version control and don't try to follow the examples in chapter 2. \n\n\nyou can get 10days access to the html version of the book for free from safaribooksonline\nhttps://www.safaribooksonline.com/library/view/pro-puppet-second/9781430260400/\n\n\npuppetlabs own documentation https://puppetlabs.com/learn\n\n\nsign up to the puppetlabs learning portal https://puppetlabs.com/learn\n\n\n2) go through the puppetlabs videos and questionnaires, it will give you an idea of the language used in the tests\n   https://puppetlabs.com/learn/library\n\n\nsome of the videos seem to be based on lessons that can be found elsewhere of the puppet website (sorry I cant find them now )\n\n\n3) puppetlabs provides a vm with puppet enterprise installed, \n   inside the zip file there is a pdf with a series of exercises\n   http://puppetlabs.com/download-learning-vm\n\n\n4) puppet practice exam\n\n\nhttps://docs.puppetlabs.com/references/glossary.html\n\n\n6) glossary of the key puppet terminology\n\n\nhttps://puppetlabs.com/services/certification/puppet-professional-practice-exam\n\n\n\n\n\n\n\n\n\n\n\n\nPuppet installation\n\n\nSystem requirements\n\n\nPuppet Enterprise 3.7 supports the following systems:\n\n\n+------------------+---------------------------+-------------+------------------------------------+\n|Operating system  |Version(s)                 | Arch        |Component(s)                        |\n|------------------|---------------------------|-------------|------------------------------------|\n|RHEL              |4, 5, 6, 7                 |x86_64       |all (RHEL 4 supports agent only)    |\n|CentOS            |4, 5, 6, 7                 |x86 \n x86_64 |all (CentOS 4 supports agent only)  |\n|Ubuntu LTS        |10.04, 12.04, 14.04        |i386 \n amd64 |all                                 |\n|Debian            |Squeeze (6), Wheezy (7)    |i386 \n amd64 |all                                 |\n|Oracle Linux      |4, 5, 6, 7                 |x86 \n x86_64 |all (Oracle Linux 4 agent only)     |\n|Scientific Linux  |4, 5, 6                    |x86 \n x86_64 |all (Scientific Linux 4 agent only) |\n|SUSE Enterprise   |10 (SP4 only), 11(SP1), 12 |x86 \n x86_64 |all (SLES 10 supports agent only)   |\n|Solaris           |10 (Update 9 or later) 11  |SPARC \n i386 |agent                               |\n|Microsoft Windows |2008 2008R2 7 Ultimate SP1 |x86 \n x64    |agent                               |\n|                  |8 Pro, 8.1 Pro 2012 2012R2 |x86 \n x64    |agent                               |\n|Microsoft Windows |2003, 2003R2               |x86          |agent                               |\n|AIX               |5.3, 6.1, 7.1              |Power        |agent                               |\n|Mac OS X          |Mavericks (10.9)           |x86_64       |agent                               |\n+------------------+---------------------------+-------------+------------------------------------+\n\n\n\nhttps://docs.puppetlabs.com/pe/latest/install_basic.html#about-puppet-enterprise-components\n   the puppet enterprise  3.1 guide refers to  puppet master role  database support role console role\n\n\nMonolithic (all-in-one) Installation\n\n\nMonolithic installs are suitable for deployments up to 500 nodes. We recommend that your hardware meets the following:\n\n\n\n\nThe puppet master, PE console, and PuppetDB node: at least 4-8 processor cores, 8 GB RAM\n\n\nAll machines require very accurate timekeeping\n\n\nPuppet agent nodes: any hardware able to run the supported operating system\n\n\nFor /var/, at least 1 GB of free space for each PE component on a given node\n\n\nFor PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering\n\n\nFor no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available\n\n\n\n\nSplit Installation\n\n\nFor larger deployments (500-1000, or more nodes), we recommend a split install. We recommend that your hardware meets the following:\n\n\n\n\nPuppet master, PE console, and PuppetDB nodes: at least 8 processor cores, 8 GB RAM (per node)\n\n\nAll machines require very accurate timekeeping\n\n\nPuppet agent nodes: any hardware able to run the supported operating system\n\n\nFor /var/, at least 1 GB of free space for each PE component on a given node\n\n\nFor PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering\n\n\nFor no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available\n\n\n\n\nFirewall Configuration\n\n\nAgent nodes contact the puppet master server on TCP ports 8140 (for Puppet) and 61613 (for orchestration).\nAny hosts that need access to the html GUI will contact the  console server on port 443(can be changed to a different port).\n\n\n+-----------------+---------+---------+---------+----------+\n|                 |  AGENT  | MASTER  | CONSOLE | ADMIN PC |\n|-----------------|---------|---------|---------|----------|\n|puppet           |   ----\n |   8140  |         |          |\n|Orchestration    | 61613--\n|\n--61613 |         |          |\n|HTML GUI         |         |         |   443   | \n----    |\n|Master Installer |         |   3000  |         | \n----    | #html installer\n+-----------------+---------+---------+---------+----------+\n\n\n\nFrom 3.1 installation guide... https://docs.puppetlabs.com/pe/3.1/install_system_requirements.html\n\n\nHardware requirements for the various puppet roles/components\n\n\nPuppet Enterprise\u2019s hardware requirements depend on the roles a machine performs.\n\n\n\n\n\n\nThe \npuppet master role\n should be installed on a robust, dedicated server.\n        Minimum requirements: 2 processor cores, 1 GB RAM, and very accurate timekeeping.\n        Recommended requirements: 2-4 processor cores, at least 4 GB RAM, and very accurate timekeeping.\n        Performance will vary, but this configuration can generally manage approximately 1,000 agent nodes.\n\n\n\n\n\n\nThe \ndatabase support role\n can be installed on the same server as the console or, optionally, on a separate, dedicated server.\n        Minimum requirements: These will vary considerably depending on the size of your deployment. \n        However, you\u2019ll need a machine able to handle moderate network traffic, perform processor-intensive background tasks, and run a disk-intensive PostgreSQL database server.\n        The machine should have two to four processor cores.\n        As a rough rule of thumb for RAM needed, start here: 1-500 nodes: 192-1024MB, 500-1000 nodes: 1-2GB, 1000-2000 nodes: 2-4 GB, 2000+ nodes, 4GB or greater.\n        So as your deployment scales, make sure to scale RAM allocations accordingly.\n        More information about scaling PuppetDB is available in the PuppetDB manual\u2019s scaling guidelines.\n\n\n\n\n\n\nThe \nconsole role\n should usually be installed on a separate server from the puppet master, but can optionally be installed on the same server in smaller deployments.\n        Minimum requirements: A machine able to handle moderate network traffic and perform processor-intensive background tasks.\n        It should have a very fast network connection to the database support server, which it uses for all of the console\u2019s database requirements.\n        Requirements will vary significantly depending on the size and complexity of your site.\n\n\n\n\n\n\nThe optional \ncloud provisioner role\n has very modest requirements.\n        Minimum requirements: A system which provides interactive shell access for trusted users.\n        This system should be kept very secure, as the cloud provisioning tools must be given cloud service account credentials in order to function.\n\n\n\n\n\n\nThe puppet \nagent role\n has very modest requirements.\n        Minimum requirements: Any hardware able to comfortably run a supported operating system.\n\n\n\n\n\n\nInstall script\n\n\n[root@master puppet-enterprise-3.7.1-el-6-x86_64]# ./puppet-enterprise-installer -h\n\n\nPuppet Enterprise v3.7.1 installer\n\n\nPuppet Enterprise documentation can be found at http://docs.puppetlabs.com/pe/3.7/\n\n\nUSAGE: puppet-enterprise-installer [-a ANSWER_FILE] [-A ANSWER_FILE] [-D] [-h] [-l LOG_FILE] [-n] [-q] [-s ANSWER_FILE] [-V]\n\n\nOPTIONS:\n\n\n\n\n-a ANSWER_FILE\n - Read answers from file and quit with error if an answer is missing.\n\n\n-A ANSWER_FILE\n - Read answers from file and prompt for input if an answer is missing.\n\n\n-D\n             - Display debugging information.\n\n\n-h\n             - Display this help.\n\n\n-l LOG_FILE\n    - Log commands and results to file.\n\n\n-n\n             - Run in 'noop' mode; show commands that would have been run during installation without running them\n\n\n-q\n             - Run in quiet mode; the installation process is not displayed. Requires answer file.\n\n\n-s ANSWER_FILE\n - Save answers to file and quit without installing.\n\n\n-V\n             - Display very verbose debugging information.\n\n\n\n\nInstall the pe-agent\n\n\ncurl -k https://\npuppet master server\n:8140/packages/current/install.bash | sudo bash\n\n\n\npuppet.conf\n\n\n/etc/puppetlabs/puppet/puppet.conf  {puppet enterprise}\nor\n/etc/puppet/puppet.conf  {puppet opensource}\n\n\n[main]\n    certname = learning.puppetlabs.vm\n    vardir = /var/opt/lib/pe-puppet\n    logdir = /var/log/pe-puppet\n    rundir = /var/run/pe-puppet\n    basemodulepath = /etc/puppetlabs/puppet/modules:/opt/puppet/share/puppet/modules\n    environmentpath = /etc/puppetlabs/puppet/environments\n    server = learning.puppetlabs.vm\n    user  = pe-puppet\n    group = pe-puppet\n    archive_files = true\n    archive_file_server = learning.puppetlabs.vm\n    module_groups = base+pe_only\n\n[agent]\n    report = true\n    classfile = $vardir/classes.txt\n    localconfig = $vardir/localconfig\n    graph = true\n    pluginsync = true\n    environment = production\n\n\n\nSSL certificates and accurate time\n\n\nIf there is a clock sync issue between an agent and the master you will receive certificate verification errors..\n\n\nroot@node3:~# puppet agent -t\nWarning: Unable to fetch my node definition, but the agent run will continue:\nWarning: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\n Info: Retrieving plugin\nError: /File[/var/lib/puppet/lib]: Failed to generate additional resources using 'eval_generate': SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\nError: /File[/var/lib/puppet/lib]: Could not evaluate: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Could not retrieve file metadata for puppet://puppet/plugins: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\nError: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\nWarning: Not using cache on failed catalog\n Error: Could not retrieve catalog; skipping run\nError: Could not send report: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\n\n\nresolution sync the clock's and restart puppet agent service or manually run puppet agent -t\n\n\nResetting a nodes certificate\n\n\n+--------------------------------+-----------------------------+\n|       PUPPET AGENT             |    PUPPET MASTER            |\n|--------------------------------|-----------------------------|\n|  service puppet stop           |                             |\n|  puppet config print ssldir    |                             |\n|  rm -rf  /var/lib/puppet/ssl   |                             |\n|                                | puppet cert list --all      |\n|                                | puppet cert clean \ncertname\n|\n|  puppet agent -t               |                             |\n|                                | puppet cert list            |\n|                                | puppet sign \ncertname\n      |\n|  puppet agent -t               |                             |\n|  service puppet start          |                             |\n+--------------------------------+-----------------------------+\n\n\n\nUntil the puppet master signs the nodes cert the agent will display the following message\n\n\nroot@node3:~# puppet agent -t\nExiting; no certificate found and waitforcert is disabled\n\n\n\nPuppet Cert command line options\n\n\npuppet cert  \naction\n  \nhost\n\n\n\n\n\n\n\n\nclean:\n\n  Revoke a host's certificate (if applicable) and remove all files\n  related to that host from puppet cert's storage. This is useful when\n  rebuilding hosts, since new certificate signing requests will only be\n  honored if puppet cert does not have a copy of a signed certificate\n  for that host. If '--all' is specified then all host certificates,\n  both signed and unsigned, will be removed.\n\n\n\n\n\n\nfingerprint:\n\n  Print the DIGEST (defaults to the signing algorithm) fingerprint of a\n  host's certificate.\n\n\n\n\n\n\ngenerate:\n\n  Generate a certificate for a named client. A certificate/keypair will\n  be generated for each client named on the command line.\n\n\n\n\n\n\nlist:\n\n  List outstanding certificate requests. If '--all' is specified, signed\n  certificates are also listed, prefixed by '+', and revoked or invalid\n  certificates are prefixed by '-' (the verification outcome is printed\n  in parenthesis).\n\n\n\n\n\n\nprint:\n\n  Print the full-text version of a host's certificate.\n\n\n\n\n\n\nrevoke:\n\n  Revoke the certificate of a client. The certificate can be specified either\n  by its serial number (given as a hexadecimal number prefixed by '0x') or by its\n  hostname. The certificate is revoked by adding it to the Certificate Revocation\n  List given by the 'cacrl' configuration option. Note that the puppet master\n  needs to be restarted after revoking certificates.\n\n\n\n\n\n\nsign:\n\n  Sign an outstanding certificate request.\n\n\n\n\n\n\nverify:\n\n  Verify the named certificate against the local CA certificate.\n\n\n\n\n\n\nreinventory:\n\n  Build an inventory of the issued certificates. This will destroy the current\n  inventory file specified by 'cert_inventory' and recreate it from the\n  certificates found in the 'certdir'. Ensure the puppet master is stopped\n  before running this action.\n\n\n\n\n\n\n\n\n\n\nNode Classification  site.pp ENC\n\n\nCertname\n\n\nBy default each nodes certification name is its FQDN.\nWhen the puppet agent on the nodes request at catalogue from the puppet master it does so using its current FQDN {even if they had already generated a certificate and had it signed by the master}. Therefore if the nodes host or domain name changes it will make a request based on the new name and you will have to sign a new cert on the master.\n\n\nClassification of nodes\n\n\nIn puppet the term classification  refers to the linking/assigning of classes to nodes. This can be achieved in many ways\n\n\n Using site.pp \n\n\nnode '\ngroup name\n' {\n  include \nclass\n\n}\n\nnode '\nnode name\n' {\n  include \nclass\n\n}\n\n\n\n Using the puppet enterprise console\n\n\n\n\nClick on Classification in the console navigation bar.\n    ( Optional steps to create a new group )\n\n\nEnter a new node group name\n\n\nclick Add group\n\n\n\n\n\n\nClick on the desired group and set the rules for this group based on facter values name/osfamily etc... click add rule\n\n\nClick commit change\n\n\n\n\nIf your changes to the site.pp manifest are'nt reflected in a puppet run triggered by the puppet agent -t command, try running the command again.\n\n\n Using an ENC External node Classifier \n\nAn executable that can be called by puppet master; it doesn\u2019t have to be written in Ruby. Its only argument is the name of the node to be classified, and it returns a YAML document describing the node.\n\n\nNode statements are an \noptional feature\n of Puppet. They can be replaced by or combined with an external node classifier. You can also use conditional statements upon facts to classify nodes.\n\n\n\n\n\n\nsite.pp syntax\n\n\nThe Default Node\n\n\nThe site.pp file has an optional default node declaration . It follows the same format as a normal node declaration except the word  default \n(without quotes)\n is used  in place of a cert/host name. If a node requests a catalogue and there is no matching cert/host name in the site.pp then the node will be assigned what ever settings have be set for the default node.\n\n\nYou can simply use the \nnode name\n:\n\n\nnode 'www.example.com' {\n  include common\n  include apache, squid\n}\n\n\n\nYou can use a \ncomma-separated list\n of names to create a group of nodes with a single node statement:\n\n\nnode 'www1.example.com', 'www2.example.com', 'www3.example.com' {\n  include common\n  include apache, squid\n}\n\n\n\nYou can use \nRegular expressions\n (regexes) can be used as node names. In puppet regexes must be surrounded by forward slashes /\n\n\nmatch any node with the name www followed by 1 or more digits (www1 , www12345)\n\n\nnode /^www\\d+$/ {\n  include common\n}\n\n\n\nmatch foo.example.com and bar.example.com\n\n\nnode /^(foo|bar)\\.example\\.com$/ {\n  include common\n}\n\n\n\nIf site.pp contains at least one node definition, it must have one for every node; compilation for a node will fail if one cannot be found. (Hence the usefulness of the default node.) If site.pp contains no node definitions, this requirement is dropped.\n\n\nMatching\n\n\nIf a nodes certname/hostname is matched by more than one node statement it will only get the contents of one node definition.\nPuppet will do the following checks in order when deciding which definition to use:\n\n\n\n\nIf there is a node definition with the node\u2019s exact name, Puppet will use it.\n\n\nIf there is a regular expression node statement that matches the node\u2019s name, Puppet will use it. (If more than one regex node matches, Puppet will use one of them, with no guarantee as to which.)\n\n\nIf the node\u2019s name looks like a fully qualified domain name (i.e. multiple period-separated groups of letters, numbers, underscores and dashes), Puppet will chop off the final group and start again at step 1. (That is, if a definition for www01.example.com isn\u2019t found, Puppet will look for a definition matching www01.example.)\n\n\nPuppet will use the default node.\n\n\n\n\nThus, for the node www1.example.com, Puppet would try the following, in order:\n\n\nwww1.example.com\nA regex that matches www1.example.com\nwww1.example\nA regex that matches www1.example\nwww1\nA regex that matches www1\ndefault\n\n\n\nIn my testing with a site.pp\n\n\nnode 'node2.puppetlabs' {\n  include hosts\n}\n\nnode /^node(2|4)/ {\n  include regexmod\n  include definedmod\n  include ntp\n}\n\n\n\nwith the node/cert name of node2.puppetlabs it is matched my the most accurate name ie \"node2.puppetlabs\"\n\n\nif the site.pp only has the shortname and a shortname+regex then the regex its matched\n\n\neg..\n\n\nnode 'node2' {\n  include hosts\n}\n\nnode /^node(2|4)/ {\n  include regexmod\n  include definedmod\n  include ntp\n}\n\n\n\nMerging site.pp With ENC Data\n\n\nNode definitions and external node classifiers can co-exist. Puppet merges their data as follows:\n\n\nVariables from an ENC are set at top scope and can thus be overridden by variables in a node definition.\nClasses from an ENC are declared at node scope, which means they will be affected by any variables set in the node definition.\n\n\nAlthough both will work together it is recommend to use  \neither\n node definitions or an ENC.\n\n\nExternal nodes override node configuration in the manifest files. If you enable an external node classifier, any duplicate node definitions in your manifest files will not be processed; they will be ignored by Puppet.\n\n\nThe use of LDAP nodes overrides node definitions in your manifest files and your ENC. If you use LDAP node definitions, you cannot define nodes in your manifest files or in an ENC.\n\n\nInheritance\n\n\nIn earlier versions of Puppet, nodes could inherit from other nodes using the inherits keyword. This feature is deprecated in Puppet 3.7,\n\n\nnode classification in puppet enterprise console\n\n\nclick the \nclassification\n tab\nselect the group or create a new group\nset the rules to match the node\n\n\n\n\nstatic matching\n, add the nodes FQDN in the certname field and pin the node to the group \n\n\ndynamic matching\n, create a \nfact + operator + value\n based rule to match the node, e.g.\n\n\n\n\nTable detailing possible matching rules\n\n\n+-----------+----------------+------------------------+\n|  FACT     | OPERATOR       |  VALUE or REGEX        |\n+-----------+----------------+------------------------+\n|  hostname |  is            |  pe-node2              |\n|  name     |  is            |  pe-node3.puppetlabs   |\n|  name     |  matches regex |  pe-node\\d.puppetlabs  |\n|  name     |  matches regex |  node(4|5).puppetlabs  |\n|  osfamily |  is            |  RedHat                |\n+-----------+----------------+------------------------+\n\n\n\nNote\n Don't forget to click \nadd Rule\n and then click \ncommit x change\n\n\ndefault node group\n\n\nPE comes preconfigured with the \ndefault node group\n.\nThis is the root of the node group hierarchy and is a parent to all other node groups. \nThe classes that are assigned to the default node group are applied to all of the nodes in your deployment. \n\n\nDeleting Nodes from puppet enterprise console\n\n\nThere are three options for deleting a node:\n\n\n\n\n\n\nHide Node\n\nRemoves the node from the node list view. To hide a node:\nOn the Nodes page, click the node.\nClick Hide.\n\n\n\n\n\n\n\nDelete Node\n\n\nRemoves all reports and information for the node from the console. The node no longer appears in the list of nodes on the Nodes page, but it continues to appear in Matching nodes until it is purged from PuppetDB. The node will reappear as a new node on the Nodes page if it submits a new Puppet run report. To delete a node:\n\n\nOn the Nodes page, click the node.\nClick Delete.\n\n\n\n\n\n\n\nDeactivate Node\n\n\nCompletely deactivates the node and frees up the license assigned to the node. Any deactivated node will be reactivated if PuppetDB receives new catalogues or facts for it. For details, see Deactivating a PE agent node. \n\n\n\n\n\n\nStop the agent service on the node you want to deactivate.\n\n\n\n\n\n\nOn the Puppet master, deactivate the agent; run \npuppet node deactivate {NODE NAME}\n.\n This deactivates the agent in PuppetDB. In some cases, the PE license count in the console will not decrease for up to 24 hours, but you can restart the pe-memcached service to update the license count sooner.\n\n\n\n\n\n\nOn the Puppet master, revoke the agent certificate; run \npuppet cert revoke {AGENT CERTNAME}\n.\n\n\n\n\n\n\nStill on the Puppet master, \nrun puppet agent -t\n to kick off a Puppet run.\n This Puppet run will copy the certificate revocation list (CRL) to the correct SSL directory for delivery to the agent.\n\n\n\n\n\n\nRestart the Puppet master with \nservice pe-puppetserver restart\n.\n The certificate is only revoked after running service pe-puppetserver restart. In addition, the Puppet server process won\u2019t re-read the certificate revocation list until the service is restarted. If you don\u2019t run service pe-puppetserver restart, the node will check in again on the next Puppet run and re-register with PuppetDB, which will increment the license count again.\n\n\n\n\n\n\nDelete the node from the console. In the console, click Nodes. Click the node that you want to delete and click the Delete button.\n This action does NOT disable MCollective/live management on the node.\n\n\n\n\n\n\nTo disable MCollective/live management on the node, uninstall the Puppet agent, stop the pe-mcollective service (on the agent, run service pe-mcollective stop), or destroy the agent altogether.\n On the agent, remove the node certificates in \n/etc/puppetlabs/mcollective/ssl/clients\n.\n\n\n\n\n\n\nclean the cert from the pauppet master ...\npuppet cert clean  {AGENT CERTNAME}\n\n\n\n\n\n\n\n\n\n\nNote: If you delete a node from the node view without first deactivating the node, the node will be absent from the node list in the console, but the license count will not decrease, and on the next Puppet run, the node will be listed in the console.\n\n\n\n\n\n\n\n\n\n\nAt this point, the node should be fully deactivated.\n\n\n\n\n\n\nRegex in puppet\n\n\nRegexes in Puppet cannot have options or encodings appended after the final slash.\nHowever, you may turn options on or off for portions of the expression using the (?\n:\n) and (?-\n:\n) notation.\nThe following example enables the \ni\n option while disabling the \nm\n and \nx\n options:\n\n\n $packages = $operatingsystem ? {\n   /(?i-mx:ubuntu|debian)/        =\n 'apache2',\n   /(?i-mx:centos|fedora|redhat)/ =\n 'httpd',\n }\n\n\n\nThe following options are allowed:\n\n\ni \u2014 Ignore case\nm \u2014 Treat a newline as a character matched by .\nx \u2014 Ignore white space and comments in the pattern\n\n\n\n\n\n\n\nPuppet run overview\n\n\n\n\nThe SSL checks are made first\n\n\nThe agent checks for a certificate matching its FQDN\n\n\nIf one is not found the agent generates one. \n\n\n\n\n\n\nThe agent checks for the CA cert.\n\n\nif not it will send its cert  to the CA servers and request for its cert to be signed.\n\n\n\n\n\n\nIf plugin sync is enabled, the agent checks the master for new plugins and downloads them if necessary.\n\n\nThe agent ask's Facter for a set of facts about itself. { facter -p }\n\n\nThe agent sends the facts the master whilst requesting a catalog.\n\n\nThe master injects those facts as variables in the root scope and processes the manifests.\n\n\nThe master then sends the catalog to the agent.\n\n\nThe agent apply's the catalog.\n\n\nIf reporting is enabled the agent sends a report back to the master.\n\n\n\n\nDiagram of puppet run\n\n\n+-----------------------+     +-----------------------+\n|        AGENT          |     |        MASTER         |\n|  +-----------------+  |     |                       |\n|  | Check local Cert|  |     |     +--------------+  |\n|  | create if needed|  |     | +--\n| Send CA Cert |  |\n|  +-----------------+  |     | |   +--------------+  |\n| +------------------+  |     | |                     |\n| | Check 4 copy of  |  |     | |                     |\n| | CA Cert,retrieve |  |     | |                     |\n| | if needed        |\n---------+                     |\n| +------------------+  |     |                       |\n| +-------------------+ |     |  +------------------+ |\n| | Is my Cert signed |\n--------\n| Sign Client Cert | |\n| +-------------------+ |     |  +------------------+ |\n|                       |     |                       |\n|  +-----------------+  |     |  +-----------------+  |\n|  | Request plugins |---------\n |  Send Plugins   |  |\n|  +-----------------+  |     |  +-----------------+  |\n|                       |     |           |           |\n|  +---------------- +  |     |           |           |\n|  | Import plugins  |\n-------------------+           |\n|  +-----------------+  |     |                       |\n|                       |     |                       |\n|  +-----------------+  |     |     +--------------+  |\n|  |                 |  |     |     |   CLASSIFY   |  |\n|  | REQUEST CATALOG +------------\n |The Node based|  |\n|  |  Send node name |  |     |     |on certname + |  |\n|  |  and facts to   |  |     |     | node groups  |  |\n|  |  the master     |  |     |     +--------------+  |\n|  |                 |  |     |     ______|______     |\n|  ------------------+  |     |    |      |      |    |\n|                       |     | +--V--++--V--++--V--+ |\n|                       |     | |CLASS||CLASS||CLASS| |\n|  +---------------+    |     | +-----++-----++-----+ |\n|  |               |    |     |      \\    |     /     |\n|  |    CATALOG    | \n-----------+  +-V---V----V-+    |\n|  |               |    |     |  |  |            |    |\n|  +--------+------+    |     |  |  |  COMPILE   |    |\n|           |           |     |  |  |            |    |\n|  +--------V---------+ |     |  |  +------------+    |\n|  |APPLY +--------+  | |     |  |        V           |\n|  |      | QUERY  |  | |     |  |        |           |\n|  |      | STATUS |  | |     |  +----\n---+           |\n|  |      +----|---+  | |     |                       |\n|  |    +------V----+ | |     |                       |\n|  |    |  ENFORCE  | | |     |                       |\n|  |    |  DEFINED  | | |     |                       |\n|  |    |  STATE    | | |     |                       |\n|  |    +-----------+ | |     |    +-------------+    |\n|  +----+-------------+ |     |    |             |    |\n|           |           |     |    |   REPORT    |    |\n|           |--------------------\n |             |    |\n|           |           |     |    +-------------+    |\n|  +--------V---------+ |     |                       |\n|  |  DEFINED SYSTEM  | |     |                       |\n|  |      STATE       | |     |                       |\n|  +------------------+ |     |                       |\n+-----------------------+     +-----------------------+\n\n\n\n\n\n\n\nResources ---}  Classes   ---}  Manifest\n\n\nManifests\n\n\nA puppet manifest is text file that contains puppet code and is appended by the .pp file extension.\n\n\nCatalog\n\n\nIn a standard agent + master puppet configuration, the agent never receives a copy of the modules, manifests, functions or variables.\nThe agent only receives the compiled catalogue of resources and relationships.\n\n\nIt is not recommended by puppetlabs, however you can send each node an entire copy of the manifests for the nodes the compile the catalog and apply it locally,\nThis may be desirable in situations where there is no direct network connectivity between the agent and master.\nWarning: each node will be able to see the entire manifest which may contain sensitive information relating to other nodes in your environment.\n\n\nModules\n\n\nModules are just directories and files with a predictable structure located in the module path.\nThe following commands will display the module path.\n\n\n# puppet agent --configprint modulepath\n# puppet config print modulepath\n/etc/puppet/modules:/usr/share/puppet/modules\n\n\n\nModules will often have a main class that shares the name of the module.\n\n\nmodule-name\n|-manifests/\n|-files/\n|-templates/\n|-lib/\n|-tests/\n|-spec/\n\n\n\nIf a class is defined in a module you can then declare that class in any manifest by name.\n\n\nAutoloading in Puppet means that your modules will be loaded by Puppet at compile time, as long as they follow a predictable structure.\n\n\npuppet _\\   compile  _\\   load\n run    /   catalog   /  modules\n\n\n\nExample module's\n\n\ntree /etc/puppetlabs/puppet/environments/production/modules/sshd\n\n/etc/puppetlabs/puppet/environments/production/modules/sshd\n\u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sshd_config\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp\n\n\n\nexample module with class includes\n\n\ncowsayings\n\u251c\u2500\u2500 manifests\n\u2502   \u251c\u2500\u2500 init.pp     -----\n   class cowsayings {\n\u2502   \u2502                          include cowsayings::cowsay\n\u2502   \u2502                          include cowsayings::fortune\n\u2502   \u2502                        }\n\u2502   \u2502\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 cowsay.pp   -----\n   class cowsayings::cowsay {\n\u2502   \u2502                          package { 'cowsay':\n\u2502   \u2502                          ensure =\n 'present',\n\u2502   \u2502                        }\n\u2502   \u2502\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 fortune.pp  -----\n   class cowsayings::fortune {\n\u2502                                package { 'fortune-mod':\n\u2502                                  ensure =\n 'present',\n\u2502                                }\n\u2502                            }\n\u2502\n\u2514\u2500\u2500 tests\n    \u2502\n    \u251c\u2500\u2500 init.pp    --\n   include cowsayings\n    \u2502\n    \u2502\n    \u251c\u2500\u2500 cowsay.pp  --\n   include cowsayings::cowsay\n    \u2502\n    \u2502\n    \u2514\u2500\u2500 fortune.pp --\n   include cowsayings::fortune\n\n\n\npuppet module command line\n\n\n#puppet module \naction\n\n\n\n\nACTIONS:\n\n\n\n\nbuild\n    -    Build a module release package.\n\n\nchanges\n  -    Show modified files of an installed module.\n\n\ngenerate\n -    Generate boilerplate for a new module.\n\n\ninstall\n   -   Install a module from the Puppet Forge or a release archive.\n\n\nlist\n   -   List installed modules\n\n\nsearch\n -      Search the Puppet Forge for a module.\n\n\nuninstall\n -   Uninstall a puppet module.\n\n\nupgrade\n -    Upgrade a puppet module.\n\n\n\n\nWhen uploading a module to the forge your must include a \n modulefile \n containing the modules  metadata (name, version, source, author, description, etc.) \n\n\n\n\n\n\nResources\n\n\nEach resource describes some aspect of a system and its state e.g....\n\n\n\n\na service that must be running\n\n\na package that must be installed\n\n\na user that must be configured\n\n\n\n\nA resource declaration is the puppet block of code that describes a resource.\nResource declarations are written the in the puppet DML Declarative Modelling Language.\n\n\nPuppet's DML is a declarative language rather than an imperative one. This means that instead of defining a process or set of commands, Puppet code describes (or declares) only the desired end state, and relies on built-in providers to deal with implementation.\n\n\nResource declaration syntax example\n\n\ntype { 'title':\n  attribute_1  =\n  value_1,\n  attribute_2  =\n  value_2,\n}\n\n\n\n\n\ntype and title must be unique for a node\n\n\nvalues must be alphanumeric (quote strings)\n\n\neach attribute + value pair must be followed by a comma\n\n\n\n\nIndividual resources are combined together to represent the desired system configuration.\n\n\nSimilar resources can be grouped into types.  such as the user type\n\n\nResource Abstraction Layer\n\n\n    +---------------------------------------+\n    | file | package | service  |  user     |  Resources\n    |---------------------------------------|\n    | Ruby | Apt     | Redhat   | Useradd   |  Providers\n    |      | Yum     | Launchd  | LDAP      |\n    |      | Gems    | SMF      | Netinfo   |\n    |      | Deb     | Debian   |           |\n    |      | RPM     |          |           |\n    +---------------------------------------+\n\n\n\nQuery a resource using the puppet command line tool\n\n\npuppet resource \ntype\n {display all the instances of the specified type}\npuppet resource package\npuppet resource host\npuppet resource user \npuppet resource \ntype\n \ntitle\n {display the details of a particular resources instance }\npuppet resource package apache\npuppet resource host example.com\npuppet resource user seamus\npuppet resource --type  {list all types}\npuppet describe \ntype\n {display a description of the type and its options}\n\n\n\nPuppet Apply\n\n\nYou can use the Puppet resource declaration syntax with the puppet apply tool to make quick changes to resources on the system.\n\n\nYou can either change the values directly from the command line using the syntax..\n\n\npuppet apply -e \"user { 'seamus': ensure =\n 'present', }\"\n\n\n\nOr you can open the resource declaration in vim with the syntax...\n\n\npuppet apply -e user seamus\n\n\n\nThe resource will be opened in vim. Make any desired changed and when you quit vim the new resource values will be applied.\n\n\n\n\n\n\nDefined Resource Type (aka \ndefined types\n or \ndefines\n)\n\n\nare types that can be evaluated multiple times using different parameters during declaration. Upon each new declaration with new parameters they act like a new resource type. \n\n\nSeamus sample module using defined resources\n[root@master modules]# tree definedmod\n\n\ndefinedmod\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 definedresource.pp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp\n\n\n\n[root@master modules]# cat definedmod/manifests/init.pp\n\n\nclass definedmod {\n  definedresource {'mydefinedresource':\n         var2 =\n 'twovar',\n         var1 =\n 'variable1',\n         var3 =\n 'thirdvariable',\n  }\n}\n\n\n\n[root@master modules]# cat definedmod/manifests/definedresource.pp\n\n\ndefine definedresource ($var1, $var2 = default2 , $var3 = default3) {\n\n  file { \"/tmp/${var1}\" :\n    ensure  =\n present,\n    content =\n \"this file name is not $var2 or $var3\",\n  }\n\n  file { \"/tmp/${var2}\" :\n    ensure  =\n present,\n    content =\n \"this file name is not $var1 or $var3\",\n  }\n\n}\n\n\n\n[root@master modules]# cat definedmod/tests/init.pp\n\n\ninclude definedmod\n\n[root@node4 ~]# cat /tmp/variable1 \nthis file name is not twovar or thirdvariable\n\n[root@node4 ~]# cat /tmp/twovar \nthis file name is not variable1 or thirdvariable\n\n[root@node4 ~]# cat /tmp/thirdvariable \nthis file name is not variable1 or twovar[root@node4 ~]#\n\n\n\n\n\n\n\nResource duplicates\n\n\nInstances of \ntype + title\n must be singleton/unique for each node, otherwise the nodes catalog will fail to compile.\n\n\nsample duplicate resource declarations:\n\n\nclass resourceduplicates {\n  user { 'user1': # duplicate title of the user resource below; \n                  # the nodes catalog will fail to compile.\n    ensure =\n 'present',\n    uid    =\n '1007',\n  }\n\n  user { 'user1':\n    ensure =\n 'present',\n    uid    =\n '1007', # duplicate UID's, puppet catalog will compile; \n  }                   # But the useradd provider will fail on the client.\n}\n\n\n\nsample error of duplicate resource:\n\n\n\n\nError: Could not retrieve catalogue from remote server: Error 400 on SERVER: Duplicate declaration: User[user2] is already declared in file\n\n\n\n\nResource refresh\n\n\nPuppet will refresh a service for each instance of a refresh, e.g if 3 files are being updated and each update triggers a refresh of the same service 3 separate refreshes will be triggered doe each of the file updates.\n\n\n# This is a test to see if puppet will restart a service multiple time or-\n# just once when multiple refresh events are triggered\nclass loop {\n\n   file { '/tmp/loop1' :\n     ensure  =\n present,\n     content =\n \"$::uptime_seconds \\n\", # the \"uptime_seconds\" is used here to ensure that\n   }                                    # the content of the file will change upon each run\n\n   file { '/tmp/loop2' :\n     ensure  =\n present,\n     content =\n \"$::uptime_seconds \\n\",\n   }\n\n   file { '/tmp/loop3' :\n     ensure  =\n present,\n     content =\n \"$::uptime_seconds \\n\",\n   }\n\n   service { 'crond' :\n     ensure =\n running,\n   }\n\n   File['/tmp/loop1'] ~\n Service['crond']\n   File['/tmp/loop2'] ~\n Service['crond']\n   File['/tmp/loop3'] ~\n Service['crond']\n\n}\n\n\n\nResource ordering and relationships with \nmetaparameters\n\n\nVia a mechanism called \nautorequires\n; For some of the built in resource types such as users and groups, Puppet is able to automatically determine the dependency relationships among some of the built-in resources such as user and groups  without a user/admin having to declare the resource interdependencies.\n\n\nFor the majority of resources there is no autorequire and therefore the order of resources in a puppet manifest is generally ignored.\nIf a group of resources have interdependencies you should use one of the metaparameters to specify the relationships between them\n\n\n\n\nbefore\n    - Causes a resource to be applied before the target resource\n\n\nrequire\n   - Causes a resource to be applied after the target resource\n\n\nnotify\n    - Causes a resource to be applied before the target resource.\n              The target resource will refresh if the notifying resource changes.\n\n\nsubscribe\n - Causes a resource to be applied after the target resource.\n              The subscribing resource will refresh if the target resource changes.\n\n\n\n\nNote: when creating a resource reference use the syntax\n\n\nType['title']\n\n\n\nNote: Use lower-case for the type name when declaring it but use a capitalised type name when calling it.\n\n\nmetaparameter declaration examples...\n\n\nbefore\n\n\n  package { 'openssh-server':\n    ensure =\n present,\n    before =\n File['/etc/ssh/sshd_config'],\n  }\n\n\n\nrequire\n\n\n  file { '/etc/ssh/sshd_config':\n    ensure  =\n file,\n    mode    =\n 600,\n    source  =\n 'puppet:///modules/sshd/sshd_config',\n    require =\n Package['openssh-server'],\n  }\n\n\n\nOrdering Arrows\n\n\n Package['openssh-server'] -\n File['/etc/ssh/sshd_config']\n\n\n\nCauses the resource on the left to be applied before the resource on the right.\n\n\nnotify\n\n\n file { '/etc/ssh/sshd_config':\n   ensure  =\n file,\n   mode    =\n 600,\n   source  =\n 'puppet:///modules/sshd/sshd_config',\n   notify  =\n Service['sshd'],\n }\n\n\n\nsubscribe\n\n\n service { 'sshd':\n   ensure    =\n running,\n   enable    =\n true,\n   subscribe =\n File['/etc/ssh/sshd_config'],\n }\n\n\n\nNotification Arrow\n\n\n File['/etc/ntp.conf'] ~\n Service['ntpd']\n\n\n\nCauses the resource on the left to be applied first and sends a refresh to the resource on the right if the resource on the left changes.\n\n\nComplete example\n\n\nclass sshd {\n\n  package { 'openssh-server':\n    ensure =\n present,\n    before =\n File['/etc/ssh/sshd_config'],\n  }\n\n  file { '/etc/ssh/sshd_config':\n    ensure =\n file,\n    mode   =\n '0600',\n    source =\n 'puppet:///modules/sshd/sshd_config',\n  }\n}\n\n  service { 'ssh':\n    ensure    =\n running,\n    enable    =\n true,\n    subscribe =\n File['/etc/ssh/sshd_config'],\n}\n\n\n\nextra metaparameters..\n\n\nManage dependencies (before, require, subscribe, notify, stage)\n\n\nManage resources' application policies (audit, noop, schedule, loglevel)\n\n\nAdd information to a resource (alias, tag)\n\n\nVirtual resources\n\n\nare a hack to get around puppets limitation of sigleton resource definitions. They enable an a particular instance of a resource type+title to be used in multiple classes. \n\n\nThey are defined as virtual by pre-pending them with the \"@\" symbol\n\n\n@user { seamus: ensure =\n present }\n\n\n\nThey can then be declared in multiple classes by using one of the following 3 syntaxes using either the realise function or the spaceship collector.\n\n\nUser \n| title == seamus |\n\nrealize User[seamus]\nrealize(User[seamus])\n\n\n\nExported resources\n\n\nEnable a resource that is defined on  a host to be exported to the puppetdb and then be used by 1 or more other hosts. Puppet collects and stores the exported resources during configuration runs.\n\n\n/etc/hosts example\n\n\n[root@master modules]# cat exportedresource/manifests/init.pp\n\n\nclass exportedresource {\n  @@host { $hostname:  ## sends the local nodes host resource to the puppetdb and \n                       ## tags it as exported\n    ip           =\n $::ipaddress,\n    name         =\n $::fqdn,\n    host_aliases =\n $::hostname,\n    comment      =\n 'addded by the exported resource class',\n   }\n  Host \n| |\n  ## retrieves all the host resources from the puppetdb\n}\n\n\n\nSSH example\n\n\nclass ssh::hostkeys {\n\n  @@sshkey { \"${::fqdn}_dsa\":\n    host_aliases =\n [ $::fqdn, $::hostname, $::ipaddress ],\n    type         =\n dsa,\n    key          =\n $::sshdsakey,\n  }\n\n  @@sshkey { \"${::fqdn}_rsa\":\n    host_aliases =\n [ $::fqdn, $::hostname, $::ipaddress ],\n    type         =\n rsa,\n    key          =\n $::sshrsakey,\n  }\n\n}\n\n\n\nSSH example of retrieving the exported resources and applying them\n\n\nclass ssh::knownhosts {\n  Sshkey \n| |\n { ensure =\n present }\n}\n\n\n\nThe ssh::knownhosts class should be included in the catalog for all nodes where Puppet should manage the SSH known_hosts file.\nNotice that we\u2019ve used double angle braces to collect resources from PuppetDB.\n\n\ndefine balancermember($url) {\n  file { '/etc/httpd/conf.d.members/worker_${name}.conf':\n    ensure  =\n file,\n    owner   =\n 'root',\n    group   =\n 'root',\n    mode    =\n '0644',\n    content =\n \"BalancerMember $url \\n\",\n  }\n}\n\n\n\nThis configuration file fragment contains a single line, the URL to a member of the load balancer pool.\nPuppet recommends using a defined resource type because all resources declared within the type will be exported when the defined type itself is exported.\n\n\nclass loadbalancer_members {\n   Balancermember \n| |\n { notify =\n Service['apache'] }\n}\n\n\n\nThis code uses the double angle brace syntax to collect all balancermember resources from the stored configuration database.   In addition, it uses a parameter block to notify the Apache service of any changes Puppet makes to the balancermember resources. Just as with virtual resources, a parameter block may be specified to add further parameters to collected resources.\n\n\nRemoving retired nodes from PuppetDB\n\n\n# puppet node deactivate mail.example.com\nSubmitted 'deactivate node' for \nnode_name\n with UUID aaaaa-bbbbb-ccccc-ddddd-eeee\n\n\n\nAfter you\u2019ve run this on the puppet master, any resources exported by this node will no longer be collected on your Puppet clients.\nNote:  Deactivated node resources will not be removed from the systems that collected them.\nYou will need to clean up those configurations manually; or some resources can be purged using the resource metatype.\n\n\nExec Resource\n\n\nExec resources execute external commands, it is important that any commands executed are Idempotent\n\n\nAny command in an  exec resource must be able to be run multiple times without casing harm.\nThere are 3 main ways for commands in exec resources to be idempotent\n1. The commands them selves can be idempotent\n2. You use an (onlyif, unless, or creates) attribute, which will prevent puppet from running a specific command unless a specific condition is met.\n3. The exec resource has the ( refreshonly=\ntrue ) value, which only allows puppet to run the command when some other resource is changed\n\n\nsyntax\n\n\n exec { 'resource-title':\n   attribute_1 =\n value_1,\n   attribute_2 =\n value_2,\n }\n\n\n\nexample\n\n\nexec { \"updatedb':\n  path    =\n '/usr/sbin',\n  command =\n 'updatedb',\n}\n\n\n\nNote: if you do not set the command value it will default to the exec resource title. You must specifiy the path because the exec resource does not inherit paths.\n\n\n\n\n\n\nClasses\n\n\nClasses define a collection of resources that are managed together as a single unit. You can also think of them as named blocks of Puppet code, which are created in one place and invoked elsewhere.\n\n\nUsing a Puppet class requires two steps.\n\n\n\n\nFirst, you'l need to define it by writing a class definition and saving it in a manifest file. When Puppet runs, it will parse this manifest and store your class definition.\n\n\nSecondly, The class can then be declared to apply it to nodes in your infrastructure.\n\n\n\n\nexample class for ssh that contains 3 resources { package, file, service}..\n\n\nresource     class\n\npackage  \\\nfile      \n-  ssh\nservice  /\n\n\n\nExample class syntax....{with no relationships defined}\n\n\nclass ssh {\n  package  { 'openssh-clients':\n    ensure  =\n present,\n  }\n\n  file { '/etc/ssh/ssh_config':\n    ensure  =\n file,\n    owner   =\n 'root',\n    group   =\n 'root',\n    source  =\n 'puppet:///modules/ssh/ssh_config',\n  }\n\n  service { 'sshd':\n    ensure  =\n running,\n    enable  =\n true,\n  }\n}\n\n\n\nDefining a class specifies the contents and behaviour but does not automatically include (apply) it in a configuration\n\n\nDeclaring a class, includes the class in the catalogue which will then be applied upon next agent run.\n\n\nTo declare a class use either of the following syntaxes in site.pp.. or in \n/tests/init.pp\n\n\n   include my_calss \n\nor\n \n   class { 'my_class': } \n\n\n\n\nclasses are reusable\n\n\nclasses are singleton\n\n\nclasses can only be used once per node\n\n\n\n\nWhen applying a class make sure you run\n\n\npuppet apply module/tests/init.pp\n\n\n\nand NOT\n\n\npuppet apply module/manifest/init.pp\n\n\n\notherwise no actions will take place and no errors will be logged, this is because you would be defining the class but not declared it anywhere (on a  node).\n\n\nParameterised Classes\n\n\nClass parameters provide a method to set variables in a class as it's declared.\nThe syntax for parameterised classes is similar to resource declarations.\n\n\nclass { 'ntp':\n  servers =\n\n  ['node1.example.com','node2.example.com','node3.example.com']\n}\n\n\n\nThe servers parameter can be populated with a single server or an array of servers.\n\n\nParameterised class definitions can be set in the site.pp file.\n\n\nnode default {\n  # This is where you can declare classes for all nodes.\n  # Example:\n  #   class { 'my_class': }\n  class { 'ntp':\n    servers =\n\n    ['node1.example.com','node2.example.com','node3.example.com']\n  }\n}\n\n\n\nYou can also override values with\n\n\nclass { '::mysql::server':\n  override_options =\n { 'mysqld' =\n { 'max_connections' =\n '1024' } },\n}\n\n\n\nClass Inheritance / Derived Classes\n\n\nBy using the inherits keyword classes can be derived from other classes.\n\n\nWhen a derived class is declared, its base class is automatically declared first and the variable are set as the parent scope.\nThe new class receives a copy of all the base class's variables and resource defaults.\nCode in the derived class is able to override any resource attributes that were originally set in the base class.\n\n\nClass Inheritance is only useful for overriding resource attributes. For any other use case it is better archive your objective by using  some other method.\n\n\nexample...\n\n\nWe create a new class zsh::developer that will enable us to deploy the zshrc.dev file instead of the zshrc file on certain nodes..\n\n\n[root@learn modules]# tree zsh/\nzsh/\n\u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 zshrc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zshrc.dev\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 developer.pp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u251c\u2500\u2500 Modulefile\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec_helper.rb\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 developer.pp\n    \u2514\u2500\u2500 init.pp\n\n\n\ncat zsh/manifests/init.pp\n\n\nclass zsh {\n  package { 'zsh':\n    ensure =\n present,\n    before =\n File['/etc/zshrc'],\n  }\n  file { '/etc/zshrc':\n    ensure =\n file,\n    owner  =\n 'root',\n    group  =\n 'root',\n    source =\n 'puppet:///modules/zsh/zshrc',\n  }\n}\n\n\n\n[root@learn modules]# cat zsh/manifests/developer.pp\n\n\n#Note there is no need for a \"package { 'zsh':\" resource definition here as it will be inherited from the zsh class\nclass zsh::developer inherits zsh {\n  File ['/etc/zshrc'] {\n    source =\n 'puppet:///modules/zsh/zshrc.dev',\n    }\n}\n\n\n\n[root@learn modules]# cat zsh/tests/init.pp\n\n\ninclude zsh\n\n\n\n[root@learn modules]# cat zsh/tests/developer.pp\n\n\ninclude zsh::developer\n\n\n\nVariables and Class  Parameters\n\n\nVariables\n are prefixed with the $ sign and assigned with the = operator\n\n\n$mystring = 'this is my string'\n$mypath   = '/tmp/puppet/'\n\n\n\nOnce a variable it is defined it can be used anywhere in the manifest in place of an regular assigned value.\n\n\n\n\n\n\nUnlike resource declarations, variable assignments are parse-order dependent. This means that you must assign a variable in your manifest before you can use it.\n\n\n\n\n\n\nIf you try to use a variable that has not been defined, the Puppet parser won't complain. Instead, Puppet will treat the variable as having the special undef value.\n\n\n\n\n\n\nYou can only assign a variable once within a single scope. Once it's assigned, the value cannot be changed. \n(therefore it should be called a constant but hey its not like the puppet people actually follow best practices or common conventions anywhere else so why would this be any different)\n\n\n\n\n\n\nVariable interpolation\n enables a string that is stored as a variable to be inserted into another string. eg..\n\n\nfile { \"${mypath}file1.txt\":\n  ...\n}\nfile { \"${mypath}file2.txt\":\n  ...\n}\n\n\n\nNote:\n A string that includes an interpolated variable must be wrapped in double quotation marks (\"...\"), rather than the single quotation marks that surround an ordinary string\n\n\nClass Parameters\n provide a method of setting the variables within a class when the class is declared rather than when it is defined.\n\n\nOnce defined a \nParameterised Class\n can be declared with a similar syntax to a resource declaration\n\n\nWhen defining a class, include a list of parameters and optional default values between the class name and the opening curly brace. So a parameterised class is defined as below:\n\n\nclass classname ( $parameter = 'default' ) {\n  ...\n }\n\n\n\nOnce defined, a parameterised class can be declared with a syntax similar to that of resource declarations, including key value pairs for each parameter you want to set.\n\n\nclass {'classname':\n  parameter =\n 'value',\n}\n\n\n\nSeamus sample module using defined resource type class with paramaters (as described in the \"defined resource section above\"\n\n\n[root@master modules]# tree definedmod\n\n\ndefinedmod\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 definedresource.pp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp\n\n\n\n[root@master modules]# cat definedmod/manifests/init.pp\n\n\nclass definedmod {\n  definedresource {'mydefinedresource':\n         var2 =\n 'twovar',\n         var1 =\n 'variable1',\n         var3 =\n 'thirdvariable',\n  }\n}\n\n\n\n[root@master modules]# cat definedmod/manifests/definedresource.pp\n\n\ndefine definedresource ($var1, $var2 = default2 , $var3 = default3) {\n\n  file { \"/tmp/${var1}\" :\n    ensure  =\n present,\n    content =\n \"this file name is not $var2 or $var3\",\n  }\n\n  file { \"/tmp/${var2}\" :\n    ensure  =\n present,\n    content =\n \"this file name is not $var1 or $var3\",\n  }\n\n}\n\n\n\n[root@master modules]# cat definedmod/tests/init.pp\n\n\ninclude definedmod\n\n[root@node4 ~]# cat /tmp/variable1 \nthis file name is not twovar or thirdvariable\n\n[root@node4 ~]# cat /tmp/twovar \nthis file name is not variable1 or thirdvariable\n\n[root@node4 ~]# cat /tmp/thirdvariable \nthis file name is not variable1 or twovar[root@node4 ~]#\n\n\n\n\n\n\n\npuppet-lint\n\n\nA 3rd party tool used to display common syntax and style errors. Packaged as a ruby gem.\n\n\ngem install puppet-lint\n\npuppet-lint /etc/puppetlabs/puppet/modules/ntplint/manifests/init.pp\n\n\n\nyou can ignore certain syntax checks\n\n\non command line\n\n\npuppet-lint  \npath\n/\nfile\n.pp --no-80chars-check\npuppet-lint  \npath\n/\nfile\n.pp --no-ensure_first_param-check\n\n\n\n~/.puppet-lint.rc\n\n\n--no-80chars-check\n--no-ensure_first_param-check\n\n\n\nusing a rake file to test multiple manifests in one go by including the following values in your rake file\n\n\nrequire 'puppet-lint/tasks/puppet-lint'\nPuppetLint.configuration.send(\"disable_\ncheck\n\")\n\n\n\nIdempotency\n\n\nBy default puppet modules describe the desired final state rather than detail a series of step to follow, this mean that no matter how many times they are run the same end state will occur.\n\n\n\n\n\n\nPuppet Console\n\n\nThe puppet enterprise console is a web-based gui. It can help..\n\n\n\n\nmanage node requests to join the puppet deployment\n\n\nassign puppet classes to nodes and groups\n\n\nview reports and activity graphs\n\n\nbrowse and compare resources on your nodes\n\n\nview inventory data\n\n\nmanage console users and their access privileges\n\n\n\n\nEvent Inspector\n\n\nThe Event Inspector is part of the Puppet Enterprise (PE) Console. \nIt is a reporting tool that provides data for investigating the current state of your infrastructure. \nIts focus is on correlating information and presenting it from multiple perspectives, in order to reveal common causes behind related events.\nEvent inspector lets you accomplish two important tasks: monitoring a summary of your infrastructure\u2019s activity and analyzing the details of important changes and failures.\nIt displays events from three perspectives Classes Nodes and Resources\n\n\nResetting the admin user password\n\n\nThis must be performed from a shell session on the console server and executing a ruby script that was part of the Puppet Enterprise installer.\n\n\n\n\nq_puppet_enterpriseconsole_auth_password=newpassword \nq_puppetagent_certname=$(puppet config print certname) \n/opt/puppet/bin/ruby update-superuser-password.rb\n\n\n\n\nEnvironments\n\n\nBy default, all nodes are assigned to a default environment named production.\n\n\nThere are three ways to assign nodes to a different environment:\n\n\n\n\nVia your ENC or node terminus\n\n\nVia each agent node\u2019s puppet.conf\n\n\nVia the PE console to set the environment for each node group.\n\n\n\n\nAssigning Environments Via an ENC\n\n\nThe interface to set the environment for a node will be different for each ENC. Some ENCs cannot manage environments.\n\n\nWhen writing an ENC, simply ensure that the environment: key is set in the YAML output that the ENC returns. See the documentation on writing ENCs for details.\n\n\nIf the environment key isn\u2019t set in the ENC\u2019s YAML output, the Puppet master will just use the environment requested by the agent.\nAssigning Environments Via the Agent\u2019s Config File\n\n\nAssigning Environment via agent config\n\n\nIn puppet.conf on each agent node, you can set the environment setting in either the agent or main config section.\nWhen that node requests a catalog from the Puppet master, it will request that environment.\nNote: If you are using an ENC and it specifies an environment for that node, it will override whatever is in the config file.\n\n\nAssigning Environments via PE console\n\nClick Classification\n   - select node group\n       - click \"edit node group metadata\"\n          - select enviroment\n\n\n\n\n\n\nScope\n\n\nScope helps to organise classes, telling Puppet where to look within the module directory structure to find each class. It also separates namespaces within the module and your Puppet manifests, preventing conflicts between variables or classes with the same name.\n\n\nThe init.pp in the \n/manifest/ directory must contain a class with the same name as the module name\n\n\nClass name   =  mysql\nFile location =  /modules/mysql/manifests/init.pp    )\n\n\nClass name   =  mysql::server\nFile location = /modules/mysql/manifests/server.pp\n\n\nClass name    =  mysql::server::account_security\nFile location = /modules/mysql/manifests/server/account_security.pp\n\n\n\n\n\n\nTypes and Providers\n\n\nA \ntype\n defines the interface for a resource: the set of properties you can use to define a desired state for the resource, and the parameters that don't directly map to things on the system, but tell Puppet how to manage the resource.\nBoth properties and parameters appear in the resource declaration syntax as attribute value pairs.\n\n\ncommand to show the types available\n\n\n[root@master ~]# puppet resource --type\n\n+-----------------------------------------------------------------------------+\n| anchor            |  mcx                        |   puppetdb_conn_validator |\n| apt_key           |  mount                      |   resources               |\n| augeas            |  nagios_command             |   router                  |\n| computer          |  nagios_contact             |   schedule                |\n| cron              |  nagios_contactgroup        |   scheduled_task          |\n| exec              |  nagios_host                |   selboolean              |\n| file              |  nagios_hostdependency      |   selmodule               |\n| file_line         |  nagios_hostescalation      |   service                 |\n| filebucket        |  nagios_hostextinfo         |   ssh_authorized_key      |\n| firewall          |  nagios_hostgroup           |   sshkey                  |\n| firewallchain     |  nagios_ser^ice             |   stage                   |\n| group             |  nagios_ser|icedependency   |   tidy                    |\n| host              |  nagios_ser|iceescalation   |   user                    |\n| ini_setting       |  nagios_ser|iceextinfo      |   vlan                    |\n| ini_subsetting    |  nagios_servicegroup        |   whit                    |\n| interface         |  nagios_timeperiod          |   yumrepo                 |\n| k5login           |  notify                     |   zfs                     |\n| macauthorization  |  package                    |   zone                    |\n| mailalias         |  postgresql_conf            |   zpool                   |\n| maillist          |  postgresql_psql            |                           |\n+-----------------------------------------------------------------------------+\n\n\n\nCore Types\n\n\nFile\n\n\nmanages local files and ensures if a file should exist sets its parameters\npresent / absent\nfile / directory / link\nsource puppet:///modules/\n\n\nPackage\n\n\nManages software packages\nensures that a package is present / absent / latest /purged / version\n\n\nService\n\n\nManages services\nensures running / stopped\nenable true / false\n\n\nnotify\n\n\necho's message to the agent run-time log\n message =\n \" hello world ! \",\n\n\nexec\n\n\nexecutes an arbitrary command on the agent node.\n\n\ncron\n\n\nmanages cron jobs\n\n\nuser\n\n\nmanages user accounts\nensure present / absent / role\nname, uid, gid, groups,  home, shell\n\n\ngroup\n\n\nensure present / absent\nname, gid\nA \nprovider\n is what does the heavy lifting to bring the system into line with the state defined by a resource declaration.\nProviders are implemented for a wide variety of supported operating systems.\nThey are a key component of the Resource Abstraction Layer (RAL), translating the universal interface defined by the type into system-specific implementations.\n\n\nResource Abstraction Layer\n\n\n   +---------------------------------------+\n   | file | package | service  |  user     |  Resources\n   |---------------------------------------|\n   | Ruby | Apt     | Redhat   | Useradd   |  Providers\n   |      | Yum     | Launchd  | LDAP      |\n   |      | Gems    | SMF      | Netinfo   |\n   |      | Deb     | Debian   |           |\n   |      | RPM     |          |           |\n   +---------------------------------------+\n\n\n\n\n\n\n\nConditional Statements\n\n\nNote: \n string matches are not case sensitive\n\n\nif   (optional elseif and else)\n\n\nclass accounts ($name) {\n\n  if $::operatingsystem == 'centos' {\n    $groups = 'wheel'\n  }\n  elsif $::operatingsystem == 'debian' {\n    $groups = 'admin'\n  }\n  else {\n    fail( \"This module doesn't support ${::operatingsystem}.\" )\n  }\n\n  notice ( \"Groups for user ${name} set to ${groups}\" )\n\n  ...\n\n}\n\n\n\nunless\n\n\nunless $memorysize \n 1024 {\n  $maxclient = 500\n }\n\n\n\ncase\n\n\ncase $::operatingsystem {\n  'CentOS': { $apache_pkg = 'httpd' }\n  'Redhat': { $apache_pkg = 'httpd' }\n  'Debian': { $apache_pkg = 'apache2' }\n  'Ubuntu': { $apache_pkg = 'apache2' }\n  default: { fail(\"Unrecognised operating system for webserver.\") }\n}\n\npackage { $apache_pkg :\n  ensure =\n present,\n}\n\n\n\nselector\n\n\nSelector statements are similar to case statements, but return a value instead of executing a code block.\nSelectors can only be used at places in the code where a plain value is expected an not inside another selector or case statement.\n\n\n$rootgroup = $::osfamily ? {\n  'Solaris'  =\n 'wheel',\n  'Darwin'   =\n 'wheel',\n  'FreeBSD'  =\n 'wheel',\n  'default'  =\n 'root',\n}\n\n\n\nmy sample class with all the selectors\n\n\n[root@pe-puppet modules]# cat  noticemod/manifests/init.pp \n\n\nclass noticemod {\n  notify { \"Hello World!\": }\n\n## SELECTOR statement\n  notify { \" SELECTOR warning !\":\n    message =\n $seamus ? {\n      'mytrue' =\n \" the value is true.\",\n      'myfalse' =\n \" the value is false.\",\n      default =\n \" the value is not set.\",\n      }, # selector statements warn you if no match is found and there is no default\n  }\n\n## CASE statement\n  case $seamus {\n    'mytrue':   { notify { \" CASE warning ! the value is true.\": } }\n    'myfalse':  { notify { \" CASE warning ! the value is false.\": } }\n    default: { fail(\" CASE warning ! failing now... if the variable is undef the whole catalog compile will fail\") }\n  } # case statements silently fall through the bottom if no match is found\n\n\n## IF statement \n  if 'rue' in $seamus {\n      notify { \" IF warning ! the value is true.\": } \n  }\n  elsif 'alse' in $seamus {\n      notify { \" IF  warning ! the value is false.\": } \n  }\n  else {\n      notify { \" IF  warning ! the value does not match.\": }\n  }\n\n\n## UNLESS statement\n\n  unless $seamus == 'myfalse' {\n    notify { \" UNLESS statement - \\$seamus does not == myfalse, it is set to $seamus \": } \n  }\n\n  unless $seamus == 'mytrue' {\n    notify { \" UNLESS statement - \\$seamus does not == mytrue, it is set to $seamus \": } \n  }\n\n\n}\n\n\n\n\n\n\n\nPuppetDB\n\n\n\n\nstores the most recent facts from every node\n\n\nthe most recent catalog from every node\n\n\n(optionally) 7 days of event reports from every node\n\n\n\n\nit is searchable using either...\n\n\n\n\nPuppetDB's query API\n\n\nPuppet's inventory service\n\n\nexported resources\n\n\n\n\nexample\n\n\n[root@learning ~]# puppet node status learning.puppetlabs.vm\nlearning.puppetlabs.vm\nCurrently active\nLast catalog: 2015-01-24T13:31:16.291Z\nLast facts: 2015-01-24T13:31:03.782Z\n\n\nDeclaring an exported resource causes that resource to be added to the catalog and marked with an \u201cexported\u201d flag, which prevents the puppet agent from managing the resource. When PuppetDB receives the catalog, it stores a record for each resource with the flag set.\n\n\nDelete reports older then 1 month :\n\n\nsudo -u puppet-dashboard rake RAILS_ENV=production reports:prune upto=1 unit=mon\n\n\n\n\n\n\n\nHiera\n\n\nhttps://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/\n\n\nHiera is a key/value lookup tool for configuration data, built to make puppet better and let you set node-specific data without repeating yourself.\nHiera support is built into puppet 3, and is available as an add-on for puppet 2.7.\n\n\nIt keeps site specific data out of the manifest.\nPuppet classes can query any data they need and hiera will act as a site wide config file.\n\n\nHiera makes it easier to:\n1. separate configuration data from the modules code.\n2. configure your own nodes: default data with multiple levels of overrides.\n3. re-use public puppet modules: you don't have to edit the code you just have to put the necessary data in hiera.\n4. makes it easier to publish your own developed modules to the forge without clashing variable names or having to redact your config values.\n5. create common data for most nodes and..\n         - Override some values for machines located at a particular facility\n         - Override some values for specific machines\n6. enables you to only write down the differences that are needed (doesn't make sense)\n\n\nTo get started with hiera there are 5 steps..\n1) download and install hiera\n2) create a hiera.yaml config file\n3) arrange a hierachy that suite your site and data\n4) write your data sources\n5) configure puppet to use hiera\n\n\nhttps://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/\nhiera - Performs a standard priority lookup and returns the most specific value for a given key. The returned value can be data of any type (strings, arrays, or hashes).\n\n\nhiera_array - Returns all matches throughout the hierarchy \u2014 not just the first match \u2014 as a flattened array of unique values. If any of the matched values are arrays, they\u2019re flattened and included in the results.\n\n\nhiera_hash - Returns a merged hash of matches from throughout the hierarchy. In cases where two or more hashes share keys, the hierarchy order determines which key/value pair will be used in the returned hash, with the pair in the highest priority data source winning.\n\n\nexample of using hiera....\n\n\n[root@learn ~]# vim  /etc/puppetlabs/puppet/hieradata/common.yaml\n\n\n---\nmessage: This string is the value that is returned when hiera('message') is called.\nmotd:  Hello there Seamus\n\n\n\ntest that the key is set correctly using the puppet apply -e\n\n\n[root@learn hieradata]# puppet apply -e 'notice(hiera(\"motd\"))'\nNotice: Scope(Class[main]): Hello there Seamus\nNotice: Compiled catalog for learn.puppetlabs.com in environment production in 0.04 seconds\nNotice: Finished catalog run in 0.02 seconds\n\n\n\nEdit the motd module to use the key from hiera instead of the value in the module files\n\n\n[root@learn hieradata]# cd /etc/puppetlabs/puppet/modules/\n[root@learn modules]# tree motd/\nmotd/\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u251c\u2500\u2500 Modulefile\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec_helper.rb\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp\n\n\n\n[root@learn modules]# vim motd/manifests/init.pp\n\n\n  Class: motd\n\nclass motd {\n\n  file { '/etc/motd':\n    ensure  =\n file,\n    owner   =\n 'root',\n    group   =\n 'root',\n    content =\n  (hiera(\"motd\")),\n  }\n\n}\n\n\n\n[root@learn modules]# puppet parser validate motd/manifests/init.pp\n[root@learn modules]# puppet apply motd/tests/init.pp\nNotice: Compiled catalog for learn.puppetlabs.com in environment production in 0.09 seconds\nNotice: /Stage[main]/Motd/File[/etc/motd]/ensure: defined content as '{md5}13c0016553b5315a8cd66f13b23f14a3'\nNotice: Finished catalog run in 0.03 seconds\n\n\n[root@learn modules]#  cat /etc/motd\n\n\nHello there Seamus\n\n\n\n\n\n\n\nFacter\n\n\nFacter stores facts as pairs of keys and values.\n\n\nFacter command line options\n\n\n-y, --yaml                       Emit facts in YAML format.\n-j, --json                       Emit facts in JSON format.\n    --plaintext                  Emit facts in plaintext format.\n    --trace                      Enable backtraces.\n    --external-dir DIR           The directory to use for external facts.\n    --no-external-dir            Turn off external facts.\n-d, --debug                      Enable debugging.\n-t, --timing                     Enable timing.\n-p, --puppet                     Load the Puppet libraries, thus allowing Facter to load Puppet-specific facts.\n-v, --version                    Print the version and exit.\n-h, --help                       Print this help message.\n\n\n\nA shell enviroment variable prepened with the name \nFACTER_\n   will be readable by facter as a fact. eg...\n\n\n[root@node1 ~]# export FACTER_seamus=murray\n[root@node1 ~]# facter seamus\nmurray\n[root@node1 ~]# facter |grep seamus\nseamus =\n murray\n\n\n\nFacter values can be \ntemporarily overridden\n by setting a shell variable prepended with \nFACTER_\n and the name of the fact. eg...\n\n\nFACTER_operatingsystem=Debian\n\n\n\nYou can even test the effect that the fake fact will have using puppet apply..\n\n\nFACTER_operatingsystem=Debian puppet apply --noop accounts/tests/init.p\n\n\n\nThe best way to distribute facts is to include them in your modules using the puppet plug-ins.\nPuppet will distribute the custom facts, custom types, providers and functions to any host that includes the module.\n\n\nyou can either distribute the facts with the modules that need them or you can create a module combining all your custom facts needed by all your other modules.\n\n\nFacter is called by the puppet agent.\nFacts apear as normal and top scope variables they can be called by\n\n\n$ipaddress\n\n\n\nor\n\n\n${::ipaddress}  (best practice)\n\n\n\nPluginsync\n\n\nCustom Facts and types can be exported to the the agent nodes from the puppet master using the plugin sync mechanism.\nBefore the client requests a catlog it checks to see if the are any new plugins or if its plugins have been updated. \nIf there has been a change it pulls down the plugins and executes the code in the custom types and facts.\n\n\nThis and be disabled from the agent nodes \npuppet.conf\n config file\n\n\n[main]\npluginsync = false\n\n\n\nCustom facts\n\n\nyou can create a custom fact with a bit of ruby code on the puppet master and use plugins and modules to distribute it to the clients/nodes\n\n\nCreate a ruby file in\n\n\nmodulename\n/lib/facter/\nmodulename\n.rb\n\n\n\nFor simple shell commands, just insert the shell command in between the \nsetcode do\n and the \nend\n.\nFor more complex comands use \nFacter::Core::Execution.exec('\n')\n between the \nsetcode do\n and the \nend\n.\n\n\nFacter.add('mycustomfact') do\n  setcode do\n    Facter::Core::Execution.exec('/bin/date')\n  end\nend\n\n\n\non the agents use plugin sync to automatically pull down the facts to  /var/lib/puppet/lib/facter/mycustomfact.rb\n\n\npuppet agent -t\n\n\n\nuse the \n-p\n facter flag to test the puppet fact\n\n\nfacter -p  mycustomfact\n\n\n\nA fact can be confined to run only if another fact is of a certain value\n\n\ne.g.\n\n\nFacter.add(:powerstates) do\n  confine :kernel =\n 'Linux'\n  setcode do\n    Facter::Core::Execution.exec('cat /sys/power/states')\n  end\nend\n\n\n\nExternal facts\n\n\nCan be written in any language as long as they return key/value pairs to stdout.\n\n\nThey are stored in ..\n\n\nMODULEPATH\n/\nMODULE\n/facts.d/\n\n\n\nand are distributed to the clients via the pluginsync mechanism.\n\n\nExample fact written in bash...\n\n\n$ cat/etc/puppet/modules/myexternalfact/facts.d/myexternalfacts.sh\n#!/bin/bash\necho \"myexternalfact1=`/bin/df -P / |/usr/bin/tail -1 | /bin/awk '{print $5}'`\"\necho \"extfact1=one\"\necho \"extfact2=two\"\necho \"extfact3=three\"\n\n\n\nCalling the external fact from the command line on a node that has recieved  the external fact by pluginsync...\n\n\n$ facter --external-dir  /var/lib/puppet/facts.d/ extfact1 extfact2 extfact3 myexternalfact1\nmyexternalfact1 =\n 33%\nextfact1 =\n one\nextfact2 =\n two\nextfact3 =\n three\n\n\n\nExample fact written in c...\n\n\n#include \nstdio.h\n\n\nint main() {\n   printf(\"my_external_fact_in_c=\\\"Hello World\\\"\\n\");\n return 0;\n}\n\n\n\nCalling the external fact written in c...\n\n\n# facter --external-dir ~/puppet-study/external-facts/ my_external_fact_in_c\n\"Hello World\"\n\n\n\nDistributing the external facts to the puppet client nodes\n\n\nIt is best to put the external facts into a module so that they can be distributed to all the agent nodes via plugin sync..\n\n\n[root@master myexternalfact]# tree  /etc/puppet/modules/myexternalfact\n/etc/puppet/modules/myexternalfact\n\u2514\u2500\u2500 facts.d\n    \u251c\u2500\u2500 myexternalfactinc\n    \u2514\u2500\u2500 myexternalfact.sh\n\n\n[root@master ~]# ls -l /etc/puppet/modules/myexternalfact/facts.d/\ntotal 16\n-rwxr-xr-x. 1 root root 8523 Feb 10 00:27 myexternalfactinc\n-rwxr-xr-x. 1 root root  235 Feb 10 00:36 myexternalfact.sh\n\n\n\nTrigger the plugin sync to trasfer the new external facts..\n\n\n[root@node1 ~]# puppet agent -t\n\n\n\nUse the \npuppet apply -e\n tool on the agent nodes to test the facter values..\n\n\n[root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_c}\" ) '\nNotice: Scope(Class[main]): \"Hello World\"\nNotice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds\nNotice: Finished catalog run in 0.03 seconds\n\n[root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_1}\" ) '\nNotice: Scope(Class[main]): 32%\nNotice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds\nNotice: Finished catalog run in 0.03 seconds\n\n[root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_2}\" ) '\nNotice: Scope(Class[main]): two\nNotice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds\nNotice: Finished catalog run in 0.03 seconds\n\n\n\nStructured facts\n\n\nCan come straight from YAML,JSON or txt files in the directories..\n\n\n/etc/facter/facts.d/\n  or \n/etc/puppetlabs/facter/facts.d/\n\n\n$ cat /etc/facter/facts.d/mystructuredfact.yaml\n---\nmystructuredfact1: factone\nmystructuredfact2: facttwo\nmystructuredfact3: factthree\n\n$ facter mystructuredfact1 mystructuredfact2 mystructuredfact3\nmystructuredfact1 =\n factone\nmystructuredfact2 =\n facttwo\nmystructuredfact3 =\n factthree\n\n\n\n\n\n\n\nRoles and Profiles\n\n\nRoles and profiles are just modules written in a specific way.\n\n\n\n\nroles are assigned to nodes\n\n\nprofiles are assigned to roles\n\n\n\n\nexample role\n\n\nclass role::weibserver inherits role {\n  include profile:: web\n}\n\nclass role::dbserver inherits role {\n  include profile::db\n}\n\nclass role::webdbserver inherits role {\n  include profile:web\n  include profile::db\n}\n\n\n\nclassification = assignment of classes to nodes\nclassification = assignment of roles to nodes\n\n\nexample profile\n\n\nclass profile::web {\n  include apache\n  include php\n  include tomcat\n  include jdk\n  include memcache\n}\n\n\n\n__\n\n\nTip:  classes are singletons, they can be included multiple times on a node, but they will only be evaluated \nonce\n. \nA defined class can be declared multiple times, because it takes parameters and each new declaration will be evaluated.\n\n\nYou can see we\u2019ve included our two classes but not the definition, apache::vhost. This is because of some module magic called autoloading. \nPuppet scans your module and loads any .pp file in the manifests directory that is named after the class it contains; for example, the install.pp file contains the apache::install class and so is autoloaded.\n\n\nThe same thing happens with definitions: The vhost.pp file contains the definition apache::vhost, and Puppet autoloads it. However, as we declare definitions, for example calling apache::vhost where we need it, we don\u2019t need to do an include apache::vhost because calling it implies inclusion.\n\n\n...............................................\n\n\nclass puppet::params {\n  $puppetserver = hiera('puppetserver')\n}\n\n\n\nIn Puppet 3, but not Puppet 2.7, there is an automatic lookup of parameters in a parameterized class. We can use this to rewrite the puppet::params class further:\n\n\nclass puppet::params (\n  $puppetserver,\n){\n}\n\n\n\nWhen this is called with no arguments, Puppet 3 will attempt to look up the puppet::params::puppetserver key in Hiera and populate it accordingly. It will not fail unless the Hiera lookup fails.\n\n\n\n\n\n\nPuppet Templates\n\n\nPuppet supports templates written in the ERB templating language, which is part of the Ruby standard library.\nTemplates are always evaluated by the parser, not by the client. This means that if you are using a puppet master server, then the templates only need to be on the server, and you never need to download them to the client. The client sees no difference between using a template and specifying all of the text of the file as a string.\nNote that the template function simply returns a string, which can be used as a value anywhere \u2014 the most common use is to fill file contents, but templates can also provide values for variables:\n\n\nags\n\n\nThe tags available in an ERB file depend on the way the ERB processor is configured. Puppet always uses the same configuration for its templates (see \u201ctrim mode\u201d below), which makes the following tags available:\n\n\n\n\n \n%= Ruby expression %\n \n \u2014 This tag will be replaced with the value of the expression it contains.\n\n\n \n% Ruby code %\n \n \u2014 This tag will execute the code it contains, but will not be replaced by a value. Useful for conditional or looping logic, setting variables, and manipulating data before printing it.\n\n\n \n%# comment %\n  \n \u2014 Anything in this tag will be suppressed in the final output.\n\n\n \n%% or %%\n  \n \u2014 A literal \n% or %\n, respectively.\n\n\n \n%- \u2014 Same as \n% \n , but suppresses any leading whitespace in the final output. Useful when indenting blocks of code for readability.\n\n\n -%\n \n \u2014 Same as \n %\n \n , but suppresses the subsequent line break in the final output. Useful with many lines of non-printing code in a row, which would otherwise appear as a long stretch of blank lines.\n\n\n\n\nPuppet parser\n\n\nIs the part of puppet that pareses the puppet DSL code in manifests files.\nIt can be called from the command line to validate your puppet code\n\n\n[root@pe-puppet]# puppet parser validate /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp \n  Error: Could not parse for environment production: Syntax error at '{'; expected '}' at /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp:3\n\n\n\n\n\n\n\nFunctions\n\n\nFunctions are extensions of the Puppet Parser. They are located and are executed on the master and only have access to resources and data that is available on the master. \nDuring the parsing of the manifest's any function code is executed and the subsequent return value/s are inserted into the resulting compilation.\n\n\nThere are two types of functions: statements and rvalues.\n\n\n\n\n\n\nStatements\n, such as the \nfail\n function, which stops the Puppet run with a parser error, perform some action.\n\n\n\n\n\n\nRvalues\n return values and can be used anywhere a normal value is expected. (This includes resource attributes, variable assignments, conditions, selector values, the arguments of other functions, etc.) These values can come from a variety of places;\n\n\n\n\ntemplate\n function reads and evaluates a template to return a string.\n\n\nstdlib\u2019s \nstr2bool\n and \nnum2bool\n functions convert values from one data type to another.\n\n\nsplit\n function parses a string and returns array elements.\n\n\n\n\n\n\n\n\n\n\n\n\nRBAC  Role Based Access Control\n\n\nPuppet enterprise ships with 3 default \nUser Roles\n\n\n\n\nAdministrators\n\n\nOperators\n\n\nViewers", 
            "title": "Puppet study notes"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppet-installation", 
            "text": "System requirements  Puppet Enterprise 3.7 supports the following systems:  +------------------+---------------------------+-------------+------------------------------------+\n|Operating system  |Version(s)                 | Arch        |Component(s)                        |\n|------------------|---------------------------|-------------|------------------------------------|\n|RHEL              |4, 5, 6, 7                 |x86_64       |all (RHEL 4 supports agent only)    |\n|CentOS            |4, 5, 6, 7                 |x86   x86_64 |all (CentOS 4 supports agent only)  |\n|Ubuntu LTS        |10.04, 12.04, 14.04        |i386   amd64 |all                                 |\n|Debian            |Squeeze (6), Wheezy (7)    |i386   amd64 |all                                 |\n|Oracle Linux      |4, 5, 6, 7                 |x86   x86_64 |all (Oracle Linux 4 agent only)     |\n|Scientific Linux  |4, 5, 6                    |x86   x86_64 |all (Scientific Linux 4 agent only) |\n|SUSE Enterprise   |10 (SP4 only), 11(SP1), 12 |x86   x86_64 |all (SLES 10 supports agent only)   |\n|Solaris           |10 (Update 9 or later) 11  |SPARC   i386 |agent                               |\n|Microsoft Windows |2008 2008R2 7 Ultimate SP1 |x86   x64    |agent                               |\n|                  |8 Pro, 8.1 Pro 2012 2012R2 |x86   x64    |agent                               |\n|Microsoft Windows |2003, 2003R2               |x86          |agent                               |\n|AIX               |5.3, 6.1, 7.1              |Power        |agent                               |\n|Mac OS X          |Mavericks (10.9)           |x86_64       |agent                               |\n+------------------+---------------------------+-------------+------------------------------------+  https://docs.puppetlabs.com/pe/latest/install_basic.html#about-puppet-enterprise-components\n   the puppet enterprise  3.1 guide refers to  puppet master role  database support role console role  Monolithic (all-in-one) Installation  Monolithic installs are suitable for deployments up to 500 nodes. We recommend that your hardware meets the following:   The puppet master, PE console, and PuppetDB node: at least 4-8 processor cores, 8 GB RAM  All machines require very accurate timekeeping  Puppet agent nodes: any hardware able to run the supported operating system  For /var/, at least 1 GB of free space for each PE component on a given node  For PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering  For no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available   Split Installation  For larger deployments (500-1000, or more nodes), we recommend a split install. We recommend that your hardware meets the following:   Puppet master, PE console, and PuppetDB nodes: at least 8 processor cores, 8 GB RAM (per node)  All machines require very accurate timekeeping  Puppet agent nodes: any hardware able to run the supported operating system  For /var/, at least 1 GB of free space for each PE component on a given node  For PE-installed PostgreSQL, at least 100 GB of free space in /opt/ for data gathering  For no PE-installed PostgreSQL, /opt/ needs at least 1 GB of disk space available   Firewall Configuration  Agent nodes contact the puppet master server on TCP ports 8140 (for Puppet) and 61613 (for orchestration).\nAny hosts that need access to the html GUI will contact the  console server on port 443(can be changed to a different port).  +-----------------+---------+---------+---------+----------+\n|                 |  AGENT  | MASTER  | CONSOLE | ADMIN PC |\n|-----------------|---------|---------|---------|----------|\n|puppet           |   ----  |   8140  |         |          |\n|Orchestration    | 61613-- | --61613 |         |          |\n|HTML GUI         |         |         |   443   |  ----    |\n|Master Installer |         |   3000  |         |  ----    | #html installer\n+-----------------+---------+---------+---------+----------+  From 3.1 installation guide... https://docs.puppetlabs.com/pe/3.1/install_system_requirements.html  Hardware requirements for the various puppet roles/components  Puppet Enterprise\u2019s hardware requirements depend on the roles a machine performs.    The  puppet master role  should be installed on a robust, dedicated server.\n        Minimum requirements: 2 processor cores, 1 GB RAM, and very accurate timekeeping.\n        Recommended requirements: 2-4 processor cores, at least 4 GB RAM, and very accurate timekeeping.\n        Performance will vary, but this configuration can generally manage approximately 1,000 agent nodes.    The  database support role  can be installed on the same server as the console or, optionally, on a separate, dedicated server.\n        Minimum requirements: These will vary considerably depending on the size of your deployment. \n        However, you\u2019ll need a machine able to handle moderate network traffic, perform processor-intensive background tasks, and run a disk-intensive PostgreSQL database server.\n        The machine should have two to four processor cores.\n        As a rough rule of thumb for RAM needed, start here: 1-500 nodes: 192-1024MB, 500-1000 nodes: 1-2GB, 1000-2000 nodes: 2-4 GB, 2000+ nodes, 4GB or greater.\n        So as your deployment scales, make sure to scale RAM allocations accordingly.\n        More information about scaling PuppetDB is available in the PuppetDB manual\u2019s scaling guidelines.    The  console role  should usually be installed on a separate server from the puppet master, but can optionally be installed on the same server in smaller deployments.\n        Minimum requirements: A machine able to handle moderate network traffic and perform processor-intensive background tasks.\n        It should have a very fast network connection to the database support server, which it uses for all of the console\u2019s database requirements.\n        Requirements will vary significantly depending on the size and complexity of your site.    The optional  cloud provisioner role  has very modest requirements.\n        Minimum requirements: A system which provides interactive shell access for trusted users.\n        This system should be kept very secure, as the cloud provisioning tools must be given cloud service account credentials in order to function.    The puppet  agent role  has very modest requirements.\n        Minimum requirements: Any hardware able to comfortably run a supported operating system.", 
            "title": "Puppet installation"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#install-script", 
            "text": "[root@master puppet-enterprise-3.7.1-el-6-x86_64]# ./puppet-enterprise-installer -h  Puppet Enterprise v3.7.1 installer  Puppet Enterprise documentation can be found at http://docs.puppetlabs.com/pe/3.7/  USAGE: puppet-enterprise-installer [-a ANSWER_FILE] [-A ANSWER_FILE] [-D] [-h] [-l LOG_FILE] [-n] [-q] [-s ANSWER_FILE] [-V]  OPTIONS:   -a ANSWER_FILE  - Read answers from file and quit with error if an answer is missing.  -A ANSWER_FILE  - Read answers from file and prompt for input if an answer is missing.  -D              - Display debugging information.  -h              - Display this help.  -l LOG_FILE     - Log commands and results to file.  -n              - Run in 'noop' mode; show commands that would have been run during installation without running them  -q              - Run in quiet mode; the installation process is not displayed. Requires answer file.  -s ANSWER_FILE  - Save answers to file and quit without installing.  -V              - Display very verbose debugging information.   Install the pe-agent  curl -k https:// puppet master server :8140/packages/current/install.bash | sudo bash  puppet.conf  /etc/puppetlabs/puppet/puppet.conf  {puppet enterprise}\nor\n/etc/puppet/puppet.conf  {puppet opensource}  [main]\n    certname = learning.puppetlabs.vm\n    vardir = /var/opt/lib/pe-puppet\n    logdir = /var/log/pe-puppet\n    rundir = /var/run/pe-puppet\n    basemodulepath = /etc/puppetlabs/puppet/modules:/opt/puppet/share/puppet/modules\n    environmentpath = /etc/puppetlabs/puppet/environments\n    server = learning.puppetlabs.vm\n    user  = pe-puppet\n    group = pe-puppet\n    archive_files = true\n    archive_file_server = learning.puppetlabs.vm\n    module_groups = base+pe_only\n\n[agent]\n    report = true\n    classfile = $vardir/classes.txt\n    localconfig = $vardir/localconfig\n    graph = true\n    pluginsync = true\n    environment = production  SSL certificates and accurate time  If there is a clock sync issue between an agent and the master you will receive certificate verification errors..  root@node3:~# puppet agent -t\nWarning: Unable to fetch my node definition, but the agent run will continue:\nWarning: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\n Info: Retrieving plugin\nError: /File[/var/lib/puppet/lib]: Failed to generate additional resources using 'eval_generate': SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\nError: /File[/var/lib/puppet/lib]: Could not evaluate: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs] Could not retrieve file metadata for puppet://puppet/plugins: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\nError: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]\nWarning: Not using cache on failed catalog\n Error: Could not retrieve catalog; skipping run\nError: Could not send report: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed: [CRL is not yet valid for /CN=master.puppetlabs]  resolution sync the clock's and restart puppet agent service or manually run puppet agent -t  Resetting a nodes certificate  +--------------------------------+-----------------------------+\n|       PUPPET AGENT             |    PUPPET MASTER            |\n|--------------------------------|-----------------------------|\n|  service puppet stop           |                             |\n|  puppet config print ssldir    |                             |\n|  rm -rf  /var/lib/puppet/ssl   |                             |\n|                                | puppet cert list --all      |\n|                                | puppet cert clean  certname |\n|  puppet agent -t               |                             |\n|                                | puppet cert list            |\n|                                | puppet sign  certname       |\n|  puppet agent -t               |                             |\n|  service puppet start          |                             |\n+--------------------------------+-----------------------------+  Until the puppet master signs the nodes cert the agent will display the following message  root@node3:~# puppet agent -t\nExiting; no certificate found and waitforcert is disabled  Puppet Cert command line options  puppet cert   action    host     clean: \n  Revoke a host's certificate (if applicable) and remove all files\n  related to that host from puppet cert's storage. This is useful when\n  rebuilding hosts, since new certificate signing requests will only be\n  honored if puppet cert does not have a copy of a signed certificate\n  for that host. If '--all' is specified then all host certificates,\n  both signed and unsigned, will be removed.    fingerprint: \n  Print the DIGEST (defaults to the signing algorithm) fingerprint of a\n  host's certificate.    generate: \n  Generate a certificate for a named client. A certificate/keypair will\n  be generated for each client named on the command line.    list: \n  List outstanding certificate requests. If '--all' is specified, signed\n  certificates are also listed, prefixed by '+', and revoked or invalid\n  certificates are prefixed by '-' (the verification outcome is printed\n  in parenthesis).    print: \n  Print the full-text version of a host's certificate.    revoke: \n  Revoke the certificate of a client. The certificate can be specified either\n  by its serial number (given as a hexadecimal number prefixed by '0x') or by its\n  hostname. The certificate is revoked by adding it to the Certificate Revocation\n  List given by the 'cacrl' configuration option. Note that the puppet master\n  needs to be restarted after revoking certificates.    sign: \n  Sign an outstanding certificate request.    verify: \n  Verify the named certificate against the local CA certificate.    reinventory: \n  Build an inventory of the issued certificates. This will destroy the current\n  inventory file specified by 'cert_inventory' and recreate it from the\n  certificates found in the 'certdir'. Ensure the puppet master is stopped\n  before running this action.", 
            "title": "Install script"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#node-classification-sitepp-enc", 
            "text": "Certname  By default each nodes certification name is its FQDN.\nWhen the puppet agent on the nodes request at catalogue from the puppet master it does so using its current FQDN {even if they had already generated a certificate and had it signed by the master}. Therefore if the nodes host or domain name changes it will make a request based on the new name and you will have to sign a new cert on the master.  Classification of nodes  In puppet the term classification  refers to the linking/assigning of classes to nodes. This can be achieved in many ways   Using site.pp   node ' group name ' {\n  include  class \n}\n\nnode ' node name ' {\n  include  class \n}   Using the puppet enterprise console   Click on Classification in the console navigation bar.\n    ( Optional steps to create a new group )  Enter a new node group name  click Add group    Click on the desired group and set the rules for this group based on facter values name/osfamily etc... click add rule  Click commit change   If your changes to the site.pp manifest are'nt reflected in a puppet run triggered by the puppet agent -t command, try running the command again.   Using an ENC External node Classifier  \nAn executable that can be called by puppet master; it doesn\u2019t have to be written in Ruby. Its only argument is the name of the node to be classified, and it returns a YAML document describing the node.  Node statements are an  optional feature  of Puppet. They can be replaced by or combined with an external node classifier. You can also use conditional statements upon facts to classify nodes.", 
            "title": "Node Classification  site.pp ENC"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#sitepp-syntax", 
            "text": "The Default Node  The site.pp file has an optional default node declaration . It follows the same format as a normal node declaration except the word  default  (without quotes)  is used  in place of a cert/host name. If a node requests a catalogue and there is no matching cert/host name in the site.pp then the node will be assigned what ever settings have be set for the default node.  You can simply use the  node name :  node 'www.example.com' {\n  include common\n  include apache, squid\n}  You can use a  comma-separated list  of names to create a group of nodes with a single node statement:  node 'www1.example.com', 'www2.example.com', 'www3.example.com' {\n  include common\n  include apache, squid\n}  You can use  Regular expressions  (regexes) can be used as node names. In puppet regexes must be surrounded by forward slashes /  match any node with the name www followed by 1 or more digits (www1 , www12345)  node /^www\\d+$/ {\n  include common\n}  match foo.example.com and bar.example.com  node /^(foo|bar)\\.example\\.com$/ {\n  include common\n}  If site.pp contains at least one node definition, it must have one for every node; compilation for a node will fail if one cannot be found. (Hence the usefulness of the default node.) If site.pp contains no node definitions, this requirement is dropped.  Matching  If a nodes certname/hostname is matched by more than one node statement it will only get the contents of one node definition.\nPuppet will do the following checks in order when deciding which definition to use:   If there is a node definition with the node\u2019s exact name, Puppet will use it.  If there is a regular expression node statement that matches the node\u2019s name, Puppet will use it. (If more than one regex node matches, Puppet will use one of them, with no guarantee as to which.)  If the node\u2019s name looks like a fully qualified domain name (i.e. multiple period-separated groups of letters, numbers, underscores and dashes), Puppet will chop off the final group and start again at step 1. (That is, if a definition for www01.example.com isn\u2019t found, Puppet will look for a definition matching www01.example.)  Puppet will use the default node.   Thus, for the node www1.example.com, Puppet would try the following, in order:  www1.example.com\nA regex that matches www1.example.com\nwww1.example\nA regex that matches www1.example\nwww1\nA regex that matches www1\ndefault  In my testing with a site.pp  node 'node2.puppetlabs' {\n  include hosts\n}\n\nnode /^node(2|4)/ {\n  include regexmod\n  include definedmod\n  include ntp\n}  with the node/cert name of node2.puppetlabs it is matched my the most accurate name ie \"node2.puppetlabs\"  if the site.pp only has the shortname and a shortname+regex then the regex its matched  eg..  node 'node2' {\n  include hosts\n}\n\nnode /^node(2|4)/ {\n  include regexmod\n  include definedmod\n  include ntp\n}  Merging site.pp With ENC Data  Node definitions and external node classifiers can co-exist. Puppet merges their data as follows:  Variables from an ENC are set at top scope and can thus be overridden by variables in a node definition.\nClasses from an ENC are declared at node scope, which means they will be affected by any variables set in the node definition.  Although both will work together it is recommend to use   either  node definitions or an ENC.  External nodes override node configuration in the manifest files. If you enable an external node classifier, any duplicate node definitions in your manifest files will not be processed; they will be ignored by Puppet.  The use of LDAP nodes overrides node definitions in your manifest files and your ENC. If you use LDAP node definitions, you cannot define nodes in your manifest files or in an ENC.  Inheritance  In earlier versions of Puppet, nodes could inherit from other nodes using the inherits keyword. This feature is deprecated in Puppet 3.7,", 
            "title": "site.pp syntax"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#node-classification-in-puppet-enterprise-console", 
            "text": "click the  classification  tab\nselect the group or create a new group\nset the rules to match the node   static matching , add the nodes FQDN in the certname field and pin the node to the group   dynamic matching , create a  fact + operator + value  based rule to match the node, e.g.   Table detailing possible matching rules  +-----------+----------------+------------------------+\n|  FACT     | OPERATOR       |  VALUE or REGEX        |\n+-----------+----------------+------------------------+\n|  hostname |  is            |  pe-node2              |\n|  name     |  is            |  pe-node3.puppetlabs   |\n|  name     |  matches regex |  pe-node\\d.puppetlabs  |\n|  name     |  matches regex |  node(4|5).puppetlabs  |\n|  osfamily |  is            |  RedHat                |\n+-----------+----------------+------------------------+  Note  Don't forget to click  add Rule  and then click  commit x change  default node group  PE comes preconfigured with the  default node group .\nThis is the root of the node group hierarchy and is a parent to all other node groups. \nThe classes that are assigned to the default node group are applied to all of the nodes in your deployment.", 
            "title": "node classification in puppet enterprise console"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#deleting-nodes-from-puppet-enterprise-console", 
            "text": "There are three options for deleting a node:    Hide Node \nRemoves the node from the node list view. To hide a node: On the Nodes page, click the node.\nClick Hide.    Delete Node  Removes all reports and information for the node from the console. The node no longer appears in the list of nodes on the Nodes page, but it continues to appear in Matching nodes until it is purged from PuppetDB. The node will reappear as a new node on the Nodes page if it submits a new Puppet run report. To delete a node:  On the Nodes page, click the node.\nClick Delete.    Deactivate Node  Completely deactivates the node and frees up the license assigned to the node. Any deactivated node will be reactivated if PuppetDB receives new catalogues or facts for it. For details, see Deactivating a PE agent node.     Stop the agent service on the node you want to deactivate.    On the Puppet master, deactivate the agent; run  puppet node deactivate {NODE NAME} .\n This deactivates the agent in PuppetDB. In some cases, the PE license count in the console will not decrease for up to 24 hours, but you can restart the pe-memcached service to update the license count sooner.    On the Puppet master, revoke the agent certificate; run  puppet cert revoke {AGENT CERTNAME} .    Still on the Puppet master,  run puppet agent -t  to kick off a Puppet run.\n This Puppet run will copy the certificate revocation list (CRL) to the correct SSL directory for delivery to the agent.    Restart the Puppet master with  service pe-puppetserver restart .\n The certificate is only revoked after running service pe-puppetserver restart. In addition, the Puppet server process won\u2019t re-read the certificate revocation list until the service is restarted. If you don\u2019t run service pe-puppetserver restart, the node will check in again on the next Puppet run and re-register with PuppetDB, which will increment the license count again.    Delete the node from the console. In the console, click Nodes. Click the node that you want to delete and click the Delete button.\n This action does NOT disable MCollective/live management on the node.    To disable MCollective/live management on the node, uninstall the Puppet agent, stop the pe-mcollective service (on the agent, run service pe-mcollective stop), or destroy the agent altogether.\n On the agent, remove the node certificates in  /etc/puppetlabs/mcollective/ssl/clients .    clean the cert from the pauppet master ... puppet cert clean  {AGENT CERTNAME}      Note: If you delete a node from the node view without first deactivating the node, the node will be absent from the node list in the console, but the license count will not decrease, and on the next Puppet run, the node will be listed in the console.      At this point, the node should be fully deactivated.", 
            "title": "Deleting Nodes from puppet enterprise console"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#regex-in-puppet", 
            "text": "Regexes in Puppet cannot have options or encodings appended after the final slash.\nHowever, you may turn options on or off for portions of the expression using the (? : ) and (?- : ) notation.\nThe following example enables the  i  option while disabling the  m  and  x  options:   $packages = $operatingsystem ? {\n   /(?i-mx:ubuntu|debian)/        =  'apache2',\n   /(?i-mx:centos|fedora|redhat)/ =  'httpd',\n }  The following options are allowed:  i \u2014 Ignore case\nm \u2014 Treat a newline as a character matched by .\nx \u2014 Ignore white space and comments in the pattern", 
            "title": "Regex in puppet"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppet-run-overview", 
            "text": "The SSL checks are made first  The agent checks for a certificate matching its FQDN  If one is not found the agent generates one.     The agent checks for the CA cert.  if not it will send its cert  to the CA servers and request for its cert to be signed.    If plugin sync is enabled, the agent checks the master for new plugins and downloads them if necessary.  The agent ask's Facter for a set of facts about itself. { facter -p }  The agent sends the facts the master whilst requesting a catalog.  The master injects those facts as variables in the root scope and processes the manifests.  The master then sends the catalog to the agent.  The agent apply's the catalog.  If reporting is enabled the agent sends a report back to the master.   Diagram of puppet run  +-----------------------+     +-----------------------+\n|        AGENT          |     |        MASTER         |\n|  +-----------------+  |     |                       |\n|  | Check local Cert|  |     |     +--------------+  |\n|  | create if needed|  |     | +-- | Send CA Cert |  |\n|  +-----------------+  |     | |   +--------------+  |\n| +------------------+  |     | |                     |\n| | Check 4 copy of  |  |     | |                     |\n| | CA Cert,retrieve |  |     | |                     |\n| | if needed        | ---------+                     |\n| +------------------+  |     |                       |\n| +-------------------+ |     |  +------------------+ |\n| | Is my Cert signed | -------- | Sign Client Cert | |\n| +-------------------+ |     |  +------------------+ |\n|                       |     |                       |\n|  +-----------------+  |     |  +-----------------+  |\n|  | Request plugins |---------  |  Send Plugins   |  |\n|  +-----------------+  |     |  +-----------------+  |\n|                       |     |           |           |\n|  +---------------- +  |     |           |           |\n|  | Import plugins  | -------------------+           |\n|  +-----------------+  |     |                       |\n|                       |     |                       |\n|  +-----------------+  |     |     +--------------+  |\n|  |                 |  |     |     |   CLASSIFY   |  |\n|  | REQUEST CATALOG +------------  |The Node based|  |\n|  |  Send node name |  |     |     |on certname + |  |\n|  |  and facts to   |  |     |     | node groups  |  |\n|  |  the master     |  |     |     +--------------+  |\n|  |                 |  |     |     ______|______     |\n|  ------------------+  |     |    |      |      |    |\n|                       |     | +--V--++--V--++--V--+ |\n|                       |     | |CLASS||CLASS||CLASS| |\n|  +---------------+    |     | +-----++-----++-----+ |\n|  |               |    |     |      \\    |     /     |\n|  |    CATALOG    |  -----------+  +-V---V----V-+    |\n|  |               |    |     |  |  |            |    |\n|  +--------+------+    |     |  |  |  COMPILE   |    |\n|           |           |     |  |  |            |    |\n|  +--------V---------+ |     |  |  +------------+    |\n|  |APPLY +--------+  | |     |  |        V           |\n|  |      | QUERY  |  | |     |  |        |           |\n|  |      | STATUS |  | |     |  +---- ---+           |\n|  |      +----|---+  | |     |                       |\n|  |    +------V----+ | |     |                       |\n|  |    |  ENFORCE  | | |     |                       |\n|  |    |  DEFINED  | | |     |                       |\n|  |    |  STATE    | | |     |                       |\n|  |    +-----------+ | |     |    +-------------+    |\n|  +----+-------------+ |     |    |             |    |\n|           |           |     |    |   REPORT    |    |\n|           |--------------------  |             |    |\n|           |           |     |    +-------------+    |\n|  +--------V---------+ |     |                       |\n|  |  DEFINED SYSTEM  | |     |                       |\n|  |      STATE       | |     |                       |\n|  +------------------+ |     |                       |\n+-----------------------+     +-----------------------+", 
            "title": "Puppet run overview"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#resources-classes-manifest", 
            "text": "Manifests  A puppet manifest is text file that contains puppet code and is appended by the .pp file extension.  Catalog  In a standard agent + master puppet configuration, the agent never receives a copy of the modules, manifests, functions or variables.\nThe agent only receives the compiled catalogue of resources and relationships.  It is not recommended by puppetlabs, however you can send each node an entire copy of the manifests for the nodes the compile the catalog and apply it locally,\nThis may be desirable in situations where there is no direct network connectivity between the agent and master.\nWarning: each node will be able to see the entire manifest which may contain sensitive information relating to other nodes in your environment.  Modules  Modules are just directories and files with a predictable structure located in the module path.\nThe following commands will display the module path.  # puppet agent --configprint modulepath\n# puppet config print modulepath\n/etc/puppet/modules:/usr/share/puppet/modules  Modules will often have a main class that shares the name of the module.  module-name\n|-manifests/\n|-files/\n|-templates/\n|-lib/\n|-tests/\n|-spec/  If a class is defined in a module you can then declare that class in any manifest by name.  Autoloading in Puppet means that your modules will be loaded by Puppet at compile time, as long as they follow a predictable structure.  puppet _\\   compile  _\\   load\n run    /   catalog   /  modules  Example module's  tree /etc/puppetlabs/puppet/environments/production/modules/sshd\n\n/etc/puppetlabs/puppet/environments/production/modules/sshd\n\u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sshd_config\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp  example module with class includes  cowsayings\n\u251c\u2500\u2500 manifests\n\u2502   \u251c\u2500\u2500 init.pp     -----    class cowsayings {\n\u2502   \u2502                          include cowsayings::cowsay\n\u2502   \u2502                          include cowsayings::fortune\n\u2502   \u2502                        }\n\u2502   \u2502\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 cowsay.pp   -----    class cowsayings::cowsay {\n\u2502   \u2502                          package { 'cowsay':\n\u2502   \u2502                          ensure =  'present',\n\u2502   \u2502                        }\n\u2502   \u2502\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 fortune.pp  -----    class cowsayings::fortune {\n\u2502                                package { 'fortune-mod':\n\u2502                                  ensure =  'present',\n\u2502                                }\n\u2502                            }\n\u2502\n\u2514\u2500\u2500 tests\n    \u2502\n    \u251c\u2500\u2500 init.pp    --    include cowsayings\n    \u2502\n    \u2502\n    \u251c\u2500\u2500 cowsay.pp  --    include cowsayings::cowsay\n    \u2502\n    \u2502\n    \u2514\u2500\u2500 fortune.pp --    include cowsayings::fortune  puppet module command line  #puppet module  action   ACTIONS:   build     -    Build a module release package.  changes   -    Show modified files of an installed module.  generate  -    Generate boilerplate for a new module.  install    -   Install a module from the Puppet Forge or a release archive.  list    -   List installed modules  search  -      Search the Puppet Forge for a module.  uninstall  -   Uninstall a puppet module.  upgrade  -    Upgrade a puppet module.   When uploading a module to the forge your must include a   modulefile   containing the modules  metadata (name, version, source, author, description, etc.)", 
            "title": "Resources ---}  Classes   ---}  Manifest"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#resources", 
            "text": "Each resource describes some aspect of a system and its state e.g....   a service that must be running  a package that must be installed  a user that must be configured   A resource declaration is the puppet block of code that describes a resource.\nResource declarations are written the in the puppet DML Declarative Modelling Language.  Puppet's DML is a declarative language rather than an imperative one. This means that instead of defining a process or set of commands, Puppet code describes (or declares) only the desired end state, and relies on built-in providers to deal with implementation.  Resource declaration syntax example  type { 'title':\n  attribute_1  =   value_1,\n  attribute_2  =   value_2,\n}   type and title must be unique for a node  values must be alphanumeric (quote strings)  each attribute + value pair must be followed by a comma   Individual resources are combined together to represent the desired system configuration.  Similar resources can be grouped into types.  such as the user type  Resource Abstraction Layer      +---------------------------------------+\n    | file | package | service  |  user     |  Resources\n    |---------------------------------------|\n    | Ruby | Apt     | Redhat   | Useradd   |  Providers\n    |      | Yum     | Launchd  | LDAP      |\n    |      | Gems    | SMF      | Netinfo   |\n    |      | Deb     | Debian   |           |\n    |      | RPM     |          |           |\n    +---------------------------------------+  Query a resource using the puppet command line tool  puppet resource  type  {display all the instances of the specified type}\npuppet resource package\npuppet resource host\npuppet resource user \npuppet resource  type   title  {display the details of a particular resources instance }\npuppet resource package apache\npuppet resource host example.com\npuppet resource user seamus\npuppet resource --type  {list all types}\npuppet describe  type  {display a description of the type and its options}  Puppet Apply  You can use the Puppet resource declaration syntax with the puppet apply tool to make quick changes to resources on the system.  You can either change the values directly from the command line using the syntax..  puppet apply -e \"user { 'seamus': ensure =  'present', }\"  Or you can open the resource declaration in vim with the syntax...  puppet apply -e user seamus  The resource will be opened in vim. Make any desired changed and when you quit vim the new resource values will be applied.    Defined Resource Type (aka  defined types  or  defines )  are types that can be evaluated multiple times using different parameters during declaration. Upon each new declaration with new parameters they act like a new resource type.   Seamus sample module using defined resources\n[root@master modules]# tree definedmod  definedmod\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 definedresource.pp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp  [root@master modules]# cat definedmod/manifests/init.pp  class definedmod {\n  definedresource {'mydefinedresource':\n         var2 =  'twovar',\n         var1 =  'variable1',\n         var3 =  'thirdvariable',\n  }\n}  [root@master modules]# cat definedmod/manifests/definedresource.pp  define definedresource ($var1, $var2 = default2 , $var3 = default3) {\n\n  file { \"/tmp/${var1}\" :\n    ensure  =  present,\n    content =  \"this file name is not $var2 or $var3\",\n  }\n\n  file { \"/tmp/${var2}\" :\n    ensure  =  present,\n    content =  \"this file name is not $var1 or $var3\",\n  }\n\n}  [root@master modules]# cat definedmod/tests/init.pp  include definedmod\n\n[root@node4 ~]# cat /tmp/variable1 \nthis file name is not twovar or thirdvariable\n\n[root@node4 ~]# cat /tmp/twovar \nthis file name is not variable1 or thirdvariable\n\n[root@node4 ~]# cat /tmp/thirdvariable \nthis file name is not variable1 or twovar[root@node4 ~]#    Resource duplicates  Instances of  type + title  must be singleton/unique for each node, otherwise the nodes catalog will fail to compile.  sample duplicate resource declarations:  class resourceduplicates {\n  user { 'user1': # duplicate title of the user resource below; \n                  # the nodes catalog will fail to compile.\n    ensure =  'present',\n    uid    =  '1007',\n  }\n\n  user { 'user1':\n    ensure =  'present',\n    uid    =  '1007', # duplicate UID's, puppet catalog will compile; \n  }                   # But the useradd provider will fail on the client.\n}  sample error of duplicate resource:   Error: Could not retrieve catalogue from remote server: Error 400 on SERVER: Duplicate declaration: User[user2] is already declared in file   Resource refresh  Puppet will refresh a service for each instance of a refresh, e.g if 3 files are being updated and each update triggers a refresh of the same service 3 separate refreshes will be triggered doe each of the file updates.  # This is a test to see if puppet will restart a service multiple time or-\n# just once when multiple refresh events are triggered\nclass loop {\n\n   file { '/tmp/loop1' :\n     ensure  =  present,\n     content =  \"$::uptime_seconds \\n\", # the \"uptime_seconds\" is used here to ensure that\n   }                                    # the content of the file will change upon each run\n\n   file { '/tmp/loop2' :\n     ensure  =  present,\n     content =  \"$::uptime_seconds \\n\",\n   }\n\n   file { '/tmp/loop3' :\n     ensure  =  present,\n     content =  \"$::uptime_seconds \\n\",\n   }\n\n   service { 'crond' :\n     ensure =  running,\n   }\n\n   File['/tmp/loop1'] ~  Service['crond']\n   File['/tmp/loop2'] ~  Service['crond']\n   File['/tmp/loop3'] ~  Service['crond']\n\n}  Resource ordering and relationships with  metaparameters  Via a mechanism called  autorequires ; For some of the built in resource types such as users and groups, Puppet is able to automatically determine the dependency relationships among some of the built-in resources such as user and groups  without a user/admin having to declare the resource interdependencies.  For the majority of resources there is no autorequire and therefore the order of resources in a puppet manifest is generally ignored.\nIf a group of resources have interdependencies you should use one of the metaparameters to specify the relationships between them   before     - Causes a resource to be applied before the target resource  require    - Causes a resource to be applied after the target resource  notify     - Causes a resource to be applied before the target resource.\n              The target resource will refresh if the notifying resource changes.  subscribe  - Causes a resource to be applied after the target resource.\n              The subscribing resource will refresh if the target resource changes.   Note: when creating a resource reference use the syntax  Type['title']  Note: Use lower-case for the type name when declaring it but use a capitalised type name when calling it.  metaparameter declaration examples...  before    package { 'openssh-server':\n    ensure =  present,\n    before =  File['/etc/ssh/sshd_config'],\n  }  require    file { '/etc/ssh/sshd_config':\n    ensure  =  file,\n    mode    =  600,\n    source  =  'puppet:///modules/sshd/sshd_config',\n    require =  Package['openssh-server'],\n  }  Ordering Arrows   Package['openssh-server'] -  File['/etc/ssh/sshd_config']  Causes the resource on the left to be applied before the resource on the right.  notify   file { '/etc/ssh/sshd_config':\n   ensure  =  file,\n   mode    =  600,\n   source  =  'puppet:///modules/sshd/sshd_config',\n   notify  =  Service['sshd'],\n }  subscribe   service { 'sshd':\n   ensure    =  running,\n   enable    =  true,\n   subscribe =  File['/etc/ssh/sshd_config'],\n }  Notification Arrow   File['/etc/ntp.conf'] ~  Service['ntpd']  Causes the resource on the left to be applied first and sends a refresh to the resource on the right if the resource on the left changes.  Complete example  class sshd {\n\n  package { 'openssh-server':\n    ensure =  present,\n    before =  File['/etc/ssh/sshd_config'],\n  }\n\n  file { '/etc/ssh/sshd_config':\n    ensure =  file,\n    mode   =  '0600',\n    source =  'puppet:///modules/sshd/sshd_config',\n  }\n}\n\n  service { 'ssh':\n    ensure    =  running,\n    enable    =  true,\n    subscribe =  File['/etc/ssh/sshd_config'],\n}  extra metaparameters..  Manage dependencies (before, require, subscribe, notify, stage)  Manage resources' application policies (audit, noop, schedule, loglevel)  Add information to a resource (alias, tag)  Virtual resources  are a hack to get around puppets limitation of sigleton resource definitions. They enable an a particular instance of a resource type+title to be used in multiple classes.   They are defined as virtual by pre-pending them with the \"@\" symbol  @user { seamus: ensure =  present }  They can then be declared in multiple classes by using one of the following 3 syntaxes using either the realise function or the spaceship collector.  User  | title == seamus | \nrealize User[seamus]\nrealize(User[seamus])  Exported resources  Enable a resource that is defined on  a host to be exported to the puppetdb and then be used by 1 or more other hosts. Puppet collects and stores the exported resources during configuration runs.  /etc/hosts example  [root@master modules]# cat exportedresource/manifests/init.pp  class exportedresource {\n  @@host { $hostname:  ## sends the local nodes host resource to the puppetdb and \n                       ## tags it as exported\n    ip           =  $::ipaddress,\n    name         =  $::fqdn,\n    host_aliases =  $::hostname,\n    comment      =  'addded by the exported resource class',\n   }\n  Host  | |   ## retrieves all the host resources from the puppetdb\n}  SSH example  class ssh::hostkeys {\n\n  @@sshkey { \"${::fqdn}_dsa\":\n    host_aliases =  [ $::fqdn, $::hostname, $::ipaddress ],\n    type         =  dsa,\n    key          =  $::sshdsakey,\n  }\n\n  @@sshkey { \"${::fqdn}_rsa\":\n    host_aliases =  [ $::fqdn, $::hostname, $::ipaddress ],\n    type         =  rsa,\n    key          =  $::sshrsakey,\n  }\n\n}  SSH example of retrieving the exported resources and applying them  class ssh::knownhosts {\n  Sshkey  | |  { ensure =  present }\n}  The ssh::knownhosts class should be included in the catalog for all nodes where Puppet should manage the SSH known_hosts file.\nNotice that we\u2019ve used double angle braces to collect resources from PuppetDB.  define balancermember($url) {\n  file { '/etc/httpd/conf.d.members/worker_${name}.conf':\n    ensure  =  file,\n    owner   =  'root',\n    group   =  'root',\n    mode    =  '0644',\n    content =  \"BalancerMember $url \\n\",\n  }\n}  This configuration file fragment contains a single line, the URL to a member of the load balancer pool.\nPuppet recommends using a defined resource type because all resources declared within the type will be exported when the defined type itself is exported.  class loadbalancer_members {\n   Balancermember  | |  { notify =  Service['apache'] }\n}  This code uses the double angle brace syntax to collect all balancermember resources from the stored configuration database.   In addition, it uses a parameter block to notify the Apache service of any changes Puppet makes to the balancermember resources. Just as with virtual resources, a parameter block may be specified to add further parameters to collected resources.  Removing retired nodes from PuppetDB  # puppet node deactivate mail.example.com\nSubmitted 'deactivate node' for  node_name  with UUID aaaaa-bbbbb-ccccc-ddddd-eeee  After you\u2019ve run this on the puppet master, any resources exported by this node will no longer be collected on your Puppet clients.\nNote:  Deactivated node resources will not be removed from the systems that collected them.\nYou will need to clean up those configurations manually; or some resources can be purged using the resource metatype.  Exec Resource  Exec resources execute external commands, it is important that any commands executed are Idempotent  Any command in an  exec resource must be able to be run multiple times without casing harm.\nThere are 3 main ways for commands in exec resources to be idempotent\n1. The commands them selves can be idempotent\n2. You use an (onlyif, unless, or creates) attribute, which will prevent puppet from running a specific command unless a specific condition is met.\n3. The exec resource has the ( refreshonly= true ) value, which only allows puppet to run the command when some other resource is changed  syntax   exec { 'resource-title':\n   attribute_1 =  value_1,\n   attribute_2 =  value_2,\n }  example  exec { \"updatedb':\n  path    =  '/usr/sbin',\n  command =  'updatedb',\n}  Note: if you do not set the command value it will default to the exec resource title. You must specifiy the path because the exec resource does not inherit paths.", 
            "title": "Resources"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#classes", 
            "text": "Classes define a collection of resources that are managed together as a single unit. You can also think of them as named blocks of Puppet code, which are created in one place and invoked elsewhere.  Using a Puppet class requires two steps.   First, you'l need to define it by writing a class definition and saving it in a manifest file. When Puppet runs, it will parse this manifest and store your class definition.  Secondly, The class can then be declared to apply it to nodes in your infrastructure.   example class for ssh that contains 3 resources { package, file, service}..  resource     class\n\npackage  \\\nfile       -  ssh\nservice  /  Example class syntax....{with no relationships defined}  class ssh {\n  package  { 'openssh-clients':\n    ensure  =  present,\n  }\n\n  file { '/etc/ssh/ssh_config':\n    ensure  =  file,\n    owner   =  'root',\n    group   =  'root',\n    source  =  'puppet:///modules/ssh/ssh_config',\n  }\n\n  service { 'sshd':\n    ensure  =  running,\n    enable  =  true,\n  }\n}  Defining a class specifies the contents and behaviour but does not automatically include (apply) it in a configuration  Declaring a class, includes the class in the catalogue which will then be applied upon next agent run.  To declare a class use either of the following syntaxes in site.pp.. or in  /tests/init.pp     include my_calss  \nor\n     class { 'my_class': }    classes are reusable  classes are singleton  classes can only be used once per node   When applying a class make sure you run  puppet apply module/tests/init.pp  and NOT  puppet apply module/manifest/init.pp  otherwise no actions will take place and no errors will be logged, this is because you would be defining the class but not declared it anywhere (on a  node).  Parameterised Classes  Class parameters provide a method to set variables in a class as it's declared.\nThe syntax for parameterised classes is similar to resource declarations.  class { 'ntp':\n  servers = \n  ['node1.example.com','node2.example.com','node3.example.com']\n}  The servers parameter can be populated with a single server or an array of servers.  Parameterised class definitions can be set in the site.pp file.  node default {\n  # This is where you can declare classes for all nodes.\n  # Example:\n  #   class { 'my_class': }\n  class { 'ntp':\n    servers = \n    ['node1.example.com','node2.example.com','node3.example.com']\n  }\n}  You can also override values with  class { '::mysql::server':\n  override_options =  { 'mysqld' =  { 'max_connections' =  '1024' } },\n}  Class Inheritance / Derived Classes  By using the inherits keyword classes can be derived from other classes.  When a derived class is declared, its base class is automatically declared first and the variable are set as the parent scope.\nThe new class receives a copy of all the base class's variables and resource defaults.\nCode in the derived class is able to override any resource attributes that were originally set in the base class.  Class Inheritance is only useful for overriding resource attributes. For any other use case it is better archive your objective by using  some other method.  example...  We create a new class zsh::developer that will enable us to deploy the zshrc.dev file instead of the zshrc file on certain nodes..  [root@learn modules]# tree zsh/\nzsh/\n\u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 zshrc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zshrc.dev\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 developer.pp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u251c\u2500\u2500 Modulefile\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec_helper.rb\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 developer.pp\n    \u2514\u2500\u2500 init.pp  cat zsh/manifests/init.pp  class zsh {\n  package { 'zsh':\n    ensure =  present,\n    before =  File['/etc/zshrc'],\n  }\n  file { '/etc/zshrc':\n    ensure =  file,\n    owner  =  'root',\n    group  =  'root',\n    source =  'puppet:///modules/zsh/zshrc',\n  }\n}  [root@learn modules]# cat zsh/manifests/developer.pp  #Note there is no need for a \"package { 'zsh':\" resource definition here as it will be inherited from the zsh class\nclass zsh::developer inherits zsh {\n  File ['/etc/zshrc'] {\n    source =  'puppet:///modules/zsh/zshrc.dev',\n    }\n}  [root@learn modules]# cat zsh/tests/init.pp  include zsh  [root@learn modules]# cat zsh/tests/developer.pp  include zsh::developer  Variables and Class  Parameters  Variables  are prefixed with the $ sign and assigned with the = operator  $mystring = 'this is my string'\n$mypath   = '/tmp/puppet/'  Once a variable it is defined it can be used anywhere in the manifest in place of an regular assigned value.    Unlike resource declarations, variable assignments are parse-order dependent. This means that you must assign a variable in your manifest before you can use it.    If you try to use a variable that has not been defined, the Puppet parser won't complain. Instead, Puppet will treat the variable as having the special undef value.    You can only assign a variable once within a single scope. Once it's assigned, the value cannot be changed. \n(therefore it should be called a constant but hey its not like the puppet people actually follow best practices or common conventions anywhere else so why would this be any different)    Variable interpolation  enables a string that is stored as a variable to be inserted into another string. eg..  file { \"${mypath}file1.txt\":\n  ...\n}\nfile { \"${mypath}file2.txt\":\n  ...\n}  Note:  A string that includes an interpolated variable must be wrapped in double quotation marks (\"...\"), rather than the single quotation marks that surround an ordinary string  Class Parameters  provide a method of setting the variables within a class when the class is declared rather than when it is defined.  Once defined a  Parameterised Class  can be declared with a similar syntax to a resource declaration  When defining a class, include a list of parameters and optional default values between the class name and the opening curly brace. So a parameterised class is defined as below:  class classname ( $parameter = 'default' ) {\n  ...\n }  Once defined, a parameterised class can be declared with a syntax similar to that of resource declarations, including key value pairs for each parameter you want to set.  class {'classname':\n  parameter =  'value',\n}  Seamus sample module using defined resource type class with paramaters (as described in the \"defined resource section above\"  [root@master modules]# tree definedmod  definedmod\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 definedresource.pp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp  [root@master modules]# cat definedmod/manifests/init.pp  class definedmod {\n  definedresource {'mydefinedresource':\n         var2 =  'twovar',\n         var1 =  'variable1',\n         var3 =  'thirdvariable',\n  }\n}  [root@master modules]# cat definedmod/manifests/definedresource.pp  define definedresource ($var1, $var2 = default2 , $var3 = default3) {\n\n  file { \"/tmp/${var1}\" :\n    ensure  =  present,\n    content =  \"this file name is not $var2 or $var3\",\n  }\n\n  file { \"/tmp/${var2}\" :\n    ensure  =  present,\n    content =  \"this file name is not $var1 or $var3\",\n  }\n\n}  [root@master modules]# cat definedmod/tests/init.pp  include definedmod\n\n[root@node4 ~]# cat /tmp/variable1 \nthis file name is not twovar or thirdvariable\n\n[root@node4 ~]# cat /tmp/twovar \nthis file name is not variable1 or thirdvariable\n\n[root@node4 ~]# cat /tmp/thirdvariable \nthis file name is not variable1 or twovar[root@node4 ~]#", 
            "title": "Classes"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppet-lint", 
            "text": "A 3rd party tool used to display common syntax and style errors. Packaged as a ruby gem.  gem install puppet-lint\n\npuppet-lint /etc/puppetlabs/puppet/modules/ntplint/manifests/init.pp  you can ignore certain syntax checks  on command line  puppet-lint   path / file .pp --no-80chars-check\npuppet-lint   path / file .pp --no-ensure_first_param-check  ~/.puppet-lint.rc  --no-80chars-check\n--no-ensure_first_param-check  using a rake file to test multiple manifests in one go by including the following values in your rake file  require 'puppet-lint/tasks/puppet-lint'\nPuppetLint.configuration.send(\"disable_ check \")", 
            "title": "puppet-lint"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#idempotency", 
            "text": "By default puppet modules describe the desired final state rather than detail a series of step to follow, this mean that no matter how many times they are run the same end state will occur.", 
            "title": "Idempotency"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppet-console", 
            "text": "The puppet enterprise console is a web-based gui. It can help..   manage node requests to join the puppet deployment  assign puppet classes to nodes and groups  view reports and activity graphs  browse and compare resources on your nodes  view inventory data  manage console users and their access privileges   Event Inspector  The Event Inspector is part of the Puppet Enterprise (PE) Console. \nIt is a reporting tool that provides data for investigating the current state of your infrastructure. \nIts focus is on correlating information and presenting it from multiple perspectives, in order to reveal common causes behind related events.\nEvent inspector lets you accomplish two important tasks: monitoring a summary of your infrastructure\u2019s activity and analyzing the details of important changes and failures.\nIt displays events from three perspectives Classes Nodes and Resources", 
            "title": "Puppet Console"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#resetting-the-admin-user-password", 
            "text": "This must be performed from a shell session on the console server and executing a ruby script that was part of the Puppet Enterprise installer.   q_puppet_enterpriseconsole_auth_password=newpassword \nq_puppetagent_certname=$(puppet config print certname) \n/opt/puppet/bin/ruby update-superuser-password.rb", 
            "title": "Resetting the admin user password"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#environments", 
            "text": "By default, all nodes are assigned to a default environment named production.  There are three ways to assign nodes to a different environment:   Via your ENC or node terminus  Via each agent node\u2019s puppet.conf  Via the PE console to set the environment for each node group.   Assigning Environments Via an ENC  The interface to set the environment for a node will be different for each ENC. Some ENCs cannot manage environments.  When writing an ENC, simply ensure that the environment: key is set in the YAML output that the ENC returns. See the documentation on writing ENCs for details.  If the environment key isn\u2019t set in the ENC\u2019s YAML output, the Puppet master will just use the environment requested by the agent.\nAssigning Environments Via the Agent\u2019s Config File  Assigning Environment via agent config  In puppet.conf on each agent node, you can set the environment setting in either the agent or main config section.\nWhen that node requests a catalog from the Puppet master, it will request that environment.\nNote: If you are using an ENC and it specifies an environment for that node, it will override whatever is in the config file.  Assigning Environments via PE console \nClick Classification\n   - select node group\n       - click \"edit node group metadata\"\n          - select enviroment", 
            "title": "Environments"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#scope", 
            "text": "Scope helps to organise classes, telling Puppet where to look within the module directory structure to find each class. It also separates namespaces within the module and your Puppet manifests, preventing conflicts between variables or classes with the same name.  The init.pp in the  /manifest/ directory must contain a class with the same name as the module name  Class name   =  mysql\nFile location =  /modules/mysql/manifests/init.pp    )  Class name   =  mysql::server\nFile location = /modules/mysql/manifests/server.pp  Class name    =  mysql::server::account_security\nFile location = /modules/mysql/manifests/server/account_security.pp", 
            "title": "Scope"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#types-and-providers", 
            "text": "A  type  defines the interface for a resource: the set of properties you can use to define a desired state for the resource, and the parameters that don't directly map to things on the system, but tell Puppet how to manage the resource.\nBoth properties and parameters appear in the resource declaration syntax as attribute value pairs.  command to show the types available  [root@master ~]# puppet resource --type\n\n+-----------------------------------------------------------------------------+\n| anchor            |  mcx                        |   puppetdb_conn_validator |\n| apt_key           |  mount                      |   resources               |\n| augeas            |  nagios_command             |   router                  |\n| computer          |  nagios_contact             |   schedule                |\n| cron              |  nagios_contactgroup        |   scheduled_task          |\n| exec              |  nagios_host                |   selboolean              |\n| file              |  nagios_hostdependency      |   selmodule               |\n| file_line         |  nagios_hostescalation      |   service                 |\n| filebucket        |  nagios_hostextinfo         |   ssh_authorized_key      |\n| firewall          |  nagios_hostgroup           |   sshkey                  |\n| firewallchain     |  nagios_ser^ice             |   stage                   |\n| group             |  nagios_ser|icedependency   |   tidy                    |\n| host              |  nagios_ser|iceescalation   |   user                    |\n| ini_setting       |  nagios_ser|iceextinfo      |   vlan                    |\n| ini_subsetting    |  nagios_servicegroup        |   whit                    |\n| interface         |  nagios_timeperiod          |   yumrepo                 |\n| k5login           |  notify                     |   zfs                     |\n| macauthorization  |  package                    |   zone                    |\n| mailalias         |  postgresql_conf            |   zpool                   |\n| maillist          |  postgresql_psql            |                           |\n+-----------------------------------------------------------------------------+  Core Types  File  manages local files and ensures if a file should exist sets its parameters\npresent / absent\nfile / directory / link\nsource puppet:///modules/  Package  Manages software packages\nensures that a package is present / absent / latest /purged / version  Service  Manages services\nensures running / stopped\nenable true / false  notify  echo's message to the agent run-time log\n message =  \" hello world ! \",  exec  executes an arbitrary command on the agent node.  cron  manages cron jobs  user  manages user accounts\nensure present / absent / role\nname, uid, gid, groups,  home, shell  group  ensure present / absent\nname, gid\nA  provider  is what does the heavy lifting to bring the system into line with the state defined by a resource declaration.\nProviders are implemented for a wide variety of supported operating systems.\nThey are a key component of the Resource Abstraction Layer (RAL), translating the universal interface defined by the type into system-specific implementations.  Resource Abstraction Layer     +---------------------------------------+\n   | file | package | service  |  user     |  Resources\n   |---------------------------------------|\n   | Ruby | Apt     | Redhat   | Useradd   |  Providers\n   |      | Yum     | Launchd  | LDAP      |\n   |      | Gems    | SMF      | Netinfo   |\n   |      | Deb     | Debian   |           |\n   |      | RPM     |          |           |\n   +---------------------------------------+", 
            "title": "Types and Providers"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#conditional-statements", 
            "text": "Note:   string matches are not case sensitive  if   (optional elseif and else)  class accounts ($name) {\n\n  if $::operatingsystem == 'centos' {\n    $groups = 'wheel'\n  }\n  elsif $::operatingsystem == 'debian' {\n    $groups = 'admin'\n  }\n  else {\n    fail( \"This module doesn't support ${::operatingsystem}.\" )\n  }\n\n  notice ( \"Groups for user ${name} set to ${groups}\" )\n\n  ...\n\n}  unless  unless $memorysize   1024 {\n  $maxclient = 500\n }  case  case $::operatingsystem {\n  'CentOS': { $apache_pkg = 'httpd' }\n  'Redhat': { $apache_pkg = 'httpd' }\n  'Debian': { $apache_pkg = 'apache2' }\n  'Ubuntu': { $apache_pkg = 'apache2' }\n  default: { fail(\"Unrecognised operating system for webserver.\") }\n}\n\npackage { $apache_pkg :\n  ensure =  present,\n}  selector  Selector statements are similar to case statements, but return a value instead of executing a code block.\nSelectors can only be used at places in the code where a plain value is expected an not inside another selector or case statement.  $rootgroup = $::osfamily ? {\n  'Solaris'  =  'wheel',\n  'Darwin'   =  'wheel',\n  'FreeBSD'  =  'wheel',\n  'default'  =  'root',\n}  my sample class with all the selectors  [root@pe-puppet modules]# cat  noticemod/manifests/init.pp   class noticemod {\n  notify { \"Hello World!\": }\n\n## SELECTOR statement\n  notify { \" SELECTOR warning !\":\n    message =  $seamus ? {\n      'mytrue' =  \" the value is true.\",\n      'myfalse' =  \" the value is false.\",\n      default =  \" the value is not set.\",\n      }, # selector statements warn you if no match is found and there is no default\n  }\n\n## CASE statement\n  case $seamus {\n    'mytrue':   { notify { \" CASE warning ! the value is true.\": } }\n    'myfalse':  { notify { \" CASE warning ! the value is false.\": } }\n    default: { fail(\" CASE warning ! failing now... if the variable is undef the whole catalog compile will fail\") }\n  } # case statements silently fall through the bottom if no match is found\n\n\n## IF statement \n  if 'rue' in $seamus {\n      notify { \" IF warning ! the value is true.\": } \n  }\n  elsif 'alse' in $seamus {\n      notify { \" IF  warning ! the value is false.\": } \n  }\n  else {\n      notify { \" IF  warning ! the value does not match.\": }\n  }\n\n\n## UNLESS statement\n\n  unless $seamus == 'myfalse' {\n    notify { \" UNLESS statement - \\$seamus does not == myfalse, it is set to $seamus \": } \n  }\n\n  unless $seamus == 'mytrue' {\n    notify { \" UNLESS statement - \\$seamus does not == mytrue, it is set to $seamus \": } \n  }\n\n\n}", 
            "title": "Conditional Statements"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppetdb", 
            "text": "stores the most recent facts from every node  the most recent catalog from every node  (optionally) 7 days of event reports from every node   it is searchable using either...   PuppetDB's query API  Puppet's inventory service  exported resources   example  [root@learning ~]# puppet node status learning.puppetlabs.vm\nlearning.puppetlabs.vm\nCurrently active\nLast catalog: 2015-01-24T13:31:16.291Z\nLast facts: 2015-01-24T13:31:03.782Z  Declaring an exported resource causes that resource to be added to the catalog and marked with an \u201cexported\u201d flag, which prevents the puppet agent from managing the resource. When PuppetDB receives the catalog, it stores a record for each resource with the flag set.  Delete reports older then 1 month :  sudo -u puppet-dashboard rake RAILS_ENV=production reports:prune upto=1 unit=mon", 
            "title": "PuppetDB"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#hiera", 
            "text": "https://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/  Hiera is a key/value lookup tool for configuration data, built to make puppet better and let you set node-specific data without repeating yourself.\nHiera support is built into puppet 3, and is available as an add-on for puppet 2.7.  It keeps site specific data out of the manifest.\nPuppet classes can query any data they need and hiera will act as a site wide config file.  Hiera makes it easier to:\n1. separate configuration data from the modules code.\n2. configure your own nodes: default data with multiple levels of overrides.\n3. re-use public puppet modules: you don't have to edit the code you just have to put the necessary data in hiera.\n4. makes it easier to publish your own developed modules to the forge without clashing variable names or having to redact your config values.\n5. create common data for most nodes and..\n         - Override some values for machines located at a particular facility\n         - Override some values for specific machines\n6. enables you to only write down the differences that are needed (doesn't make sense)  To get started with hiera there are 5 steps..\n1) download and install hiera\n2) create a hiera.yaml config file\n3) arrange a hierachy that suite your site and data\n4) write your data sources\n5) configure puppet to use hiera  https://ask.puppetlabs.com/question/13592/when-to-use-hiera-hiera_array-and-hiera_hash/\nhiera - Performs a standard priority lookup and returns the most specific value for a given key. The returned value can be data of any type (strings, arrays, or hashes).  hiera_array - Returns all matches throughout the hierarchy \u2014 not just the first match \u2014 as a flattened array of unique values. If any of the matched values are arrays, they\u2019re flattened and included in the results.  hiera_hash - Returns a merged hash of matches from throughout the hierarchy. In cases where two or more hashes share keys, the hierarchy order determines which key/value pair will be used in the returned hash, with the pair in the highest priority data source winning.  example of using hiera....  [root@learn ~]# vim  /etc/puppetlabs/puppet/hieradata/common.yaml  ---\nmessage: This string is the value that is returned when hiera('message') is called.\nmotd:  Hello there Seamus  test that the key is set correctly using the puppet apply -e  [root@learn hieradata]# puppet apply -e 'notice(hiera(\"motd\"))'\nNotice: Scope(Class[main]): Hello there Seamus\nNotice: Compiled catalog for learn.puppetlabs.com in environment production in 0.04 seconds\nNotice: Finished catalog run in 0.02 seconds  Edit the motd module to use the key from hiera instead of the value in the module files  [root@learn hieradata]# cd /etc/puppetlabs/puppet/modules/\n[root@learn modules]# tree motd/\nmotd/\n\u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 init.pp\n\u251c\u2500\u2500 Modulefile\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 spec\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spec_helper.rb\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 init.pp  [root@learn modules]# vim motd/manifests/init.pp    Class: motd\n\nclass motd {\n\n  file { '/etc/motd':\n    ensure  =  file,\n    owner   =  'root',\n    group   =  'root',\n    content =   (hiera(\"motd\")),\n  }\n\n}  [root@learn modules]# puppet parser validate motd/manifests/init.pp\n[root@learn modules]# puppet apply motd/tests/init.pp\nNotice: Compiled catalog for learn.puppetlabs.com in environment production in 0.09 seconds\nNotice: /Stage[main]/Motd/File[/etc/motd]/ensure: defined content as '{md5}13c0016553b5315a8cd66f13b23f14a3'\nNotice: Finished catalog run in 0.03 seconds  [root@learn modules]#  cat /etc/motd  Hello there Seamus", 
            "title": "Hiera"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#facter", 
            "text": "Facter stores facts as pairs of keys and values.  Facter command line options  -y, --yaml                       Emit facts in YAML format.\n-j, --json                       Emit facts in JSON format.\n    --plaintext                  Emit facts in plaintext format.\n    --trace                      Enable backtraces.\n    --external-dir DIR           The directory to use for external facts.\n    --no-external-dir            Turn off external facts.\n-d, --debug                      Enable debugging.\n-t, --timing                     Enable timing.\n-p, --puppet                     Load the Puppet libraries, thus allowing Facter to load Puppet-specific facts.\n-v, --version                    Print the version and exit.\n-h, --help                       Print this help message.  A shell enviroment variable prepened with the name  FACTER_    will be readable by facter as a fact. eg...  [root@node1 ~]# export FACTER_seamus=murray\n[root@node1 ~]# facter seamus\nmurray\n[root@node1 ~]# facter |grep seamus\nseamus =  murray  Facter values can be  temporarily overridden  by setting a shell variable prepended with  FACTER_  and the name of the fact. eg...  FACTER_operatingsystem=Debian  You can even test the effect that the fake fact will have using puppet apply..  FACTER_operatingsystem=Debian puppet apply --noop accounts/tests/init.p  The best way to distribute facts is to include them in your modules using the puppet plug-ins.\nPuppet will distribute the custom facts, custom types, providers and functions to any host that includes the module.  you can either distribute the facts with the modules that need them or you can create a module combining all your custom facts needed by all your other modules.  Facter is called by the puppet agent.\nFacts apear as normal and top scope variables they can be called by  $ipaddress  or  ${::ipaddress}  (best practice)  Pluginsync  Custom Facts and types can be exported to the the agent nodes from the puppet master using the plugin sync mechanism.\nBefore the client requests a catlog it checks to see if the are any new plugins or if its plugins have been updated. \nIf there has been a change it pulls down the plugins and executes the code in the custom types and facts.  This and be disabled from the agent nodes  puppet.conf  config file  [main]\npluginsync = false  Custom facts  you can create a custom fact with a bit of ruby code on the puppet master and use plugins and modules to distribute it to the clients/nodes  Create a ruby file in  modulename /lib/facter/ modulename .rb  For simple shell commands, just insert the shell command in between the  setcode do  and the  end .\nFor more complex comands use  Facter::Core::Execution.exec(' ')  between the  setcode do  and the  end .  Facter.add('mycustomfact') do\n  setcode do\n    Facter::Core::Execution.exec('/bin/date')\n  end\nend  on the agents use plugin sync to automatically pull down the facts to  /var/lib/puppet/lib/facter/mycustomfact.rb  puppet agent -t  use the  -p  facter flag to test the puppet fact  facter -p  mycustomfact  A fact can be confined to run only if another fact is of a certain value  e.g.  Facter.add(:powerstates) do\n  confine :kernel =  'Linux'\n  setcode do\n    Facter::Core::Execution.exec('cat /sys/power/states')\n  end\nend  External facts  Can be written in any language as long as they return key/value pairs to stdout.  They are stored in ..  MODULEPATH / MODULE /facts.d/  and are distributed to the clients via the pluginsync mechanism.  Example fact written in bash...  $ cat/etc/puppet/modules/myexternalfact/facts.d/myexternalfacts.sh\n#!/bin/bash\necho \"myexternalfact1=`/bin/df -P / |/usr/bin/tail -1 | /bin/awk '{print $5}'`\"\necho \"extfact1=one\"\necho \"extfact2=two\"\necho \"extfact3=three\"  Calling the external fact from the command line on a node that has recieved  the external fact by pluginsync...  $ facter --external-dir  /var/lib/puppet/facts.d/ extfact1 extfact2 extfact3 myexternalfact1\nmyexternalfact1 =  33%\nextfact1 =  one\nextfact2 =  two\nextfact3 =  three  Example fact written in c...  #include  stdio.h \n\nint main() {\n   printf(\"my_external_fact_in_c=\\\"Hello World\\\"\\n\");\n return 0;\n}  Calling the external fact written in c...  # facter --external-dir ~/puppet-study/external-facts/ my_external_fact_in_c\n\"Hello World\"  Distributing the external facts to the puppet client nodes  It is best to put the external facts into a module so that they can be distributed to all the agent nodes via plugin sync..  [root@master myexternalfact]# tree  /etc/puppet/modules/myexternalfact\n/etc/puppet/modules/myexternalfact\n\u2514\u2500\u2500 facts.d\n    \u251c\u2500\u2500 myexternalfactinc\n    \u2514\u2500\u2500 myexternalfact.sh\n\n\n[root@master ~]# ls -l /etc/puppet/modules/myexternalfact/facts.d/\ntotal 16\n-rwxr-xr-x. 1 root root 8523 Feb 10 00:27 myexternalfactinc\n-rwxr-xr-x. 1 root root  235 Feb 10 00:36 myexternalfact.sh  Trigger the plugin sync to trasfer the new external facts..  [root@node1 ~]# puppet agent -t  Use the  puppet apply -e  tool on the agent nodes to test the facter values..  [root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_c}\" ) '\nNotice: Scope(Class[main]): \"Hello World\"\nNotice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds\nNotice: Finished catalog run in 0.03 seconds\n\n[root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_1}\" ) '\nNotice: Scope(Class[main]): 32%\nNotice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds\nNotice: Finished catalog run in 0.03 seconds\n\n[root@node1 ~]# puppet apply -e ' notice( \"${::my_external_fact_in_bash_2}\" ) '\nNotice: Scope(Class[main]): two\nNotice: Compiled catalog for node1.puppetlabs in environment production in 0.02 seconds\nNotice: Finished catalog run in 0.03 seconds  Structured facts  Can come straight from YAML,JSON or txt files in the directories..  /etc/facter/facts.d/   or  /etc/puppetlabs/facter/facts.d/  $ cat /etc/facter/facts.d/mystructuredfact.yaml\n---\nmystructuredfact1: factone\nmystructuredfact2: facttwo\nmystructuredfact3: factthree\n\n$ facter mystructuredfact1 mystructuredfact2 mystructuredfact3\nmystructuredfact1 =  factone\nmystructuredfact2 =  facttwo\nmystructuredfact3 =  factthree", 
            "title": "Facter"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#roles-and-profiles", 
            "text": "Roles and profiles are just modules written in a specific way.   roles are assigned to nodes  profiles are assigned to roles   example role  class role::weibserver inherits role {\n  include profile:: web\n}\n\nclass role::dbserver inherits role {\n  include profile::db\n}\n\nclass role::webdbserver inherits role {\n  include profile:web\n  include profile::db\n}  classification = assignment of classes to nodes\nclassification = assignment of roles to nodes  example profile  class profile::web {\n  include apache\n  include php\n  include tomcat\n  include jdk\n  include memcache\n}  __  Tip:  classes are singletons, they can be included multiple times on a node, but they will only be evaluated  once . \nA defined class can be declared multiple times, because it takes parameters and each new declaration will be evaluated.  You can see we\u2019ve included our two classes but not the definition, apache::vhost. This is because of some module magic called autoloading. \nPuppet scans your module and loads any .pp file in the manifests directory that is named after the class it contains; for example, the install.pp file contains the apache::install class and so is autoloaded.  The same thing happens with definitions: The vhost.pp file contains the definition apache::vhost, and Puppet autoloads it. However, as we declare definitions, for example calling apache::vhost where we need it, we don\u2019t need to do an include apache::vhost because calling it implies inclusion.  ...............................................  class puppet::params {\n  $puppetserver = hiera('puppetserver')\n}  In Puppet 3, but not Puppet 2.7, there is an automatic lookup of parameters in a parameterized class. We can use this to rewrite the puppet::params class further:  class puppet::params (\n  $puppetserver,\n){\n}  When this is called with no arguments, Puppet 3 will attempt to look up the puppet::params::puppetserver key in Hiera and populate it accordingly. It will not fail unless the Hiera lookup fails.", 
            "title": "Roles and Profiles"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppet-templates", 
            "text": "Puppet supports templates written in the ERB templating language, which is part of the Ruby standard library.\nTemplates are always evaluated by the parser, not by the client. This means that if you are using a puppet master server, then the templates only need to be on the server, and you never need to download them to the client. The client sees no difference between using a template and specifying all of the text of the file as a string.\nNote that the template function simply returns a string, which can be used as a value anywhere \u2014 the most common use is to fill file contents, but templates can also provide values for variables:  ags  The tags available in an ERB file depend on the way the ERB processor is configured. Puppet always uses the same configuration for its templates (see \u201ctrim mode\u201d below), which makes the following tags available:     %= Ruby expression %    \u2014 This tag will be replaced with the value of the expression it contains.    % Ruby code %    \u2014 This tag will execute the code it contains, but will not be replaced by a value. Useful for conditional or looping logic, setting variables, and manipulating data before printing it.    %# comment %     \u2014 Anything in this tag will be suppressed in the final output.    %% or %%     \u2014 A literal  % or % , respectively.    %- \u2014 Same as  %   , but suppresses any leading whitespace in the final output. Useful when indenting blocks of code for readability.   -%    \u2014 Same as   %    , but suppresses the subsequent line break in the final output. Useful with many lines of non-printing code in a row, which would otherwise appear as a long stretch of blank lines.", 
            "title": "Puppet Templates"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#puppet-parser", 
            "text": "Is the part of puppet that pareses the puppet DSL code in manifests files.\nIt can be called from the command line to validate your puppet code  [root@pe-puppet]# puppet parser validate /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp \n  Error: Could not parse for environment production: Syntax error at '{'; expected '}' at /etc/puppetlabs/puppet/modules/myexternalfact/manifests/init.pp:3", 
            "title": "Puppet parser"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#functions", 
            "text": "Functions are extensions of the Puppet Parser. They are located and are executed on the master and only have access to resources and data that is available on the master. \nDuring the parsing of the manifest's any function code is executed and the subsequent return value/s are inserted into the resulting compilation.  There are two types of functions: statements and rvalues.    Statements , such as the  fail  function, which stops the Puppet run with a parser error, perform some action.    Rvalues  return values and can be used anywhere a normal value is expected. (This includes resource attributes, variable assignments, conditions, selector values, the arguments of other functions, etc.) These values can come from a variety of places;   template  function reads and evaluates a template to return a string.  stdlib\u2019s  str2bool  and  num2bool  functions convert values from one data type to another.  split  function parses a string and returns array elements.", 
            "title": "Functions"
        }, 
        {
            "location": "/puppet/puppet_study_notes/#rbac-role-based-access-control", 
            "text": "Puppet enterprise ships with 3 default  User Roles   Administrators  Operators  Viewers", 
            "title": "RBAC  Role Based Access Control"
        }, 
        {
            "location": "/scripting/Column_tally/", 
            "text": "simple script to tally the numbers in a defined column of text\n\nSample input file..\nrow1 1 2 3 4 5 6 7 8 9\nrow2 2 3 4 5 6 7 8 9 10\nrow3 1 2 3 4 5 6 7 8 9\nrow4 1 2 3 4 5 6 7 8 9\nrow5 1 2 3 4 5 6 7 8 9\nrow6 1 2 3 4 5 6 7 8 9\nrow7 1 2 3 4 5 6 7 8 9\nrow8 1 2 3 4 5 6 7 8 9\nrow9 1 2 3 4 5 6 7 8 9\nrow10 1 2 3 4 5 6 7 8 9\n\nSample outputs\n./column-tally.sh -f input -c second -v ON\ninput is readable\nColumn is second\nrow1    1       1\nrow2    2       3\nrow3    1       4\nrow4    1       5\nrow5    1       6\nrow6    1       7\nrow7    1       8\nrow8    1       9\nrow9    1       10\nrow10   1       11\nThe cumulative value of the second column is 11\n\n./column-tally.sh -f input -c tenth -v ON\ninput is readable\nColumn is tenth\nrow1    9       9\nrow2    10      19\nrow3    9       28\nrow4    9       37\nrow5    9       46\nrow6    9       55\nrow7    9       64\nrow8    9       73\nrow9    9       82\nrow10   9       91\nThe cumulative value of the tenth column is 91\n\n\n\n\n\n#!/bin/bash\n\n\nusage()\n    {\n    cat \n EOF\nusage: $0 -f input.txt -c first second\n       $0 -f input.txt -c first second -v ON\n\nThis script will tally the numerical values in the specified column.\n\nOPTIONS:\n   -f      input file\n   -c      column number  {first,second....tenth)etc..\n   -v      verbosity {ON}\n\nEOF\n    }\n\n\nif [ $# -lt 4 ]\nthen\n  usage\n  exit 1\nfi\n\n\n\n\nwhile getopts \":f:c:v:\" opt; do\n  case $opt in\n    f)\n      InPutFile=$OPTARG\n        if [ -r \"$InPutFile\" ]\n         then\n         echo \"$InPutFile is readable\"\n         else\n         echo \"Unable to read input file --\n $InPutFile !!!\"\n         usage\n         exit 1\n        fi\n      ;;\n    v)\n      Verbose=$OPTARG\n       ;;\n    c)\n      echo \"Column is $OPTARG\"\n      Column=$OPTARG\n        if [[ $Column == \"first\" ||  $Column == 'second' || $Column == 'third' || $Column == 'fourth' || $Column == 'fifth' || $Column == 'sixth' || $Column == 'seventh' || $Column == 'eighth' || $Column == 'ninth' || $Column == 'tenth' ]]\n         then\n           true\n         else\n          echo\" not a valid column value\"\n          echo\" must be either: first second third fourth fifth sixth seventh eighth ninth tenth\"\n          usage\n          exit 1\n        fi  \n      ;;\n    :)\n      echo \"Option -$OPTARG requires an argument.\" \n2\n      usage\n      exit 1\n      ;;\n    ?)\n      echo \"Invalid option: -$OPTARG\" \n2\n      usage\n      exit 1\n      ;;\n    *  )\n      echo \"Unimplemented option: -$OPTARG\" \n2\n      usage\n      exit 1\n      ;;\n  esac\ndone\n\ncumulative=0\n while read -r first second third fourth fifth sixth seventh eighth ninth tenth; do\nlet \" cumulative += Column \"\nlet \" value = Column \"\n  if [[ $Verbose == 'ON' || $Verbose == 'OFF' ]]\n     then\n       true\n     else\n       echo \"invalid Verbosity setting please use -v ON   or -v OFF\"\n       exit 1\n  fi\n  if [ $Verbose == 'ON' ]\n    then\n   echo -e \"$first \\t$value \\t$cumulative\"\n    else\n   true\n  fi\ndone \n $InPutFile\necho \"The cumulative value of the $Column column is $cumulative\"", 
            "title": "Column tally"
        }, 
        {
            "location": "/scripting/Convert_kilobytes_to_MB_GB_TB_PB/", 
            "text": "#!/bin/bash\n#converts kilobytes to Mega/Giga/Tera/Peta\n\n\nusage()\n    {\n    cat \n EOF\nusage: $0 {number of Kilobytes}\neg\n\n# $0 100\n cumulative 100 kB\n\n# $0 1000\n cumulative 1000 kB\n\n# $0 1024\n cumulative 1024 kB\n1 MB\n\n# $0 100000\n cumulative 100000 kB\n97 MB\n\nEOF\n    }\n\n\nif [ $# != 1 ]\nthen\n  usage\n  exit 1\nfi\n\n\n\n\n\ncumulative=$1\necho cumulative $cumulative kB\nlet cumulativeMB=$cumulative/1024\n  if [ $cumulativeMB -ge \"1\" ]\n   then\n   echo $cumulativeMB MB\n  fi\nlet cumulativeGB=$cumulativeMB/1024\n  if [ $cumulativeGB -ge \"1\" ]\n   then\n   echo $cumulativeGB GB\nlet remainderGB=$cumulativeMB-1024\n   echo $remainderGB MB\n  fi\nlet cumulativeTB=$cumulativeGB/1024\n  if [ $cumulativeTB -ge \"1\" ]\n   then\n   echo $cumulativeTB TB\n  fi", 
            "title": "Convert kilobytes to MB GB TB PB"
        }, 
        {
            "location": "/scripting/DNS_validate_entries/", 
            "text": "A a script I created to validate the entries in DNS without having address to the zone files or being able to perform a zone transfer\n\n\n#!/bin/bash\n#This script will perform..\n# 1. forward lookups on a list of dns shortnames\n# 2. it will then perform reverse lookups on the ips returned\n# 3. it will then perform the same steps again for various sub domains\n# Note: replace example with your domain name , input a list of either shortnames or FQDNs\n\nfor short_name in `head  list_systems | awk -F. '{ print $1 }'` \n  do\n  domain_name=\".example.local\"\n  host $short_name$domain_name\n    if [ $? = '0' ] ; then\n     fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" \n     host $fw_ip  \n         if [ $? = '0' ] ; then\n           bw_ip=\"`host $fw_ip | awk '{ print $5 }'`\"\n            \n out\n          else \n           echo \"$short_name$domain_name reverse failed $fw_ip\" \n errors\n           bw_ip=\"\"\n         fi\n    else\n     echo \"$short_name$domain_name forward failed\" \n errors\n     fw_ip=\"\"\n    fi\n\n  domain_name=\".management.example.local\"\n  host $short_name$domain_name\n    if [ $? = '0' ] ; then\n     m_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" \n     host $fw_ip  \n         if [ $? = '0' ] ; then\n           m_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\"\n           echo $short_name$domain_name\",\"$m_fw_ip\",\"$m_bw_ip\",\" \n m_out\n          else \n           echo \"$short_name$domain_name reverse failed $m_fw_ip\" \n m_errors\n           m_bw_ip=\"\"\n         fi\n    else\n     echo \"$short_name$domain_name forward failed\" \n errors\n     m_fw_ip=\"\"\n    fi\n\n  domain_name=\".backupnetwork.example.local\"\n  host $short_name$domain_name\n    if [ $? = '0' ] ; then\n     b_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" \n     host $fw_ip  \n         if [ $? = '0' ] ; then\n           b_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\"\n           echo $short_name$domain_name\",\"$b_fw_ip\",\"$b_bw_ip\",\" \n b_out\n          else \n           echo \"$short_name$domain_name reverse failed $b_fw_ip\" \n b_errors\n           b_bw_ip=\"\"\n         fi\n    else\n     echo \"$short_name$domain_name forward failed\" \n errors\n     b_fw_ip=\"\"\n    fi\n\n  echo $short_name$domain_name\",\"$fw_ip\",\"$bw_ip\",\"\"$m_fw_ip\",\"$m_bw_ip\",\"\"$b_fw_ip\",\"$b_bw_ip\",\" \n out\ndone", 
            "title": "DNS validate entries"
        }, 
        {
            "location": "/scripting/IP_survey/", 
            "text": "#!/bin/bash\n\nHOST_NAME=`hostname -s`\nNICS=`ls /sys/class/net | grep -v lo`\n\n\nfor i in $NICS\n  do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://`\ndone\n\n\n#Server_Name eth0 10.10.10.10 Mask:255.255.252.0\n#Server_Name eth1 10.11.11.11 Mask:255.255.252.0\n#Server_Name eth2 192.168.0.240 Mask:255.255.255.128", 
            "title": "IP survey"
        }, 
        {
            "location": "/scripting/Shuffle_deck_of_cards/", 
            "text": "#!/bin/bash\n#bash script to shuffle a deck of 52 cards\nfor ((i=0;i\n52;i++))\n do\n  cards[$i]=$i\n done\nfor ((i=51; i\n0; i-- ))\n do\n  rand=$(( $RANDOM % (i+1) ))\n  #echo \"rand = $rand $RANDOM\"\n  tmp=${cards[i]} cards[i]=${cards[rand]}\n  cards[rand]=$tmp\n done\necho ${cards[@]}\n\n\n\n\n\nuse the seq and sort commands to do the work for you\n\n\n # seq 1 52 \n deck\n # sort -R deck", 
            "title": "Shuffle deck of cards"
        }, 
        {
            "location": "/scripting/bash_and_grep_script_to_compare_2_lists/", 
            "text": "reverse grep to discover values from one list that are not in another list\n\n\nSample Input Lists\n\n\n[user@server]# cat list-full      [user@server]# cat list-partial\n1                                 1\n2\n3                                 3\n4\n5                                 5\n6\n7                                 7\n8\n9                                 9\n10\n\n\n\nSample output\n\n\n[user@server]# ./list-diff.sh\n2\n4\n6\n8\n10\n[user@server]# ./list-match.sh\n1\n3\n5\n7\n9\n\n\n\n\n\nlist-match.sh\n\n\n#!/bin/bash\n# this will compare two lists and output the lines that appear in both\nfor i in `cat list-partial`\n    do if  grep -q -v $i list-full\n        then echo $i\n       else\n        true\n      fi\n    done\n\n\n\nlist-diff.sh\n\n\n#!/bin/bash\n#this will compare two lists and output the lines that appear in the list-full only\n\nfor i in `cat list-full`\n    do if grep -q $i list-partial\n        then true\n        else echo $i\n       fi\n    done\n\n\n\nExample application\n\n\nYou have 2 list a list containing all your servers and another list of the servers with a software package installed.\n\n\nfor i in `cat systems_list_2014-02-20-sorted` \n  do\n    if grep -q  $i package_listinstalledsystems_example-1.2.3-4.x86_6-sorted \n      then true \n      else echo $i \n    fi \ndone \n systems_without__example-1.2.3-4.x86_6", 
            "title": "Bash and grep script to compare 2 lists"
        }, 
        {
            "location": "/scripting/bash_argument_shift_until_loop/", 
            "text": "#!/bin/bash\n#bash script to shift through the arguments one by one using an until loop\n\nif  [ \"$#\" -le 0 ]; then\n    echo \"You need to enter at least 1 argument\"\nfi\n\nuntil [ -z \"$1\" ] # until there are no arguments left\ndo\n  echo -n \"$1 \"\n  sleep 1 \n  shift\ndone\n\necho # newline", 
            "title": "Bash argument shift until loop"
        }, 
        {
            "location": "/scripting/bash_argument_shift_while_loop/", 
            "text": "#!/bin/bash\n#bash script to shift through the arguments one by one using a while loop\n\nif  [ \"$#\" -le 0 ]; then\n    echo \"You need to enter at least 1 argument\"\nfi\n\nwhile (( $# \n 0 ))\ndo\n   echo \"parsing the arguments $*\"\n   echo $1\n   shift \ndone", 
            "title": "Bash argument shift while loop"
        }, 
        {
            "location": "/scripting/bash_arguments/", 
            "text": "#!/bin/bash\n if [ -z \"$1\" ]; then\n     echo usage: $0 \"{arg} {arg} etc..\"\n      exit\n fi\n\n if [ $# = 1 ]; then\n\n      echo you specified a single argument\n      exit\n fi\n if [ $# -gt 1 ]; then\n\n      echo your specified $# arguments\n      exit\n fi", 
            "title": "Bash arguments"
        }, 
        {
            "location": "/scripting/bash_capture_keyboard_entry/", 
            "text": "#!/bin/bash\n# Author: Sigurd Solaas, 20 Apr 2011\n# Used in ABS Guide with permission.\n# Requires version 4.2+ of Bash.\n# handy script to capture keyboard input and take action\nkey=\"no value yet\"\nwhile true; do\n  clear\n  echo \"Bash Extra Keys Demo. Keys to try:\"\n  echo\n  echo \"* Insert, Delete, Home, End, Page_Up and Page_Down\"\n  echo \"* The four arrow keys\"\n  echo \"* Tab, enter, escape, and space key\"\n  echo \"* The letter and number keys, etc.\"\n  echo\n  echo \"    d = show date/time\"\n  echo \"    q = quit\"\n  echo \"================================\"\n  echo\n\n # Convert the separate home-key to home-key_num_7:\n if [ \"$key\" = $'\\x1b\\x4f\\x48' ]; then\n  key=$'\\x1b\\x5b\\x31\\x7e'\n  #   Quoted string-expansion construct. \n fi\n\n # Convert the separate end-key to end-key_num_1.\n if [ \"$key\" = $'\\x1b\\x4f\\x46' ]; then\n  key=$'\\x1b\\x5b\\x34\\x7e'\n fi\n\n case \"$key\" in\n  $'\\x1b\\x5b\\x32\\x7e')  # Insert\n   echo Insert Key\n  ;;\n  $'\\x1b\\x5b\\x33\\x7e')  # Delete\n   echo Delete Key\n  ;;\n  $'\\x1b\\x5b\\x31\\x7e')  # Home_key_num_7\n   echo Home Key\n  ;;\n  $'\\x1b\\x5b\\x34\\x7e')  # End_key_num_1\n   echo End Key\n  ;;\n  $'\\x1b\\x5b\\x35\\x7e')  # Page_Up\n   echo Page_Up\n  ;;\n  $'\\x1b\\x5b\\x36\\x7e')  # Page_Down\n   echo Page_Down\n  ;;\n  $'\\x1b\\x5b\\x41')  # Up_arrow\n   echo Up arrow\n  ;;\n  $'\\x1b\\x5b\\x42')  # Down_arrow\n   echo Down arrow\n  ;;\n  $'\\x1b\\x5b\\x43')  # Right_arrow\n   echo Right arrow\n  ;;\n  $'\\x1b\\x5b\\x44')  # Left_arrow\n   echo Left arrow\n  ;;\n  $'\\x09')  # Tab\n   echo Tab Key\n  ;;\n  $'\\x0a')  # Enter\n   echo Enter Key\n  ;;\n  $'\\x1b')  # Escape\n   echo Escape Key\n  ;;\n  $'\\x20')  # Space\n   echo Space Key\n  ;;\n  d)\n   date\n  ;;\n  q)\n  echo Time to quit...\n  echo\n  exit 0\n  ;;\n  *)\n   echo You pressed: \\'\"$key\"\\'\n  ;;\n esac\n\n echo\n echo \"================================\"\n\n unset K1 K2 K3\n read -s -N1 -p \"Press a key: \"\n K1=\"$REPLY\"\n read -s -N2 -t 0.001\n K2=\"$REPLY\"\n read -s -N1 -t 0.001\n K3=\"$REPLY\"\n key=\"$K1$K2$K3\"\n\ndone\n\nexit $?", 
            "title": "Bash capture keyboard entry"
        }, 
        {
            "location": "/scripting/bash_case/", 
            "text": "#!/bin/bash\necho -n what colour \\?\nread colour\necho \"starting an xterm with $colour text\" \ncase $colour in\n[Bb]lu?)\n  xterm -foreground blue \n\n  ;;\n[Gg]reen)\n  xterm -foreground darkgreen  \n\n  ;;\nred)\n   echo red\n  xterm -foreground red  \n\n  ;;\n*)\n  xterm \n\n  ;;\nesac", 
            "title": "Bash case"
        }, 
        {
            "location": "/scripting/bash_fail_and_exit/", 
            "text": "This is a handy function for when you have a script that you need to run interactively \nyou can call it with the || or operator, if any steps fail it will print out the line number and exit\n\n\n#!/bin/bash\nfail_notify() {\n echo \"step \"$@\" on line $BASH_LINENO failed \n\"\n exit\n}\n\nmount -o rw,remount / || fail_notify \"remount of root\"", 
            "title": "Bash fail and exit"
        }, 
        {
            "location": "/scripting/bash_fail_trap/", 
            "text": "Using the bash option set -e / set -o errexit and a trap to discontinue the execution of a script and notify the user if any step fails.\n\n\n#!/bin/bash\n\nfail_notice () {\n  echo \"Command $BASH_COMMAND failed\"\n  exit\n}\n\ntrap fail_notice EXIT\nset -o errexit #Exit the script if any untested command fails.\n\necho \"before fail\"\nfalse # cause non zero exit code and trigger the trap\necho \"after fail\"\n\ntrap Exit # need to reset the Exit trap otherwise the fail_notice() \n          # will be triggered upon script completion (exit).", 
            "title": "Bash fail trap"
        }, 
        {
            "location": "/scripting/bash_function/", 
            "text": "#!/bin/bash\n#bash functions\nfunction my_fn { \n    echo function has ran with $1 \n}\n\nfunction exit_fn {\n    echo \"exit function called\"\n    exit\n}\n\nmy_fn seamus\nmy_fn seamus2\nexit_fn\necho \"exit_fn call failed\"", 
            "title": "Bash function"
        }, 
        {
            "location": "/scripting/bash_functions/", 
            "text": "from http://tldp.org/LDP/abs/html/complexfunct.html\n\n\n#!/bin/bash\n# Functions and parameters\n\nDEFAULT=default                             # Default param value.\n\nfunc2 () {\n   if [ -z \"$1\" ]                           # Is parameter #1 zero length?\n   then\n     echo \"-Parameter #1 is zero length.-\"  # Or no parameter passed.\n   else\n     echo \"-Parameter #1 is \\\"$1\\\".-\"\n   fi\n\n   variable=${1-$DEFAULT}                   #  What does\n   echo \"variable = $variable\"              #+ parameter substitution show?\n                                            #  ---------------------------\n                                            #  It distinguishes between\n                                            #+ no param and a null param.\n\n   if [ \"$2\" ]\n   then\n     echo \"-Parameter #2 is \\\"$2\\\".-\"\n   fi\n\n   return 0\n}\n\nfunc2 $1 $2\n\n\n\nExecute the script...\n    scriptname.sh 1\n    scriptname.sh 2", 
            "title": "Bash functions"
        }, 
        {
            "location": "/scripting/bash_local_vs_global_variables/", 
            "text": "[user@server]# ./functions-and-variables.sh\nLocal Variable value before calling function1 = Australia\nLocal Variable value during calling function1 = NewZealand\nLocal Variable value after  calling function1 = Australia\nGlobal Variable value before calling function1 = Earth\nGlobal variable value during calling function1 = Mars\nGlobal Variable value after  calling function1 = Mars\n\n[user@server]# cat functions-and-variables.sh\n#!/bin/bash\ncountry=Australia\nplanet=Earth\n\nfunction local_var_function {\n              local country=NewZealand\n              echo \"Local Variable value during calling function1 = $country \"\n            }\n    echo \"Local Variable value before calling function1 = $country \"\n    local_var_function\n    echo \"Local Variable value after  calling function1 = $country \"\n\n\nfunction global_var_function {\n              planet=Mars   #changing the global variable\n              echo \"Global variable value during calling function1 = $planet \"\n            }\n    echo \"Global Variable value before calling function1 = $planet \"\n    global_var_function\n    echo \"Global Variable value after  calling function1 = $planet \"", 
            "title": "Bash local vs global variables"
        }, 
        {
            "location": "/scripting/bash_nested_if_else/", 
            "text": "#!/bin/bash\n#nested if else statement\n\nif [ \"$1\" -gt 1 ]\n then\n   if [ \"$1\" -lt 4 ]\n   then\n   echo is between 2 or 3\n   else\n   echo is greater than or equal to 4\n   fi\n else\n echo is less than  or equal to 1\nfi", 
            "title": "Bash nested if else"
        }, 
        {
            "location": "/scripting/bash_read_input/", 
            "text": "#!/bin/bash\n#simple example of how to read user input with bash\necho \"Please enter a word\"\nread word\necho Please enter your first and last name\nread Last First \necho \"$First $Last, You entered the word $word\"", 
            "title": "Bash read input"
        }, 
        {
            "location": "/scripting/bash_scripting/", 
            "text": "Bash Scripting\n\n\nBash case statement example\n\n\n#!/bin/bash\necho -n \"what colour ?  red green or blue  \"\nread colour\n\ncase $colour in\n[Bb]lu?)\n  echo creating a bash subshell with blue foreground\n  env PS1=\"\\e[1;34m[\\u@\\h \\W]\\$ \\e[m \" bash -i\n  ;;\n[Gg]reen)\n   echo creating a bash subshell with green foreground\n   env PS1=\"\\e[1;32m[\\u@\\h \\W]\\$ \\e[m \" bash -i\n  ;;\nred)\n  echo creating a bash subshell with red foreground\n  env PS1=\"\\e[1;31m[\\u@\\h \\W]\\$ \\e[m \" bash -i\n  ;;\n*)\n  echo \"invalid selection\" \n\n  ;;\nesac\n\n\n\nBash nested if else statement\n\n\n#!/bin/bash\n#bash nested if else statement\nif [ -z  \"$1\" ]\n  then\n    echo \"Usage $0 {integer between 1-10}\"\n    exit 1\nelse\n  true\nfi\nif [ \"$1\" -gt 1 ]\n then\n   if [ \"$1\" -lt 4 ]\n    then\n     echo is between 2 or 3\n   else\n     echo is greater than or equal to 4\n   fi\n else\n   echo is less than  or equal to 1\nfi\n\n\n\nBash wait for user interaction\n\n\n#!/bin/bash\n#echo are you sure ?\n#read  VALUE\n\nuntil  [[ \"$VALUE\" = \"yes\" ]]\n\n   do\n    echo \"are you sure ?\"\nread  VALUE\ndone\n\n\n\nBash select menu example 1\n\n\n#!/bin/bash\nOPTIONS=\"Hello Quit extra\"\nselect opt in $OPTIONS; do\n        if [ \"$opt\" = \"Quit\" ]; then\n          echo done\n          exit\n        elif [ \"$opt\" = \"Hello\" ]; then\n          echo Hello World\n        elif [ \"$opt\" = \"extra\" ]; then\n          echo \"Sorry! extra is not implemented yet\"\n        else\n          clear\n          echo bad option\n        fi\ndone\n\n\n\nBash select menu example 2\n\n\n#!/bin/bash\nPS3=\"Select program: \"\nselect program in  ls pwd date top exit\ndo\n $program\ndone\n\n\n\nBash shifting through arguments\n\n\n#!/bin/bash\nif [ \"$#\" == \"0\" ]; then\n        echo \"Usage: $0 ARG1 ARG2 ARG3 etc....\"\n        exit 1\nfi\n\nwhile (( \"$#\" ))\ndo\n  echo -e \"processing argument $1 \\n\"\n  sleep 1\n  shift\ndone\n\nexit\n\n\n\nBash compare 2 strings    #really bad dont use on production code\n\n\n#!/bin/bash\n#bash test compare 2 strings\n#Usage: string-compare.sh string1 string2\nS1=$1\nS2=$2\nif [ \"$S1\" != \"$S2\" ] ;then\n     echo \"S1('$S1') is not equal to S2('$S2')\"\nfi\nif [ \"$S1\" = \"$S2\" ] ;then\n     echo \"S1('$S1') is equal to S2('$S2')\"\nfi\n\nif [[ \"$S1\" == *\"$S2\"* ]]; then\n   echo \"String 1 contains String 2\"\nfi\n\nif [[ \"$S2\" == *\"$S1\"* ]]; then\n#if         [[ \"*\"$S1\"*\" == \"$S2\" ]]; then\n   echo \"String 2 contains String 1\"\nfi\n\n\n\nBash script using lock file to control another script\n\n\n#!/bin/bash\nanswer=unset\nif [ -f /tmp/stop ]\n  then\n   echo -e \"/tmp/stop exists \\n exiting\"\n   exit 1\nelse\n  while [ ! -f /tmp/stop ]\n    do\n        echo \"go\" \n sleep 1\n  done\nfi\necho stopping\n\n\n\nUntil loop example\n\n\n#!/bin/bash\na=0\nwhile [ $a -ne 10 ]\ndo \n    echo $a\n    a=$(( $a +1 ))\ndone\n\n\n\nWhile loop example\n\n\n#!/bin/bash\na=0\nuntil [ $a -gt 10 ]\ndo \n    echo $a\n    a=$(( $a +1 ))\ndone", 
            "title": "Bash scripting"
        }, 
        {
            "location": "/scripting/bash_select/", 
            "text": "#!/bin/bash\n#Using the bash select construct to generate a Menu\necho Select an option from the menu...\nOPTIONS=\"Hello Quit Clear\"\nselect opt in $OPTIONS; do\n    if [ \"$opt\" = \"Quit\" ]; then\n          echo Quitting from Menu\n          exit\n    elif [ \"$opt\" = \"Hello\" ]; then\n          echo Hello World\n    elif [ \"$opt\" = \"Clear\" ]; then\n         echo Clearing Screen.....\n         sleep 2\n         clear\n    else\n         echo That was not a valid menu option !\n         sleep 1\n    fi\ndone", 
            "title": "Bash select"
        }, 
        {
            "location": "/scripting/bash_sort/", 
            "text": "#!/bin/bash\n# A C style script written in bash to randomly sort a range of numbers\n# I didn't write this I must have copied it from the Internet somewhere\nfor ((i=0;i\n52;i++))\n  do\n    deck[$i]=$i\n  done\nfor ((i=51; i\n0; i-- ))\n  do\n   rand=$(( $RANDOM % (i+1) ))\n   tmp=${deck[i]} deck[i]=${deck[rand]}\n   deck[rand]=$tmp\n  done\necho ${deck[@]}", 
            "title": "Bash sort"
        }, 
        {
            "location": "/scripting/bash_string_comparison/", 
            "text": "#!/bin/bash\n    S1=$1\n    S2=$2\n    if [ $S1 != $S2 ];\n    then\n            echo \"S1('$S1') is not equal to S2('$S2')\"\n    fi\n    if [ $S1 = $S2 ];\n    then\n            echo \"S1('$S1') is equal to S2('$S2')\"\n    fi", 
            "title": "Bash string comparison"
        }, 
        {
            "location": "/scripting/bash_test_to_only_run_as_root/", 
            "text": "#!/bin/bash\nif [ \"${UID}\" != \"0\" ]\n  then\n    echo \"Permission denied!!!    You need to be root to execute this command\"\n    exit 1\n  else\n    echo \"You are root\"\nfi", 
            "title": "Bash test to only run as root"
        }, 
        {
            "location": "/scripting/bash_tips/", 
            "text": "mv this-is-a-really-long-filename this-is-a-really-long-filename.bak\nmv this-is-a-really-long-filename{,.bak}\n\n\n\nif you're typing a multi line command, instead of editing the line by going back and forth use the \"fc\" command to open the line in an editor\n\n\nIf you want to use the last argument from your last command \npress ctrl+.\n\n\nIf you want to use the first argument from your last command \npress alt+ctrl+y\n\n\nYou can use !: (word designator) to reference arguments from the previous command line.\n    !:0 is the command, \n    !:1 is the first argument\n    !:2 is the second etc...\n\n\n$ ls -l /\n...\n$ echo !:1\n-l", 
            "title": "Bash tips"
        }, 
        {
            "location": "/scripting/bash_trap_user_ctrl-c/", 
            "text": "#!/bin/bash\n# Example of how to trap a user pressing ^c during script execution\necho \"Starting...\nPlease wait until I have completed\"\n\ntrap message INT\n\nfunction message() {\n        echo \"Please do not use CTRL^C, I have files open\"\n        sleep 5\n}\n\nfor i in `seq 10 -1 0`; do\n    echo -ne \"\\r $i seconds remaining  \"\n    sleep 1\ndone\n\necho \"I have finished\"", 
            "title": "Bash trap user ctrl c"
        }, 
        {
            "location": "/scripting/bash_wait_for_user_input/", 
            "text": "#!/bin/bash\n#bash wait until user enters \"yes\"\nuntil  [[ \"$VALUE\" = \"yes\" ]]\n   do\n    echo \"are you sure you want to quit ?\"\nread  VALUE\ndone", 
            "title": "Bash wait for user input"
        }, 
        {
            "location": "/scripting/diff/", 
            "text": "Ocasionaly i have needed to compare parts of the lines in log files or other text files.\nThe diff command has a handy option of ignoring the first X bytes of each line  which is especially handy to exclude time stamps.\n\n\nBellow we will create 2 input  files that will illustrate the point.\n\n\n[seamusmurray@example ~]$ cat seamus.before\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n12345678901234567890\n\n[seamusmurray@example ~]$ cat seamus.after\n12345678901234567890\n 2345678901234567890\n1 345678901234567890\n12 45678901234567890\n123 5678901234567890\n1234 678901234567890\n12345 78901234567890\n123456 8901234567890\n1234567 901234567890\n12345678 01234567890\n123456789 1234567890\n1234567890 234567890\n12345678901 34567890\n123456789012 4567890\n1234567890123 567890\n12345678901234 67890\n123456789012345 7890\n1234567890123456 890\n12345678901234567 90\n123456789012345678 0\n1234567890123456789\n12345678901234567890\n\n\n\nDiif of just the characters after column 19\n\n\n[seamusmurray@example ~]$ diff -y \n(cut -b19- seamus.before) \n(cut -b19- seamus.after)\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                              90\n90                                                            |  0\n90                                                            | 9\n90                                                              90\n\n\n[seamusmurray@example ~]$ diff  \n(cut -b19- seamus.before) \n(cut -b19- seamus.after)\n20,21c20,21\n\n 90\n\n 90\n---\n\n  0\n\n 9", 
            "title": "Diff"
        }, 
        {
            "location": "/scripting/expect/", 
            "text": "###Warning I have no idea when I wrote this and when I last used expect\n\n#!/usr/bin/expect\n\nset timeout 20\n\nset host [lindex $argv 0]\n\nset password \"secret\"\n\n\nset prompt \"$ \"\n\nproc dostuff {} {\n   send -- \"put seamus.sh\\r\"\n   return\n}\n\n#spawn scp -q seamus.sh ${host}:/tmp/shell_script.sh\nspawn sftp -q ${host}\n\n\nwhile (1) {\n\n   expect {\n     ;# -- This is the prompt when you first use\n     ;# -- ssh that says \"Are you sure you want to continue ...\"\n\n     \"no)? \" {\n        send -- \"yes\\r\"\n     }\n\n     ;# -- the prompt for password\n     \"Password: \" {\n         send -- \"$password\\r\"\n     }\n\n     ;# -- and finally we got a shell prompt\n     \"$prompt\" {\n        dostuff\n        break\n     }\n   }\n\n}\n\n;# -- exit\nexpect \"$prompt\"\nsend -- \"exit\\r\"\n\nexpect eof\n\n#######################################################################\n#!/usr/bin/expect\n\nset timeout 20\n\nset password [lindex $argv 0]\n\nspawn  sudo su \\-\n\nexpect \"seamusmurray:\"\n\nsend \"$password\\r\";", 
            "title": "Expect"
        }, 
        {
            "location": "/scripting/map/", 
            "text": ".\n\u251c\u2500\u2500 bash_and_grep_script_to_compare_2_lists.md\n\u251c\u2500\u2500 bash_and_powershell_failover\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 failover\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 BreakLoop.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Break.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 check_lagtime\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scripts\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 DR\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 break_snap_mirror.sh\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 initiate_dr.sh\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 options.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 primary_confirm_fail.sh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 fence_netapp\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 fence.txt.txt\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 Primary\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 primary_test.sh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 steward\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 schedule_forwarderA.xml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 slc_steward.ps1\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 SSH_Sessions.PSM1\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 where_is_the_luns.sh\n\u251c\u2500\u2500 bash_argument_shift_until_loop.md\n\u251c\u2500\u2500 bash_argument_shift_while_loop.md\n\u251c\u2500\u2500 bash_arguments.md\n\u251c\u2500\u2500 bash_array\n\u251c\u2500\u2500 bash_capture_keyboard_entry.md\n\u251c\u2500\u2500 bash_case.md\n\u251c\u2500\u2500 bash_fail_and_exit.md\n\u251c\u2500\u2500 bash_fail_trap.md\n\u251c\u2500\u2500 bash_function.md\n\u251c\u2500\u2500 bash_functions.md\n\u251c\u2500\u2500 bash_local_vs_global_variables.md\n\u251c\u2500\u2500 bash_nested_if_else.md\n\u251c\u2500\u2500 bash_read_input.md\n\u251c\u2500\u2500 Bash_scripting.md\n\u251c\u2500\u2500 bash_select.md\n\u251c\u2500\u2500 bash_sort.md\n\u251c\u2500\u2500 bash_string_comparison.md\n\u251c\u2500\u2500 bash_test_to_only_run_as_root.md\n\u251c\u2500\u2500 bash_tips.md\n\u251c\u2500\u2500 bash_trap_user_ctrl-c.md\n\u251c\u2500\u2500 bash_wait_for_user_input.md\n\u251c\u2500\u2500 Column_tally.md\n\u251c\u2500\u2500 Convert_kilobytes_to_MB_GB_TB_PB.md\n\u251c\u2500\u2500 diff.md\n\u251c\u2500\u2500 DNS_validate_entries.md\n\u251c\u2500\u2500 expect.md\n\u251c\u2500\u2500 IP_survey.md\n\u251c\u2500\u2500 map.md\n\u251c\u2500\u2500 script_to_markdown.md\n\u251c\u2500\u2500 Shuffle_deck_of_cards.md\n\u251c\u2500\u2500 simulate_site_failure.md\n\u2514\u2500\u2500 use_a_file_to_control_bash_script_operation.md\n\n7 directories, 47 files", 
            "title": "Map"
        }, 
        {
            "location": "/scripting/script_to_markdown/", 
            "text": "sed -i  s/^/\\ \\ \\ \\ / \nfile\n\n\n\n\nfix the urls so the files can be served from local filesystem on phone\n\n\nfor i in `find . | grep '.html'` ; do  sed -i s/\\\\/\\\"\\\n/\\\\/index.html\\\"\\\n/g $i ; done\n\n\n\nremove the footer\n\n\nfor i in `find . | grep '.html'` ; do sed -i '/Documentation\\ built\\ with/d' $i ; done\n\n\n\ncreate the map pages\n\n\ntree | sed  s/^/\\ \\ \\ \\ / \n map.md\n\n\n\nfor i in \nfind . | grep '.html'\n ; do  sed -i '/icon-home/d' $i ; done", 
            "title": "Script to markdown"
        }, 
        {
            "location": "/scripting/simulate_site_failure/", 
            "text": "I generated random numbers into a file   0.00 to 9.99\n\n\nwhile true ; do echo `echo $RANDOM | cut -c 1`\".\"`echo $RANDOM | cut -c 1-2`  ; done \n random\n\n\n#!/bin/bash\n\n\nfor SLEEP in `cat /root/random`\n  do\n    sleep 10\n    echo \"ifdown eth0 ; sleep $SLEEP ; ifup eth0\"\n    echo `date`\n    ifdown eth0 ; sleep $SLEEP ; ifup eth0 ; sleep $SLEEP\ndone", 
            "title": "Simulate site failure"
        }, 
        {
            "location": "/scripting/use_a_file_to_control_bash_script_operation/", 
            "text": "#!/bin/bash\n#Using a file to control the operation of a bash script\n#Handy in situations where you are looking for a log file or another process or job to create/update or delete a file\ncontrol_file=\"/tmp/control\"\ncontrol_value=\"start\"\nif [ -e $control_file ] ; then\n    echo \"Terminating due to the presence of the control file $control_file\"\n    echo \"No actions were perfomred\"\n    exit\nfi\n\necho \"I will continue to run until I detect the control file $control_file\"\necho \"You can stop this script by creating the control file e.g.\"\necho \"# touch $control_file\"\nuntil [ -e $control_file  ]\n    do\n    echo \"Performing some arbitrary action.....\" \n sleep 1\n    done\necho Program $0 has finished.", 
            "title": "Use a file to control bash script operation"
        }, 
        {
            "location": "/networking/traceroute_animation_1_and_2_routers/", 
            "text": "traceroute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  $(function() {\n\n    first = new freezeframe('.my_class_first').capture().setup();\n\n    $('.start_first').click(function(e) {\n      e.preventDefault();\n      first.trigger();\n    });\n\n    $('.stop_first').click(function(e) {\n      e.preventDefault();\n      first.release();\n    });\n\n    second = new freezeframe('.my_class_second').capture().setup();\n\n    $('.start_second').click(function(e) {\n      e.preventDefault();\n      second.trigger();\n    });\n\n    $('.stop_second').click(function(e) {\n      e.preventDefault();\n      second.release();\n    });\n\n    third = new freezeframe('.my_class_third').capture().setup();\n\n    $('.start_third').click(function(e) {\n      e.preventDefault();\n      third.trigger();\n    });\n\n    $('.stop_third').click(function(e) {\n      e.preventDefault();\n      third.release();\n    });\n\n    fourth = new freezeframe('.my_class_fourth').capture().setup();\n\n    $('.start_fourth').click(function(e) {\n      e.preventDefault();\n      fourth.trigger();\n    });\n\n    $('.stop_fourth').click(function(e) {\n      e.preventDefault();\n      fourth.release();\n    });\n\n   fifth = new freezeframe('.my_class_fifth').capture().setup();\n\n    $('.start_fifth').click(function(e) {\n      e.preventDefault();\n      fifth.trigger();\n    });\n\n    $('.stop_fifth').click(function(e) {\n      e.preventDefault();\n      fifth.release();\n    });\n\n  sixth = new freezeframe('.my_class_sixth').capture().setup();\n\n    $('.start_sixth').click(function(e) {\n      e.preventDefault();\n      sixth.trigger();\n    });\n\n    $('.stop_sixth').click(function(e) {\n      e.preventDefault();\n      sixth.release();\n    });\n\n\n  })\n\n\n\n\n\n\nbody {\n  text-align: left;\n  max-width: 1000px;\n  margin: 0px auto;\n  padding: 20px;\n  color: #202020;\n  font-size: 13px;\n}\n\na {\n  color: #202020;\n}\n\np {\n  font-family: monospace;\n  text-align: left;\n\n  padding: 10px 0px;\n}\n\nbutton {\n  font-family: monospace;\n  text-align: center;\n\n  width: 100px;\n  margin: 0px 10px 10px 0px;\n  padding: 5px 0px;\n  border: 1px solid #000;\n  background: #fff;\n\n  outline: none;\n}\n\nbutton:active {\n  background: #000;\n  color: #fff;\n}\n\n.bold {\n  font-weight: 700;\n}\n\n.italic {\n  font-style: italic;\n}\n\ndiv[name=screen]{\n    background-color: black;\n    color: white;\n    padding: 25px;\n    border: 10px solid white;\n    margin: 25px;\n    font-family: \"Courier New\";\n    font-size:20px;\n    display: inline-block;\n}\n\n\n\n\n\n\n\n\n\nThe purpose of this presentation is to clarify/correct some of the common misconceptions that people have about traceroute and its related tools.\n\nLastUpdate 2016 05 06  02:02 by seamus murray\n\n\n\nIn This example a packet is routed / passed though the router.\n\n\n\nThe source computer 192.168.0.10 generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe packet arrives at the router with a destination of 10.10.10.10\n\n\nThe router decrements the TTL by 1\n\n\nThe router forwards the packet to the destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\n\nstart\nstop\n1\n\n\n\n\n\nIn this example the computer on the left is performing a traceroute to the computer on the right (10.10.10.10)..\n\n\n\nThe source computer 192.168.0.10 generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe packet arrives at the router with a destination of 10.10.10.10 \n\n\nThe router decrements the TTL by 1\n\n\nThe packet has now expired on the router\n\n\nThe router generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0) message\n \n\nThe message will have the routers IP address as the source address\n\n\nThe router sends the message back to the original source IP 192.168.0.10\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to the computer on the right (10.10.10.10)\n\n\n\nThe computer on the right (10.10.10.10) sends the reply to the source computer.\n\n\n\n\n\n\nstart\nstop\n2\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 10 ms\n\n2 \n 10.10.10.10 \n 20 ms\n\n\n\n\n\n\n\n\nIn this example we have 2 routers between the source computer and the destination computer.\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nRouter 1 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the router 1 IP addess as the source address\n\n\nRouter 1 sends the message back to the original source IP 192.168.0.10\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to router 2\n\n\nRouter 2 decrements the TTL by 1\n\n\nThe 2nd packet has now expired on router 2\n\n\nRouter 2 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the routers 2 IP address as the source address\n\n\nRouter 2 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 3 and a destination IP address of 10.10.10.10\n\n\nThe 3rd packet arrives at router 1, TTL -1, forwarded to router 2, TTL -1, forwarded to destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n3\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 2.0 ms\n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\n\n\n\n\n\nIn this example there is latency induced by the single router...\n\n\nEach packet that is sent though router 1 is delayed.\n\nThe traceroute results are similar to the previous example except for the value of the router 1 delay is added to each query.\n\n\n\n\nstart\nstop\n4a\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 15 ms\n\n2 \n 10.10.10.10 \n 30 ms\n\n\n\n\n\n\n\n\nIn this example there is latency induced by router 1...\n\n\nEach packet that is sent though router 1 is delayed.\n\nThe traceroute results are similar to the previous example except for the value of the router 1 delay is added to each query.\n\n\n\n\nstart\nstop\n4b\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 5.0 ms\n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\n\n\n\n\n\nIn this example router drops expired packets but still routes the non exired packets...\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is too busy to generate and send an ICMP Time-to-Live Exceeded\n \n\nThe traceroute client on the source computer times out..\n\n\nBecause there was no packet received with the routers IP as the source address, the source computer can not determine the IP address of router 1, therefore it marks the message as *\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to computer 10.10.10.10\n\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n5a\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n * \n\n2 \n 10.10.10.10 \n 20 ms\n\n\nNote: there is no delay in any of the packets passing \nthrough\n router 1.\n\n\n\n\n\nIn this example router 1 drops expired packets...\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is too busy to generate and send an ICMP Time-to-Live Exceeded\n \n\nThe traceroute client on the source computer times out..\n\n\nBecause there was no packet received with the routers IP as the source address, the source computer can not determine the IP address of router 1, therefore it \nmarks the message as *\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to router 2\n\n\nRouter 2 decrements the TTL by 1\n\n\nThe 2nd packet has now expired on router 2\n\n\nRouter 2 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the routers 2 IP address as the source address\n\n\nRouter 2 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 3 and a destination IP address of 10.10.10.10\n\n\nThe 3rd packet arrives at router 1, TTL -1, forwarded to router 2, TTL -1, forwarded to destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n5b\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n * \n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\nNote: there is no delay in any of the packets passing \nthrough\n router 1.\n\n\n\n\n\nIn this example ..2-routers-delay-response.gif\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is busy and there is a delay while it completes other tasks (updating route tables, compressing log files etc..)\n \n\nRouter 1 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the router 1 IP address as the source address\n\n\nRouter 1 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to computer 10.10.10.10\n\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n6a\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 30 ms \n\n2 \n 10.10.10.10 \n 20 ms\n\n\n\n\nNote: there is no delay in any of the packets passing \nthrough\n router 1.\n\n\n\n\n\n\nIn this example ..2-routers-delay-response.gif\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is busy and there is a delay while it completes other tasks (updating route tables, compressing log files etc..)\n \n\nRouter 1 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the router 1 IP address as the source address\n\n\nRouter 1 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to router 2\n\n\nRouter 2 decrements the TTL by 1\n\n\nThe 2nd packet has now expired on router 2\n\n\nRouter 2 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the routers 2 IP address as the source address\n\n\nRouter 2 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 3 and a destination IP address of 10.10.10.10\n\n\nThe 3rd packet arrives at router 1, TTL -1, forwarded to router 2, TTL -1, forwarded to destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n6b\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 7.0 ms \n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\n\nNote: there is no delay in any of the packets passing \nthrough\n router 1.", 
            "title": "Traceroute animation 1 and 2 routers"
        }, 
        {
            "location": "/networking/traceroute_animation_1_router/", 
            "text": "traceroute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  $(function() {\n\n    first = new freezeframe('.my_class_first').capture().setup();\n\n    $('.start_first').click(function(e) {\n      e.preventDefault();\n      first.trigger();\n    });\n\n    $('.stop_first').click(function(e) {\n      e.preventDefault();\n      first.release();\n    });\n\n    second = new freezeframe('.my_class_second').capture().setup();\n\n    $('.start_second').click(function(e) {\n      e.preventDefault();\n      second.trigger();\n    });\n\n    $('.stop_second').click(function(e) {\n      e.preventDefault();\n      second.release();\n    });\n\n//    third = new freezeframe('.my_class_third').capture().setup();\n\n//    $('.start_third').click(function(e) {\n//      e.preventDefault();\n//      third.trigger();\n//    });\n\n//    $('.stop_third').click(function(e) {\n//      e.preventDefault();\n//      third.release();\n//    });\n\n    fourth = new freezeframe('.my_class_fourth').capture().setup();\n\n    $('.start_fourth').click(function(e) {\n      e.preventDefault();\n      fourth.trigger();\n    });\n\n    $('.stop_fourth').click(function(e) {\n      e.preventDefault();\n      fourth.release();\n    });\n\n   fifth = new freezeframe('.my_class_fifth').capture().setup();\n\n    $('.start_fifth').click(function(e) {\n      e.preventDefault();\n      fifth.trigger();\n    });\n\n    $('.stop_fifth').click(function(e) {\n      e.preventDefault();\n      fifth.release();\n    });\n\n  sixth = new freezeframe('.my_class_sixth').capture().setup();\n\n    $('.start_sixth').click(function(e) {\n      e.preventDefault();\n      sixth.trigger();\n    });\n\n    $('.stop_sixth').click(function(e) {\n      e.preventDefault();\n      sixth.release();\n    });\n\n\n  })\n\n\n\n\n\n\nbody {\n  text-align: left;\n  max-width: 1000px;\n  margin: 0px auto;\n  padding: 20px;\n  color: #202020;\n  font-size: 13px;\n}\n\na {\n  color: #202020;\n}\n\np {\n  font-family: monospace;\n  text-align: left;\n\n  padding: 10px 0px;\n}\n\nbutton {\n  font-family: monospace;\n  text-align: center;\n\n  width: 100px;\n  margin: 0px 10px 10px 0px;\n  padding: 5px 0px;\n  border: 1px solid #000;\n  background: #fff;\n\n  outline: none;\n}\n\nbutton:active {\n  background: #000;\n  color: #fff;\n}\n\n.bold {\n  font-weight: 700;\n}\n\n.italic {\n  font-style: italic;\n}\n\ndiv[name=screen]{\n    background-color: black;\n    color: white;\n    padding: 25px;\n    border: 10px solid white;\n    margin: 25px;\n    font-family: \"Courier New\";\n    font-size:20px;\n    display: inline-block;\n}\n\n\n\n\n\n\n\n\n\nThe purpose of this presentation is to clarify/correct some of the common misconceptions that people have about traceroute and its related tools.\n\nLastUpdate 2016 05 07  08:14 by seamus murray\n\n\n\nIn This example a packet is routed / passed though the router.\n\n\n\nThe source computer 192.168.0.10 generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe packet arrives at the router with a destination of 10.10.10.10\n\n\nThe router decrements the TTL by 1\n\n\nThe router forwards the packet to the destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\n\nstart\nstop\n1\n\n\n\n\n\nIn this example the computer on the left is performing a traceroute to the computer on the right (10.10.10.10)..\n\n\n\nThe source computer 192.168.0.10 generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe packet arrives at the router with a destination of 10.10.10.10 \n\n\nThe router decrements the TTL by 1\n\n\nThe packet has now expired on the router\n\n\nThe router generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0) message\n \n\nThe message will have the routers IP address as the source address\n\n\nThe router sends the message back to the original source IP 192.168.0.10\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to the computer on the right (10.10.10.10)\n\n\n\nThe computer on the right (10.10.10.10) sends the reply to the source computer.\n\n\n\n\n\n\nstart\nstop\n2\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 10 ms\n\n2 \n 10.10.10.10 \n 20 ms\n\n\n\n\n\n\n\n\nIn this example there is latency induced by the single router...\n\n\nEach packet that is sent though router 1 is delayed.\n\nThe traceroute results are similar to the previous example except for the value of the router 1 delay is added to each query.\n\n\n\n\nstart\nstop\n4\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 15 ms\n\n2 \n 10.10.10.10 \n 30 ms\n\n\n\n\n\n\n\n\nIn this example router drops expired packets but still routes the non exired packets...\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is too busy to generate and send an ICMP Time-to-Live Exceeded\n \n\nThe traceroute client on the source computer times out..\n\n\nBecause there was no packet received with the routers IP as the source address, the source computer can not determine the IP address of router 1, therefore it marks the message as *\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to computer 10.10.10.10\n\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n5\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n * \n\n2 \n 10.10.10.10 \n 20 ms\n\n\nNote: there is no delay in any of the packets passing \nthrough\n router 1.\n\n\n\n\n\nIn this example ..2-routers-delay-response.gif\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is busy and there is a delay while it completes other tasks (updating route tables, compressing log files etc..)\n \n\nRouter 1 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the router 1 IP address as the source address\n\n\nRouter 1 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to computer 10.10.10.10\n\n\n\nThe destination computer sends the reply to the source computer 192.168.0.10\n\n\n\n\n\n\nstart\nstop\n6\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 30 ms \n\n2 \n 10.10.10.10 \n 20 ms\n\n\n\n\nNote: there is no delay in any of the packets passing \nthrough\n router 1.", 
            "title": "Traceroute animation 1 router"
        }, 
        {
            "location": "/networking/traceroute_animation_2_routers/", 
            "text": "traceroute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  $(function() {\n\n    first = new freezeframe('.my_class_first').capture().setup();\n\n    $('.start_first').click(function(e) {\n      e.preventDefault();\n      first.trigger();\n    });\n\n    $('.stop_first').click(function(e) {\n      e.preventDefault();\n      first.release();\n    });\n\n    second = new freezeframe('.my_class_second').capture().setup();\n\n    $('.start_second').click(function(e) {\n      e.preventDefault();\n      second.trigger();\n    });\n\n    $('.stop_second').click(function(e) {\n      e.preventDefault();\n      second.release();\n    });\n\n    third = new freezeframe('.my_class_third').capture().setup();\n\n    $('.start_third').click(function(e) {\n      e.preventDefault();\n      third.trigger();\n    });\n\n    $('.stop_third').click(function(e) {\n      e.preventDefault();\n      third.release();\n    });\n\n    fourth = new freezeframe('.my_class_fourth').capture().setup();\n\n    $('.start_fourth').click(function(e) {\n      e.preventDefault();\n      fourth.trigger();\n    });\n\n    $('.stop_fourth').click(function(e) {\n      e.preventDefault();\n      fourth.release();\n    });\n\n   fifth = new freezeframe('.my_class_fifth').capture().setup();\n\n    $('.start_fifth').click(function(e) {\n      e.preventDefault();\n      fifth.trigger();\n    });\n\n    $('.stop_fifth').click(function(e) {\n      e.preventDefault();\n      fifth.release();\n    });\n\n  sixth = new freezeframe('.my_class_sixth').capture().setup();\n\n    $('.start_sixth').click(function(e) {\n      e.preventDefault();\n      sixth.trigger();\n    });\n\n    $('.stop_sixth').click(function(e) {\n      e.preventDefault();\n      sixth.release();\n    });\n\n\n  })\n\n\n\n\n\n\nbody {\n  text-align: left;\n  max-width: 1000px;\n  margin: 0px auto;\n  padding: 20px;\n  color: #202020;\n  font-size: 13px;\n}\n\na {\n  color: #202020;\n}\n\np {\n  font-family: monospace;\n  text-align: left;\n\n  padding: 10px 0px;\n}\n\nbutton {\n  font-family: monospace;\n  text-align: center;\n\n  width: 100px;\n  margin: 0px 10px 10px 0px;\n  padding: 5px 0px;\n  border: 1px solid #000;\n  background: #fff;\n\n  outline: none;\n}\n\nbutton:active {\n  background: #000;\n  color: #fff;\n}\n\n.bold {\n  font-weight: 700;\n}\n\n.italic {\n  font-style: italic;\n}\n\ndiv[name=screen]{\n    background-color: black;\n    color: white;\n    padding: 25px;\n    border: 10px solid white;\n    margin: 25px;\n    font-family: \"Courier New\";\n    font-size:20px;\n    display: inline-block;\n}\n\n\n\n\n\n\n\n\n\nThe purpose of this presentation is to clarify/correct some of the common misconceptions that people have about traceroute and its related tools.\n\nLastUpdate 2016 05 05 23:51 by seamus murray\n\n\n\nIn This example a packet is routed / passed though the router.\n\n\n\nThe source computer 192.168.0.10 generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe packet arrives at the router with a destination of 10.10.10.10\n\n\nThe router decrements the TTL by 1\n\n\nThe router sends the packet to the destination 10.10.10.10\n\n\nThe destination computer sends a reply\n\n\n\n\n\n\n\nstart\nstop\n\n\n\n\n\n\nIn this example the computer on the left is performing a traceroute to the computer on the right (10.10.10.10)..\n\n\n\nThe packet arrives at the router with a destination of 10.10.10.10 \n\n\nThe router decrements the TTL by 1\n\n\nThe packet has now expired on the router\n\n\nThe router generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0) message\n \n\nThe message will have the routers IP address as the source address\n\n\nThe router sends the message back to the original source IP\n\n\n\n\n\n\nstart\nstop\n\n\n\n\n\n\nIn this example we have a source computer, a destination computer and 2 routers.\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nRouter 1 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the router 1 IP addess as the source address\n\n\nRouter 1 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to router 2\n\n\nRouter 2 decrements the TTL by 1\n\n\nThe 2nd packet has now expired on router 2\n\n\nRouter 2 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the routers 2 IP address as the source address\n\n\nRouter 2 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 3 and a destination IP address of 10.10.10.10\n\n\nThe 3rd packet arrives at router 1, TTL -1, forwarded to router 2, TTL -1, forwarded to destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer, the path follows in reverse.\n\n\n\n\n\n\nstart\nstop\n\n\n\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 2.0 ms\n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\n\n\n\n\n\nIn this example there is latency induced by router 1...\n\n\nEach packet that is sent though router 1 is delayed.\n\nThe traceroute results are similar to the previous example except for the value of the router 1 delay is added to each query.\n\n\n\n\nstart\nstop\n\n\n\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 5.0 ms\n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\n\n\n\n\n\nIn this example router 1 drops expired packets...\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is too busy to generate and send an ICMP Time-to-Live Exceeded\n \n\nThe traceroute client on the source computer times out..\n\n\nBecause there was no packet received with the routers IP as the source address, the source computer can not determine the IP address of router 1, therefore it \nmarks the message as *\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to router 2\n\n\nRouter 2 decrements the TTL by 1\n\n\nThe 2nd packet has now expired on router 2\n\n\nRouter 2 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the routers 2 IP address as the source address\n\n\nRouter 2 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 3 and a destination IP address of 10.10.10.10\n\n\nThe 3rd packet arrives at router 1, TTL -1, forwarded to router 2, TTL -1, forwarded to destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer, the path follows in reverse.\n\n\n\n\n\n\nstart\nstop\n\n\n\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n * \n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\nNote: there is no delay in any of the packets passing through router 1.\n\n\n\n\n\nIn this example ..2-routers-delay-response.gif\n\n\n\nThe source computer generates and sends a packet with the TTL set to 1 and a destination IP address of 10.10.10.10\n\n\nThe 1st packet arrives at the router 1 with a destination of 10.10.10.10\n\n\nRouter 1 decrements the TTL by 1\n\n\nThe 1st packet has now expired on router 1\n\n\nThe control plane of Router 1 is busy and there is a delay while it completes other tasks\n \n\nRouter 1 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the router 1 IP address as the source address\n\n\nRouter 1 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 2 and a destination IP address of 10.10.10.10\n\n\nThe 2nd packet arrives at the router 1 with a destination of 10.10.10.10 \n\n\nRouter 1 decrements the TTL by 1\n\n\nRouter 1 forwards the packet to router 2\n\n\nRouter 2 decrements the TTL by 1\n\n\nThe 2nd packet has now expired on router 2\n\n\nRouter 2 generates an ICMP Time-to-Live Exceeded (Type 11), TTL equal 0 during transit (Code 0)\n \n\nThe message will have the routers 2 IP address as the source address\n\n\nRouter 2 sends the message back to the original source IP\n\n\n\nThe source computer generates and sends a packet with the TTL set to 3 and a destination IP address of 10.10.10.10\n\n\nThe 3rd packet arrives at router 1, TTL -1, forwarded to router 2, TTL -1, forwarded to destination computer 10.10.10.10\n\n\nThe destination computer sends the reply to the source computer, the path follows in reverse.\n\n\n\n\n\n\nstart\nstop\n\n\n\n\n\n\nc:\\> tracert 10.10.10.10\n\n1 \n 192.168.0.1 \n 7.0 ms \n\n2 \n 10.10.10.1 \n \n ms\n\n3 \n 10.10.10.10 \n 7.5 ms\n\n\n\n\n\n\nNote: there is no delay in any of the packets passing through router 1.", 
            "title": "Traceroute animation 2 routers"
        }, 
        {
            "location": "/linux/hot_add_memory_and_cpus/", 
            "text": "How to Hot-add Memory in Linux RHEL 5 ?\n\n\nCheck if the acpi modules are loaded\n\n\nlsmod | grep acpi\n\n\n\nYou should see both ...\n\n\nacpiphp                43673  0\nacpi_memhotplug        42199  0\n\n\n\nIf not loaded, load the modules... \n\n\n modprobe acpiphp\n modprobe acpi_memhotplug\n\n\n\nDisplay the total number of modules available in the system.\n\n\n cat /sys/devices/system/memory/memory*/state | wc\n\n\n\nIncrease your server memory with either physical Hot-add memory modules or if its a virtual machine simply increase the amount a memory allocated to the VM guest.\n\n\nDisplay the number of modules that are offline\n\n\n grep offline /sys/devices/system/memory/memory*/state | wc\n\n\n\nExecute either of the following to set the modules to be online \n\n\n for offline_module in `grep offline /sys/devices/system/memory/memory*/state | sed s/\\:offline//g`\n do echo  \"online\" \n $offline_module\n done\n\n\n\nValidate that none of the modules are still in an offline state\n\n\ngrep offline /sys/devices/system/memory/memory*/state\n\n\n\nDisplay the number of online modules and ensure that it is greater than the previous value\n\n\ncat /sys/devices/system/memory/memory*/state | wc\n\n\n\nFor Ubuntu you have to try with another script as below,\n\n\n#!/bin/sh\ni=0\nwhile [ $i -lt 4500 ]\ndo\n   if [ -f /sys/devices/system/memory/memory$i/state ]\n   then\n      if grep \"offline\" /sys/devices/system/memory/memory$i/state\n      then\n         echo 'online' \n /sys/devices/system/memory/memory$i/state\n      fi\n   fi\n   i=`expr $i + 1`\ndone\n\n\n\nTo validate that the memory has been successfully added  run either..\n\n\ncat /proc/meminfo\nfree\n\n\n\n\n\nHot add a CPU\n\n\necho 1 \n /sys/devices/system/cpu/cpu1/online", 
            "title": "Hot add memory and cpus"
        }, 
        {
            "location": "/linux/hot_add_memory_and_cpus/#hot-add-a-cpu", 
            "text": "echo 1   /sys/devices/system/cpu/cpu1/online", 
            "title": "Hot add a CPU"
        }, 
        {
            "location": "/linux/linux_proccess_signaling/", 
            "text": "Adventures in the land of Linux signalling\n\n\nIf you have a process that is consuming too much CPU or I/O you can pause it by sending it a STOP signal,\nIf it has been called by a shell that you have access too you can simply press CTRL^Z, the job will be paused and put in the back ground. execute fg to start the job\n\n\nBut if the process is not a shell command or is owned by another shell you can still pause the job by sending it a SIGSTOP signal using the kill command\n\n\nkill -19 \npid\n\n\nkill -s SIGSTOP \npid\n\n\n\n\nThis will put the process into the background.\n\n\nTo un-pause the process send it a SIGCONT (continue) signal \n\n\nkill -18 \npid\n\n\nkill -s SIGCONT \npid\n\n\n\n\nIf a process forks a child but does not call wait() if the child dies first the process will still stick around in the process table until someone picks up the exit code, if no one picks up the exit code init will do it\n\n\nseamus@ubuntu144:~$ ps -U seamus -v\nPID TTY      STAT   TIME  MAJFL   TRS   DRS   RSS %MEM COMMAND\n2772 pts/23   S      0:00      0   955 26460  4380  0.0 -su\n3177 pts/11   S      0:00      0   955 26464  4400  0.0 -su\n3219 pts/11   S+     0:08      0     2 43253   356  0.0 ./fork.exe\n3220 pts/11   S+     0:00      0     2 82317   104  0.0 ./fcrk.exe\n3221 pts/23   R+     0:00      0    84 22555  1076  0.0 ps -U seamus -v\n\nseamus@ubuntu144:~$ kill 3220\n\nseamus@ubuntu144:~$ ps -U seamus -v\nPID TTY      STAT   TIME  MAJFL   TRS   DRS   RSS %MEM COMMAND\n2772 pts/23   S      0:00      0   955 26460  4380  0.0 -su\n3177 pts/11   S      0:00      0   955 26464  4400  0.0 -su\n3219 pts/11   S+     0:08      0     2 43253   356  0.0 ./fork.exe\n3220 pts/11   Z+     0:00      0     0     0     0  0.0 [fork.exe] \ndefunct\n\n3222 pts/23   R+     0:00      0    84 22555  1072  0.0 ps -U seamus -v\n\n\n\nIf I add a wait call after the fork..\n   #include \n\n\nprintf(\"child exited with status of = %d\\n\",wait(\nexitstatus));\n\n\nAnd send the child a kill signal then all it good, no zombie and the parent receives the signal\n\n\nseamus@ubuntu144:~$ ps -U seamus -v\nPID TTY      STAT   TIME  MAJFL   TRS   DRS   RSS %MEM COMMAND\n2772 pts/23   S      0:00      0   955 26460  4380  0.0 -su\n3177 pts/11   S      0:00      0   955 26548  4620  0.0 -su\n3334 pts/11   S+     0:08      0     2 43253   352  0.0 ./fork.exe\n3340 pts/11   S+     0:00      0     2 43253   100  0.0 ./fork.exe\n3343 pts/23   R+     0:00      0    84 22555  1076  0.0 ps -U seamus -v\n\nseamus@ubuntu144:~$ kill 3340\n\nseamus@ubuntu144:~$ ps -U seamus -v\nPID TTY      STAT   TIME  MAJFL   TRS   DRS   RSS %MEM COMMAND\n2772 pts/23   S      0:00      0   955 26460  4380  0.0 -su\n3177 pts/11   S      0:00      0   955 26548  4620  0.0 -su\n3334 pts/11   S+     0:08      0     2 43253   352  0.0 ./fork.exe\n3344 pts/23   R+     0:00      0    84 22555  1072  0.0 ps -U seamus -v\n\nseamus@ubuntu144:~$ ./fork.exe \nI am the parent process \nI am the parent of the child proccess ID 3340\nchild exited with status of = 3340\n\n\n\nDD and signals\n\n\nSending signals to the DD command to force it to display a progress update.\n\n\nwhile killall -USR1 dd; do sleep 1; done\n\n\nguess@desktop:~$ dd if=/dev/urandom of=file bs=512 \n253427+0 records in\n253426+0 records out\n129754112 bytes (130 MB) copied, 11.1867 s, 11.6 MB/s\n276324+0 records in\n276323+0 records out\n141477376 bytes (141 MB) copied, 12.1911 s, 11.6 MB/s\n299038+0 records in\n299037+0 records out\n153106944 bytes (153 MB) copied, 13.1956 s, 11.6 MB/s\n321950+0 records in\n321949+0 records out\n164837888 bytes (165 MB) copied, 14.2001 s, 11.6 MB/s", 
            "title": "Linux proccess signaling"
        }, 
        {
            "location": "/linux/map/", 
            "text": "\u251c\u2500\u2500 Backups\n\u251c\u2500\u2500 Clustering\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cluster2_arch.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_automatic_failover_manual_fence_override.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_automatic_failover.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_cluster_build_guide.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_cluster_build.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_manual_failover.html\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 HA_LVM_manual_failover.txt.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 homemade_cluster.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mkclusterconf.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 RHEL4_cluster_installation.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 simple.cluster.conf.md\n\u251c\u2500\u2500 Commands\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 email_on_linux.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 hot_add_memory.md\n\u251c\u2500\u2500 Core_Utils\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 awk.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bash_shortcuts.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spacecmd_breaking_up_large_groups_of_hosts_with_awk.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test.log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 user_creation.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 vim_sudo.mk\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 watch_and_timeout.md\n\u251c\u2500\u2500 Disks_and_File_Systems\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 boot_cdrom_dvdrom.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 disk_filesystem_partition_imaging_using_squashfs_and_dd.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Extend_lvm_filesystem.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 filesystem_shuffle.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Linux_LVM_Logical_Volume_Manager.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Listing_Oracle_ASM_disks.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Multipath_and_NetApp_fixes.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Online_lun_resize.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 RHEL_floppy_kickstart.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rsync.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 shrink_lvm_filesystem.md\n\u251c\u2500\u2500 Firewalls_and_Security\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auditd_rules.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 iptables_notes.md\n\u251c\u2500\u2500 hot_add_memory_and_cpus.md\n\u251c\u2500\u2500 Installing_Linux\n\u251c\u2500\u2500 linux_proccess_signaling.md\n\u251c\u2500\u2500 Monitoring\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nagios_2.6_install.md\n\u251c\u2500\u2500 Networking\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 check_dns_records.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 determine_the_routes_a_packet_will_take.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 linux_networking.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 local_IP_address_survey.md\n\u251c\u2500\u2500 RedHat\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kickstart.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 satellite_spacewalk\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 perl_script_to_calulate_uptime_of_satellite_clients.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replacing_your_satellite_certificate.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 satellite_certificate_licence_install.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 using_spacecmd.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 yum_rpm\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 create_local_repos_from_redhat_install_media.md\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 working_with_yum_and_rpm.md\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 yum_and_rpm.md\n\u251c\u2500\u2500 Software_Packaging\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 apt_get.md\n\u2514\u2500\u2500 Time\n    \u251c\u2500\u2500 Adventures_in_the_land_of_symmetric_ntp_authentication.md\n    \u2514\u2500\u2500 Setting_time.md", 
            "title": "Map"
        }, 
        {
            "location": "/linux/Networking/check_dns_records/", 
            "text": "Bash script to check DNS records\n\n\n#!/bin/bash\n#This script will perform..\n# 1. forward lookups on a list of dns shortnames\n# 2. it will then perform reverse lookups on the IPs returned\n# 3. it will then perform the same steps again for various sub domains\n# Note: replace example with your domain name , input a list of either shortnames or FQDNs\n#       assumes there are 3 nics each with its own name, remove the last 2 stanzas if you only have a single nic and domain\n\nfor short_name in `head  list_systems | awk -F. '{ print $1 }'` \n  do\n  domain_name=\".example.local\"\n  host $short_name$domain_name\n    if [ $? = '0' ] ; then\n     fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" \n     host $fw_ip  \n         if [ $? = '0' ] ; then\n           bw_ip=\"`host $fw_ip | awk '{ print $5 }'`\"\n            \n out\n          else \n           echo \"$short_name$domain_name reverse failed $fw_ip\" \n errors\n           bw_ip=\"\"\n         fi\n    else\n     echo \"$short_name$domain_name forward failed\" \n errors\n     fw_ip=\"\"\n    fi\n\n  domain_name=\".subdomain1.example.local\"\n  host $short_name$domain_name\n    if [ $? = '0' ] ; then\n     m_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" \n     host $fw_ip  \n         if [ $? = '0' ] ; then\n           m_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\"\n           echo $short_name$domain_name\",\"$m_fw_ip\",\"$m_bw_ip\",\" \n m_out\n          else \n           echo \"$short_name$domain_name reverse failed $m_fw_ip\" \n m_errors\n           m_bw_ip=\"\"\n         fi\n    else\n     echo \"$short_name$domain_name forward failed\" \n errors\n     m_fw_ip=\"\"\n    fi\n\n  domain_name=\".subdomain2.example.local\"\n  host $short_name$domain_name\n    if [ $? = '0' ] ; then\n     b_fw_ip=\"`host $short_name$domain_name | awk '{ print $4}'`\" \n     host $fw_ip  \n         if [ $? = '0' ] ; then\n           b_bw_ip=\"`host $m_fw_ip | awk '{ print $5 }'`\"\n           echo $short_name$domain_name\",\"$b_fw_ip\",\"$b_bw_ip\",\" \n b_out\n          else \n           echo \"$short_name$domain_name reverse failed $b_fw_ip\" \n b_errors\n           b_bw_ip=\"\"\n         fi\n    else\n     echo \"$short_name$domain_name forward failed\" \n errors\n     b_fw_ip=\"\"\n    fi\n\n  echo     $short_name$domain_name\",\"$fw_ip\",\"$bw_ip\",\"\"$m_fw_ip\",\"$m_bw_ip\",\"\"$b_fw_ip\",\"$b_bw_ip\",\" \n out\ndone", 
            "title": "Check dns records"
        }, 
        {
            "location": "/linux/Networking/determine_the_routes_a_packet_will_take/", 
            "text": "determine the routes a packet will take on a Linux server\n\n\nip route get to 8.8.8.8\n\n\n8.8.8.8 via 192.168.0.1 dev eth0  src 192.168.0.6 \ncache", 
            "title": "Determine the routes a packet will take"
        }, 
        {
            "location": "/linux/Networking/linux_networking/", 
            "text": "Three methods for displaying the current route table on linux\n\n\n[root@spacewalk ~]# netstat -nr\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n10.0.2.0        0.0.0.0         255.255.255.0   U         0 0          0 eth0\n192.168.56.0    0.0.0.0         255.255.255.0   U         0 0          0 eth1\n169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth0\n169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth1\n0.0.0.0         10.0.2.2        0.0.0.0         UG        0 0          0 eth0\n\n[root@spacewalk ~]# route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n10.0.2.0        0.0.0.0         255.255.255.0   U     0      0        0 eth0\n192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 eth1\n169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 eth0\n169.254.0.0     0.0.0.0         255.255.0.0     U     1003   0        0 eth1\n0.0.0.0         10.0.2.2        0.0.0.0         UG    0      0        0 eth0\n\n[root@spacewalk ~]# cat /proc/net/route\nIface Destination Gateway  Flags RefCnt Use Metric Mask     MTU Window IRTT\neth0  0002000A    00000000 0001  0      0   0      00FFFFFF 0   0      0 \neth1  0038A8C0    00000000 0001  0      0   0      00FFFFFF 0   0      0\neth0  0000FEA9    00000000 0001  0      0   1002   0000FFFF 0   0      0 \neth1  0000FEA9    00000000 0001  0      0   1003   0000FFFF 0   0      0\neth0  00000000    0202000A 0003  0      0   0      00000000 0   0      0\n\n\n\nCommand to determine which route will be used for a certain destination\n\n\nip route get to 10.10.10.0\nip route get to 192.168.0\n\n\n\nSimple bash script to simulate network failure\n\n\n#!/bin/bash\n\nethcard=eth0\nMAXWAITTIME=30\nNUMBEROFRUNS=100\nWAITTIME=10 #initial value over written by random_delay()\n\nrandom_delay() {\n MAXTIME=30  #5 Minutes\n delay=$RANDOM\n let \"delay %= $MAXWAITTIME\"\n WAITTIME=$delay\n}\n\nrandom_delay\n\nfor i in `seq 1 $NUMBEROFRUNS`\ndo\n  random_delay\n    echo \"bringing $ethcard Offline for $WAITTIME seconds\"\n      ifdown $ethcard;  sleep $WAITTIME\n  random_delay\n    echo \"bringing $ethcard Online for $WAITTIME seconds\"\n      ifup $ethcard ;  sleep $WAITTIME\ndone\n\n\n\nDetermine the routes a packet will take on a linuix server\n\n\nip route get to 8.8.8.8\n\n\n8.8.8.8 via 192.168.0.1 dev eth0  src 192.168.0.6 \ncache", 
            "title": "Linux networking"
        }, 
        {
            "location": "/linux/Networking/local_IP_address_survey/", 
            "text": "This was used to create a space separated list of IPs in use on a server. \n\n\nServer_Name eth0 10.10.10.10 Mask:255.255.252.0\nServer_Name eth1 10.11.11.11 Mask:255.255.252.0\nServer_Name eth2 192.168.0.240 Mask:255.255.255.128\n\n\n#!/bin/bash\n\nHOST_NAME=`hostname -s`\nNICS=`ls /sys/class/net | grep -v lo`\n\n\nfor i in $NICS\n  do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://`\ndone", 
            "title": "local IP address survey"
        }, 
        {
            "location": "/linux/Firewalls_and_Security/auditd_rules/", 
            "text": "Using auuditd and aureport on RHEL to track whats going on.\n\n\nGenerate list of SUID programs\n\n\nfor SUID_BIN in `find / -xdev \\( -perm -4000 \\) -type f -print`\ndo AUDIT_CMD=\"auditctl -A exit,always -F  path=\"\nAUDIT_PERM=\"-pxwra\"\nKEY=`echo $SUID_BIN | awk  -F\\/ '{  print \"-k key-suid-\"$NF }'`\necho $AUDIT_CMD$SUID_BIN\" \"$AUDIT_PERM $KEY \ndone\n\n\n\nBackup the existing audit rules\n\n\ncd /etc/audit/\ncp audit.rules audit.rules.orig \n  vi audit.rules\n\n\n\nSample audit rules\n\n\n#Attempts to perform unauthorised functions\n#-a exit,always  -F arch=b64 -F success!=0 -S open -k key-fopen-failure\n\n#Additions, deletions and modifications to security/audit log parameters\n\n#-a exit,always -F dir=/var/log -F perm=wa -k key-writes-access-var-log\n\n#Critical file changes\n#Activity performed by privileged accounts Modifications to system settings (parameters)\n-w /etc/ -p w -k key-file-change-etc\n\n\n#The authority and access to use advanced operating system utilities and commands that bypass system access controls must be monitored, logged, reviewed and restricted to those individuals who require access to perform their job functions.\nauditctl -A exit,always -F path=/opt/google/chrome/chrome-sandbox -pxwra -k key-suid-chrome-sandbox\nauditctl -A exit,always -F path=/usr/sbin/pppd -pxwra -k key-suid-pppd\nauditctl -A exit,always -F path=/usr/sbin/uuidd -pxwra -k key-suid-uuidd\nauditctl -A exit,always -F path=/usr/lib/dbus-1.0/dbus-daemon-launch-helper -pxwra -k key-suid-dbus-daemon-launch-helper\nauditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxSDL -pxwra -k key-suid-VBoxSDL\nauditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxNetAdpCtl -pxwra -k key-suid-VBoxNetAdpCtl\nauditctl -A exit,always -F path=/usr/lib/virtualbox/VirtualBox -pxwra -k key-suid-VirtualBox\nauditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxHeadless -pxwra -k key-suid-VBoxHeadless\nauditctl -A exit,always -F path=/usr/lib/virtualbox/VBoxNetDHCP -pxwra -k key-suid-VBoxNetDHCP\nauditctl -A exit,always -F path=/usr/lib/x86_64-linux-gnu/oxide-qt/chrome-sandbox -pxwra -k key-suid-chrome-sandbox\nauditctl -A exit,always -F path=/usr/lib/policykit-1/polkit-agent-helper-1 -pxwra -k key-suid-polkit-agent-helper-1\nauditctl -A exit,always -F path=/usr/lib/eject/dmcrypt-get-device -pxwra -k key-suid-dmcrypt-get-device\nauditctl -A exit,always -F path=/usr/lib/pt_chown -pxwra -k key-suid-pt_chown\nauditctl -A exit,always -F path=/usr/lib/openssh/ssh-keysign -pxwra -k key-suid-ssh-keysign\nauditctl -A exit,always -F path=/usr/bin/traceroute6.iputils -pxwra -k key-suid-traceroute6.iputils\nauditctl -A exit,always -F path=/usr/bin/chfn -pxwra -k key-suid-chfn\nauditctl -A exit,always -F path=/usr/bin/pkexec -pxwra -k key-suid-pkexec\nauditctl -A exit,always -F path=/usr/bin/passwd -pxwra -k key-suid-passwd\nauditctl -A exit,always -F path=/usr/bin/X -pxwra -k key-suid-X\nauditctl -A exit,always -F path=/usr/bin/fping -pxwra -k key-suid-fping\nauditctl -A exit,always -F path=/usr/bin/chsh -pxwra -k key-suid-chsh\nauditctl -A exit,always -F path=/usr/bin/fping6 -pxwra -k key-suid-fping6\nauditctl -A exit,always -F path=/usr/bin/mtr -pxwra -k key-suid-mtr\nauditctl -A exit,always -F path=/usr/bin/gpasswd -pxwra -k key-suid-gpasswd\nauditctl -A exit,always -F path=/usr/bin/sudo -pxwra -k key-suid-sudo\nauditctl -A exit,always -F path=/usr/bin/lppasswd -pxwra -k key-suid-lppasswd\nauditctl -A exit,always -F path=/usr/bin/newgrp -pxwra -k key-suid-newgrp\nauditctl -A exit,always -F path=/home/guess/.pia_manager/openvpn_launcher.64 -pxwra -k key-suid-openvpn_launcher.64\nauditctl -A exit,always -F path=/home/guess/.pia_manager/openvpn_launcher.32 -pxwra -k key-suid-openvpn_launcher.32\nauditctl -A exit,always -F path=/bin/mount -pxwra -k key-suid-mount\nauditctl -A exit,always -F path=/bin/ping6 -pxwra -k key-suid-ping6\nauditctl -A exit,always -F path=/bin/fusermount -pxwra -k key-suid-fusermount\nauditctl -A exit,always -F path=/bin/umount -pxwra -k key-suid-umount\nauditctl -A exit,always -F path=/bin/su -pxwra -k key-suid-su\nauditctl -A exit,always -F path=/bin/ping -pxwra -k key-suid-ping\n\n\n\nrestart the auditd daemon\n\n\nservice auditd restart\n\ntail -f /var/log/audit/audit.log\n\n\n\nsome extra audit rules to monitor changes to the file system. Warning these will be very noisy on a busy system\n\n\n-a always,exit -F arch=b32 -S chmod -F auid\n=500 -F auid!=4294967295 -k perm_mod\n-a always,exit -F path=/bin/chmod -F auid\n=500 -F auid!=4294967295 -k perm_mod\n-a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid\n=500 -F auid!=4294967295 -k delete\n-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid\n=500 -F auid!=4294967295 -k access\n-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid\n=500 -F auid!=4294967295 -k access\n\n(host=example1np OR host=example2np) source=\"/var/log/audit/audit.log\" index=\"os_secure\"\nhost=example1np  source=\"/var/log/audit/audit.log\" index=\"os_secure\"\n\n\n\nretrieving info from the audit system\n\n\naureport\n\nSummary Report\n======================\nRange of time in logs: 5/08/2015 15:52:22 - 5/08/2015 18:43:12\nSelected time for report: 5/08/2015 15:52:22 - 5/08/2015 18:43:12\nNumber of changes in configuration: 0\nNumber of changes to accounts, groups, or roles: 0\nNumber of logins: 0\nNumber of failed logins: 0\nNumber of authentications: 17\nNumber of failed authentications: 0\nNumber of users: 2\nNumber of terminals: 2\nNumber of host names: 2\nNumber of executables: 25\nNumber of files: 28649\nNumber of AVC's: 0\nNumber of MAC events: 0\nNumber of failed syscalls: 512\nNumber of anomaly events: 0\nNumber of responses to anomaly events: 0\nNumber of crypto events: 39\nNumber of keys: 5\nNumber of process IDs: 1234\nNumber of events: 23455\n\n\n\ndefining a time range\n\n\naureport  -ts 12/06/2013 09:00:09.082 -te 12/06/2013 10:21:27.308", 
            "title": "Auditd rules"
        }, 
        {
            "location": "/linux/Firewalls_and_Security/iptables_notes/", 
            "text": "My notes on IPtables\n\n\nI cant remember where I copied and pasted this from..\n\n\nThe packet processing is called net filter the user space commend is called iptables\n\n\nhook points from the kernel\n\n\nFORWARD\n\n\nINPUT\n\n\nOUTPUT\n\n\nPOSTROUTING\n\n\nPREROUTING\n\n\nbuilt in tables\n\n\nRAW\n\n\nFILTER\n\n\nNAT\n\n\nMANGLE\n\n\nIf you don't specify a table then the FILTER table is assumed\n\n\nipchains\n\n\nby default each table has chains which are initially empty\n\n\nboth the match and target portion of the rule are optional\n\n\n\n\nif there is no match criteria then all packets match\n\n\nif the is no target then all packets pass through\n\n\n\n\nTARGETS\n  are used to specify what action to take when  a packet matches a rule\n  the are 4 default TARGETs built in\n   * ACCEPT\n   * DROP\n   * QUEUE\n   * RETURN\n\n\nIP filtering terms and expressions\n\n\nTo fully understand the upcoming chapters there are a few general terms and expressions that one must understand, including a lot of details regarding the TCP/IP chapter. This is a listing of the most common terms used in IP filtering.\n\n\nDrop/Deny - When a packet is dropped or denied, it is simply deleted, and no further actions are taken. No reply to tell the host it was dropped, nor is the receiving host of the packet notified in any way. The packet simply disappears.\n\n\nReject - This is basically the same as a drop or deny target or policy, except that we also send a reply to the host sending the packet that was dropped. The reply may be specified, or automatically calculated to some value. (To this date, there is unfortunately no iptables functionality to also send a packet notifying the receiving host of the rejected packet what happened (ie, doing the reverse of the Reject target). This would be very good in certain circumstances, since the receiving host has no ability to stop Denial of Service attacks from happening.)\n\n\nState - A specific state of a packet in comparison to a whole stream of packets. For example, if the packet is the first that the firewall sees or knows about, it is considered new (the SYN packet in a TCP connection), or if it is part of an already established connection that the firewall knows about, it is considered to be established. States are known through the connection tracking system, which keeps track of all the sessions.\n\n\nChain - A chain contains a ruleset of rules that are applied on packets that traverses the chain. Each chain has a specific purpose (e.g., which table it is connected to, which specifies what this chain is able to do), as well as a specific application area (e.g., only forwarded packets, or only packets destined for this host). In iptables, there are several different chains, which will be discussed in depth in later chapters.\n\n\nTable - Each table has a specific purpose, and in iptables there are 4 tables. The raw, nat, mangle and filter tables. For example, the filter table is specifically designed to filter packets, while the nat table is specifically designed to NAT (Network Address Translation) packets.\n\n\nMatch - This word can have two different meanings when it comes to IP filtering. The first meaning would be a single match that tells a rule that this header must contain this and this information. For example, the --source match tells us that the source address must be a specific network range or host address. The second meaning is if a whole rule is a match. If the packet matches the whole rule, the jump or target instructions will be carried out (e.g., the packet will be dropped.)\n\n\nTarget - There is generally a target set for each rule in a ruleset. If the rule has matched fully, the target specification tells us what to do with the packet. For example, if we should drop or accept it, or NAT it, etc. There is also something called a jump specification, for more information see the jump description in this list. As a last note, there might not be a target or jump for each rule, but there may be.\n\n\nRule - A rule is a set of a match or several matches together with a single target in most implementations of IP filters, including the iptables implementation. There are some implementations which let you use several targets/actions per rule.\n\n\nRuleset - A ruleset is the complete set of rules that are put into a whole IP filter implementation. In the case of iptables, this includes all of the rules set in the filter, nat, raw and mangle tables, and in all of the subsequent chains. Most of the time, they are written down in a configuration file of some sort.\n\n\nJump - The jump instruction is closely related to a target. A jump instruction is written exactly the same as a target in iptables, with the exception that instead of writing a target name, you write the name of another chain. If the rule matches, the packet will hence be sent to this second chain and be processed as usual in that chain.\n\n\nConnection tracking - A firewall which implements connection tracking is able to track connections/streams simply put. The ability to do so is often done at the impact of lots of processor and memory usage. This is unfortunately true in iptables as well, but much work has been done to work on this. However, the good side is that the firewall will be much more secure with connection tracking properly used by the implementer of the firewall policies.\n\n\nAccept - To accept a packet and to let it through the firewall rules. This is the opposite of the drop or deny targets, as well as the reject target.\n\n\nPolicy - There are two kinds of policies that we speak about most of the time when implementing a firewall. First we have the chain policies, which tells the firewall implementation the default behaviour to take on a packet if there was no rule that matched it. This is the main usage of the word that we will use in this book. The second type of policy is the security policy that we may have written documentation on, for example for the whole company or for this specific network segment. Security policies are very good documents to have thought through properly and to study properly before starting to actually implement the firewall.", 
            "title": "Iptables notes"
        }, 
        {
            "location": "/linux/Firewalls_and_Security/iptables_notes/#my-notes-on-iptables", 
            "text": "I cant remember where I copied and pasted this from..  The packet processing is called net filter the user space commend is called iptables  hook points from the kernel  FORWARD  INPUT  OUTPUT  POSTROUTING  PREROUTING  built in tables  RAW  FILTER  NAT  MANGLE  If you don't specify a table then the FILTER table is assumed  ipchains  by default each table has chains which are initially empty  both the match and target portion of the rule are optional   if there is no match criteria then all packets match  if the is no target then all packets pass through   TARGETS\n  are used to specify what action to take when  a packet matches a rule\n  the are 4 default TARGETs built in\n   * ACCEPT\n   * DROP\n   * QUEUE\n   * RETURN  IP filtering terms and expressions  To fully understand the upcoming chapters there are a few general terms and expressions that one must understand, including a lot of details regarding the TCP/IP chapter. This is a listing of the most common terms used in IP filtering.  Drop/Deny - When a packet is dropped or denied, it is simply deleted, and no further actions are taken. No reply to tell the host it was dropped, nor is the receiving host of the packet notified in any way. The packet simply disappears.  Reject - This is basically the same as a drop or deny target or policy, except that we also send a reply to the host sending the packet that was dropped. The reply may be specified, or automatically calculated to some value. (To this date, there is unfortunately no iptables functionality to also send a packet notifying the receiving host of the rejected packet what happened (ie, doing the reverse of the Reject target). This would be very good in certain circumstances, since the receiving host has no ability to stop Denial of Service attacks from happening.)  State - A specific state of a packet in comparison to a whole stream of packets. For example, if the packet is the first that the firewall sees or knows about, it is considered new (the SYN packet in a TCP connection), or if it is part of an already established connection that the firewall knows about, it is considered to be established. States are known through the connection tracking system, which keeps track of all the sessions.  Chain - A chain contains a ruleset of rules that are applied on packets that traverses the chain. Each chain has a specific purpose (e.g., which table it is connected to, which specifies what this chain is able to do), as well as a specific application area (e.g., only forwarded packets, or only packets destined for this host). In iptables, there are several different chains, which will be discussed in depth in later chapters.  Table - Each table has a specific purpose, and in iptables there are 4 tables. The raw, nat, mangle and filter tables. For example, the filter table is specifically designed to filter packets, while the nat table is specifically designed to NAT (Network Address Translation) packets.  Match - This word can have two different meanings when it comes to IP filtering. The first meaning would be a single match that tells a rule that this header must contain this and this information. For example, the --source match tells us that the source address must be a specific network range or host address. The second meaning is if a whole rule is a match. If the packet matches the whole rule, the jump or target instructions will be carried out (e.g., the packet will be dropped.)  Target - There is generally a target set for each rule in a ruleset. If the rule has matched fully, the target specification tells us what to do with the packet. For example, if we should drop or accept it, or NAT it, etc. There is also something called a jump specification, for more information see the jump description in this list. As a last note, there might not be a target or jump for each rule, but there may be.  Rule - A rule is a set of a match or several matches together with a single target in most implementations of IP filters, including the iptables implementation. There are some implementations which let you use several targets/actions per rule.  Ruleset - A ruleset is the complete set of rules that are put into a whole IP filter implementation. In the case of iptables, this includes all of the rules set in the filter, nat, raw and mangle tables, and in all of the subsequent chains. Most of the time, they are written down in a configuration file of some sort.  Jump - The jump instruction is closely related to a target. A jump instruction is written exactly the same as a target in iptables, with the exception that instead of writing a target name, you write the name of another chain. If the rule matches, the packet will hence be sent to this second chain and be processed as usual in that chain.  Connection tracking - A firewall which implements connection tracking is able to track connections/streams simply put. The ability to do so is often done at the impact of lots of processor and memory usage. This is unfortunately true in iptables as well, but much work has been done to work on this. However, the good side is that the firewall will be much more secure with connection tracking properly used by the implementer of the firewall policies.  Accept - To accept a packet and to let it through the firewall rules. This is the opposite of the drop or deny targets, as well as the reject target.  Policy - There are two kinds of policies that we speak about most of the time when implementing a firewall. First we have the chain policies, which tells the firewall implementation the default behaviour to take on a packet if there was no rule that matched it. This is the main usage of the word that we will use in this book. The second type of policy is the security policy that we may have written documentation on, for example for the whole company or for this specific network segment. Security policies are very good documents to have thought through properly and to study properly before starting to actually implement the firewall.", 
            "title": "My notes on IPtables"
        }, 
        {
            "location": "/linux/Software_Packaging/apt_get/", 
            "text": "Show all packages in the cache\n\n\napt-cache pkgnames\n\n\n\nShow all packages in the cache starting with vim\n\n\napt-cache show vim\n\n\n\nShow all packages in the cache mentioning vim\n\n\napt-cache search vim\n\n\n\nShow the details of a certain package\n\n\napt-cache pkgnames vim\n\n\n\nShow the dependencies of a package\n\n\napt-cache showpkg vim\n\n\n\nUpdate the local cache from the upstream repos\n\n\napt-get update\n\n\n\nInstall a package\n\n\nsudo apt-get install vim\n\n\n\nUpgrade a package\n\n\nsudo apt-get install vim --only-upgrade\n\n\n\nInstall a package version\n\n\nsudo apt-get install vim=2.3.5-3ubuntu1\n\n\n\nUninstall a package\n\n\napt-get remove vim\n\n\n\nRemove config files for a package\n\n\napt-get purge vim\n\n\n\nFlush the local cache\n\n\napt-get clean\n\n\n\nDownload a package\n\n\napt-get download vim", 
            "title": "Apt get"
        }, 
        {
            "location": "/linux/Core_Utils/awk/", 
            "text": "using awk to append numbers to the start of a list\n\n\nawk '{print NR \" \" $0}' list \n list-numbered\n\n\n\nusing awk to print a range of lines from a list\n\n\nawk '$1 == 2 , $1 == 7' list\n\n\n\n\n\ndisplay non blank and non commented lines in file\n\n\nfilter blank and comment lines from file\n\n\ncat /etc/ssh/sshd_config  | grep -v \\# | sed '/^$/d' \ncat /etc/ssh/sshd_config  | grep -v \\# | awk 'NF'\ngrep  -v ^# /etc/samba/smb.conf | grep -v ^\\; | awk 'NF'\n\n\n\n\n\nUse awk to auto generate repeating block of code\neg.. creating a really long if else or case statement\n\n\nI was creating a keyboard mapper program and instead of typing loads of else if statements manually... I create a paired list of the input/match and the output/action and used the awk script below to generate the else if statements.\n\n\nA 14\nB 2F\nC 26\nD 24\nE 22\nF 2C\nG 2D\nH 35\nI 3A\nJ 34\nK 3C\nL 44\nM 36\nN 37\nO 42\nP 4A\nQ 12\nR 2A\nS 1C\nT 2B\nU 32\nV 2E\nW 1A\nX 1E\nY 33\nZ 16\n\n\n\nuse awk to generate\n\n\n#the spaces in the following line are required\ncat inputed_list | awk '{ print \"       else if ( c = '\"'\"'\"$1\"'\"'\"' ){\\n             key_value = \"$2\";\\n            single_upper(c);\\n             }\"}'\n\n\n\nthe output c code\n\n\n  else if ( c = 'T' ){\n         key_value = 2B;\n         single_upper(c);\n         }\n  else if ( c = 'U' ){\n         key_value = 32;\n         single_upper(c);\n         }\n  else if ( c = 'V' ){\n         key_value = 2E;\n         single_upper(c);\n         }\n  else if ( c = 'W' ){\n         key_value = 1A;\n         single_upper(c);\n         }\n  else if ( c = 'X' ){\n         key_value = 1E;\n         single_upper(c);\n         }\n  else if ( c = 'Y' ){\n         key_value = 33;\n         single_upper(c);\n         }\n  else if ( c = 'Z' ){\n         key_value = 16;\n         single_upper(c);\n\n\n\n\n\nSimple script using awk to dump a list of the filesystems that are over 70% full to a file in /tmp\n\n\n#!/bin/bash\n#seamus Dec 2004\nout=/tmp/diskusage\nname=`uname -n`\ndate=`date`\necho \n  $out\n\necho $name $date \n $out \ndf -k | awk '{if( $5 \n 70 ) print $0 }'  | awk '!/Filesystem/' \n $out\ncat $out\n\n\n\nuse sed to append spaces to start of each line for markdown input\n\n\ncat {file} | sed s/^/\\ \\ \\ \\ /g\n\n\n\nscript to replace the spaces in filenames with underscrores\n\n\n#!/bin/bash\n# this script was created to replace the spaces in filenames with underscrores\n#seamus Dec 2004\n#if [ $1 != do ] || [ $1 != check ]\n#then \n#   echo USAGE: do or check \n#   exit\n#\n#fi\ncase \"$1\" in\ndo)\n    for i in *\n    do \n    mv \"$i\" `echo -n \"$i\" | sed 's/\\ /_/g'`\n    done\n;;\ncheck) \n        for i in *\n        do\n        echo;echo -n \"rename\" \n            echo;echo  \"$i\" \n        echo  to.......\n        echo -n \"$i\" | sed 's/\\ /_/g'\n    echo\n    done\n;;\n*)\n    echo USAGE: do or check\nesac", 
            "title": "Awk"
        }, 
        {
            "location": "/linux/Core_Utils/bash_shortcuts/", 
            "text": "Handy shortcut when you mistype a command name, in the example below I mistype \"rm\" as \"em\"\n\n\nem /var/log/path/really-long-file-name.log \nem: command not found\n\n$ rm !:1\nrm /var/log/path/really-long-file-name.log\n\n\n\nRepeating the last command, most of the time you would just use the arrow up key however not all keyboards have arrow keys\n\n\n$ echo \"Hello World!\"\nHello World!\n\n$ !!\necho \"Hello World!\"\nHello World!\n\n\n\nHandy shortcut for when you forget to prepend a command with sudo\n\n\n$ apt-get install sshd\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied) \nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\n$ sudo !!\nsudo apt-get install sshd\n[sudo] password for user:\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\n\n\n\nIf you want to reuse the last argument from the previous command\n\n\n$ cat /etc/hosts\n127.0.0.1   localhost\n127.0.1.1   desktop\n\n$ vi !$\nvi /etc/hosts\n\n\n\nCleaning up the terminal\n\n\nClear the screen\n\n\n$clear\n\nCtrl + l\n\n\n\nStop the standard out of a program from displaying in your terminal but keep the program executing.\n\n\nCtrl + s\n\nCtrl + q\n\n\n\nBash tab completion works with wild cards\n\n\n$ rm *.txt  \nTAB", 
            "title": "Bash shortcuts"
        }, 
        {
            "location": "/linux/Core_Utils/rename/", 
            "text": "rename files using the perl rename script written by Larry Wall\n\n\n$ rename -nv 's/\\.sh//' *\nbash_argument_shift_until_loop.sh.md renamed as bash_argument_shift_until_loop.md", 
            "title": "Rename"
        }, 
        {
            "location": "/linux/Core_Utils/spacecmd_breaking_up_large_groups_of_hosts_with_awk/", 
            "text": "using awk to append numbers to the start of a list  and then use those numbers as a key to divide up list into smaller lists\n\n\nI know you dont need to add the numbers to the list for this task, however in my case I needed the list for other purposes  therefore the numbers were handy.\n\n\nadd line numbers to the file\n\n\n awk '{print NR \"\" $0}' dc1-dev-list-chomped \n dc1-dev-list-chomped-numbered\n\n\n\nprint lines between the 2 values\n\n\nawk '$1 == \"288\", $1 == \"299\"' dc1-dev-list-chomped-numbered\n\nspacecmd group_create package-upgrade-dc1-0599-0699\n\nspacecmd group_addsystems package-upgrade-dc1-0299-0599\n\nspacecmd group_listsystems package-upgrade-dc1-0299-0599\n\nSTARTNUM=900 LASTNUM=1218 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered \n dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM\n\nspacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM package-upgrade-dc1-STARTNUM-LASTNUM for i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM  | awk '{print $2}' ; do\n\nspacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM $i ; sleep 1; done\n\nSTARTNUM=0000 LASTNUM=0099 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered \n dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM.example.local\n\nspacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM.example.local package-upgrade-dc1-STARTNUM-LASTNUM.example.local\n\nfor i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM  | awk '{print $2}' ; do spacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local $i.example.local ; done\n\nspacecmd group_listsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local\n\npackage-upgrade-dc1-1-299.example.local\n\npackage_listinstalledsystems packageClient-6.1.101-87.x86_64", 
            "title": "Spacecmd breaking up large groups of hosts with awk"
        }, 
        {
            "location": "/linux/Core_Utils/spacecmd_breaking_up_large_groups_of_hosts_with_awk/#using-awk-to-append-numbers-to-the-start-of-a-list-and-then-use-those-numbers-as-a-key-to-divide-up-list-into-smaller-lists", 
            "text": "I know you dont need to add the numbers to the list for this task, however in my case I needed the list for other purposes  therefore the numbers were handy.  add line numbers to the file   awk '{print NR \"\" $0}' dc1-dev-list-chomped   dc1-dev-list-chomped-numbered  print lines between the 2 values  awk '$1 == \"288\", $1 == \"299\"' dc1-dev-list-chomped-numbered\n\nspacecmd group_create package-upgrade-dc1-0599-0699\n\nspacecmd group_addsystems package-upgrade-dc1-0299-0599\n\nspacecmd group_listsystems package-upgrade-dc1-0299-0599\n\nSTARTNUM=900 LASTNUM=1218 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered   dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM\n\nspacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM package-upgrade-dc1-STARTNUM-LASTNUM for i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM  | awk '{print $2}' ; do\n\nspacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM $i ; sleep 1; done\n\nSTARTNUM=0000 LASTNUM=0099 awk '1 == 'STARTNUM' , 1 == 'LASTNUM'' dc1-dev-list-chomped-numbered   dc1-dev-list-chomped-numbered-STARTNUM-LASTNUM.example.local\n\nspacecmd group_create package-upgrade-dc1-STARTNUM-LASTNUM.example.local package-upgrade-dc1-STARTNUM-LASTNUM.example.local\n\nfor i in cat dc1-dev-list-chomped-numbered-$STARTNUM-$LASTNUM  | awk '{print $2}' ; do spacecmd group_addsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local $i.example.local ; done\n\nspacecmd group_listsystems package-upgrade-dc1-STARTNUM-LASTNUM.example.local\n\npackage-upgrade-dc1-1-299.example.local\n\npackage_listinstalledsystems packageClient-6.1.101-87.x86_64", 
            "title": "using awk to append numbers to the start of a list  and then use those numbers as a key to divide up list into smaller lists"
        }, 
        {
            "location": "/linux/Core_Utils/user_creation/", 
            "text": "groupadd -g 50001 group-name\nuseradd -g 50001 -c \"user created by seamus\" -p'$6$8$%^FGRGH*GH' -s /bin/bash -u 50001 user-name\n\nssh user-name@`hostname`", 
            "title": "User creation"
        }, 
        {
            "location": "/linux/Core_Utils/watch_and_timeout/", 
            "text": "Both the watch and timeout GNU utilities are very hands for monitoring and putting limits on scripts and programs\n\n\nusing the watch command to repeatedly run a command\n\n\n#Run the df command every 5 seconds\nwatch -n 5 df\n\n#run the df command every 5 seconds and highlight the differences\nwatch -n 5 -d df\n\n\n\nUsing the timeout command from coreutils to limit the length of time a process can run\n\n\n timeout 5 top\n\n\n\nThe timeout command accepts as integer followed by an option unit of either s m h d   seconds minutes hours days\n\n\nUsing the at atd atrm commands to schedule once off jobs\n\n\nIf you just want to run some simple command line tools you can enter the commands interactively.\n\n\nat now + 1 min\nwarning: commands will be executed using /bin/sh\nat\n echo seamus \n test.txt\nat\n ^d\n\n\n\nIf you have a shell script with your commands you can simply use the \"-f\" option\n\n\nat -f ./job.sh now + 3 min\n\n\n\nUsing echo to schedule a job, this is handy to call from another script\n\n\necho \"echo seamus \n test.txt\" | at now + 1 min\n\n\n\nList the jobs in the queue.\n\n\natq\n\n\n\nShow the contents of a particular job.\n\n\nat -c \njob number\n\n\n\n\nRemoving a job.\n\n\natq\n13  Sun Aug 16 22:12:00 2015 a guess\n16  Sun Aug 16 22:12:00 2015 a guess\n14  Sun Aug 16 22:12:00 2015 a guess\n15  Sun Aug 16 22:12:00 2015 a guess\n\natrm 14\n\natq\n13  Sun Aug 16 22:12:00 2015 a guess\n16  Sun Aug 16 22:12:00 2015 a guess\n15  Sun Aug 16 22:12:00 2015 a guess\n\n\n\nCrontab layout\n\n\n# +---------------- minute (0 - 59)\n# |  +------------- hour (0 - 23)\n# |  |  +---------- day of month (1 - 31)\n# |  |  |  +------- month (1 - 12)\n# |  |  |  |  +---- day of week (0 - 6) (Sunday=0 or 7)\n# |  |  |  |  |\n  *  *  *  *  *  command to be executed\n\n\n\nA little script I wrote back in 2004\n\n\n#!/bin/bash\nif  [ $# -le 0 ]\n  then\n    echo \"USSAGE = count (number) (delay) (command)\"\n    exit\n  fi\n count=$1\n while [ $count -ne 0 ]\n   do \n      \"$3\"  \n sleep $2\n      count=$(( $count -1 ))\n   done", 
            "title": "Watch and timeout"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover/", 
            "text": "below is a test of the automatic failover of a 2 node RHEL 5.7 cluster\n\n\n\n\nThere is 1 service called ServiceName\n\n\nThere is a single shared VG/LV  called /dev/shared_vg1/shared_lv1\n\n\n\n\n\n\n?xml version=\"1.0\"?\n\n\ncluster alias=\"round-and-round\" config_version=\"40\" name=\"round-and-round\"\n\n        \nfence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/\n\n        \nclusternodes\n\n                \nclusternode name=\"hostname1.domain.private\" nodeid=\"1\" votes=\"1\"\n\n                        \nfence\n\n                                \nmethod name=\"1\"\n\n                                        \ndevice name=\"ipmi-hostname1\"/\n\n                                \n/method\n\n                        \n/fence\n\n                \n/clusternode\n\n                \nclusternode name=\"hostname2.domain.private\" nodeid=\"2\" votes=\"1\"\n\n                        \nfence\n\n                                \nmethod name=\"1\"\n\n                                        \ndevice name=\"ipmi-hostname2\"/\n\n                                \n/method\n\n                        \n/fence\n\n                \n/clusternode\n\n        \n/clusternodes\n\n        \ncman expected_votes=\"1\" two_node=\"1\"\n\n                \nmulticast addr=\"224.0.0.001\"/\n\n        \n/cman\n\n        \nfencedevices\n\n                \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/\n\n                \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/\n=\n        \n/fencedevices\n\n        \nrm\n\n                \nfailoverdomains\n\n                        \nfailoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"\n\n                                \nfailoverdomainnode name=\"hostname1.domain.private\" priority=\"1\"/\n\n                                \nfailoverdomainnode name=\"hostname2.domain.private\" priority=\"2\"/\n\n                        \n/failoverdomain\n\n                \n/failoverdomains\n\n                \nresources\n\n                        \nlvm lv_name=\"shared_lv1\" name=\"shared_lv1_name\" self_fence=\"1\" vg_name=\"shared_vg1\"/\n\n                        \nfs device=\"/dev/shared_vg1/shared_lv1\" force_fsck=\"0\" force_unmount=\"1\" fsid=\"6742\" fstype=\"ext3\" mountpoint=\"/mnt/shared\" name=\"shared-fs\" self_fence=\"1\"/\n\n                        \nip address=\"192.168.1.3\" monitor_link=\"1\"/\n\n                        \nscript file=\"/usr/local/bin/pc.sh\" name=\"start.sh\"/\n\n                \n/resources\n\n                \nservice autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"ServiceName\"\n\n                        \nlvm ref=\"shared_lv1_name\"\n\n                                \nfs ref=\"shared-fs\"/\n\n                        \nip ref=\"192.168.1.3\"\n\n                        \nscript ref=\"start.sh\"/\n\n                        \n/ip\n\n                        \n/lvm\n\n                \n/service\n\n        \n/rm\n\n\n/cluster\n\n\n\n\n\n\nthe kernel crash will be triggered by executing\n\n\n echo c \n /proc/sysrq-trigger\n\n\n\nstatus of cluster \nbefore\n triggering kernel panic on primary node hostname1\n\n\n[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:31:00 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Online\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname1.domain.private      started\n\n\n\nstatus of cluster \nafter\n triggering kernel panic on primary node hostname1\n\n\n[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:31:40 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Offline\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname1.domain.private      started\n\n\n\nstatus of cluster \nduring\n fencing and service migration\n\n\n[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:32:02 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Offline\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname2.domain.private      starting\n\n\n\nstatus of cluster \nafter\n fencing and service migration\n\n\n[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:39:13 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Offline\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname2.domain.private      started\n\n\n\nJuly 1 16:31:16 hostname2 root: seamus simulating kernel panic echo c on primary node\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state.\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes).\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes).\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] entering GATHER state from 2.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering GATHER state from 0.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Creating commit token because I am the rep.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Storing new sequence id for ring 1530\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering COMMIT state.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering RECOVERY state.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] position [0] member 10.10.90.1:\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] previous ring seq 5420 rep 10.10.90.1\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] aru 5c high delivered 5c received flag 1\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Did not need to originate any messages in recovery.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Sending initial ORF token\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] CLM CONFIGURATION CHANGE\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] New Configuration:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ]      r(0) ip(10.10.90.1)\nJuly 1 16:31:33 hostname2 kernel: dlm: closing connection to node 2\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Left:\nJuly 1 16:31:33 hostname2 fenced[5990]: hostname1.domain.private not a cluster member after 0 sec post_fail_delay\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ]      r(0) ip(10.10.90.2)\nJuly 1 16:31:33 hostname2 fenced[5990]: fencing node \"hostname1.domain.private\"\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Joined:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] CLM CONFIGURATION CHANGE\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] New Configuration:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ]      r(0) ip(10.10.90.1)\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Left:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Joined:\nJuly 1 16:31:33 hostname2 openais[5968]: [SYNC ] This node is within the primary component and will provide service.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering OPERATIONAL state.\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] got nodejoin message 10.10.90.1\nJuly 1 16:31:33 hostname2 openais[5968]: [CPG  ] got joinlist message from node 1\nJuly 1 16:31:50 hostname2 fenced[5990]: fence \"hostname1.domain.private\" success\nJuly 1 16:31:50 hostname2 clurgmgrd[6229]: \nnotice\n Taking over service service:ServiceName from down member hostname1.domain.private\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]: \nnotice\n Owner of shared_vg1/shared_lv1 is not in the cluster\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]: \nnotice\n Stealing shared_vg1/shared_lv1\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]: \nnotice\n Activating shared_vg1/shared_lv1\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]: \nnotice\n Making resilient : lvchange -ay shared_vg1/shared_lv1\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]: \nnotice\n Resilient command: lvchange -ay shared_vg1/shared_lv1\n{wrapped} --config devices{filter=[\"a|/dev/mpath/mpshared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]}\nJuly 1 16:31:51 hostname2 multipathd: dm-12: devmap not registered, can't remove\nJuly 1 16:31:51 hostname2 multipathd: dm-12: add map (uevent)\nJuly 1 16:31:52 hostname2 kernel: kjournald starting.  Commit interval 5 seconds\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs warning: maximal mount count reached, running e2fsck is recommended\nJuly 1 16:31:52 hostname2 kernel: EXT3 FS on dm-12, internal journal\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs: dm-12: 1 orphan inode deleted\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs: recovery complete.\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs: mounted filesystem with ordered data mode.\nJuly 1 16:31:54 hostname2 avahi-daemon[5564]: Registering new address record for 192.168.1.3 on eth0.\nJuly 1 16:33:30 hostname2 clurgmgrd[6229]: \nnotice\n Service service:ServiceName started", 
            "title": "HA LVM automatic failover"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover/#the-kernel-crash-will-be-triggered-by-executing", 
            "text": "echo c   /proc/sysrq-trigger", 
            "title": "the kernel crash will be triggered by executing"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-before-triggering-kernel-panic-on-primary-node-hostname1", 
            "text": "[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:31:00 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Online\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname1.domain.private      started", 
            "title": "status of cluster before triggering kernel panic on primary node hostname1"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-after-triggering-kernel-panic-on-primary-node-hostname1", 
            "text": "[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:31:40 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Offline\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname1.domain.private      started", 
            "title": "status of cluster after triggering kernel panic on primary node hostname1"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-during-fencing-and-service-migration", 
            "text": "[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:32:02 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Offline\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname2.domain.private      starting", 
            "title": "status of cluster during fencing and service migration"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover/#status-of-cluster-after-fencing-and-service-migration", 
            "text": "[root@hostname2 ~]# clustat\nCluster Status for round-and-round @ Fri July 1 16:39:13 2013\nMember Status: Quorate\n\n Member Name                             ID   Status\n ------ ----                             ---- ------\n hostname2.domain.private                   1 Online, Local, rgmanager\n hostname1.domain.private                   2 Offline\n\n Service Name                   Owner (Last)                   State\n ------- ----                   ----- ------                   -----\n service:ServiceName                  hostname2.domain.private      started\n\n\n\nJuly 1 16:31:16 hostname2 root: seamus simulating kernel panic echo c on primary node\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state.\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes).\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes).\nJuly 1 16:31:31 hostname2 openais[5968]: [TOTEM] entering GATHER state from 2.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering GATHER state from 0.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Creating commit token because I am the rep.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Storing new sequence id for ring 1530\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering COMMIT state.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering RECOVERY state.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] position [0] member 10.10.90.1:\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] previous ring seq 5420 rep 10.10.90.1\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] aru 5c high delivered 5c received flag 1\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Did not need to originate any messages in recovery.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] Sending initial ORF token\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] CLM CONFIGURATION CHANGE\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] New Configuration:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ]      r(0) ip(10.10.90.1)\nJuly 1 16:31:33 hostname2 kernel: dlm: closing connection to node 2\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Left:\nJuly 1 16:31:33 hostname2 fenced[5990]: hostname1.domain.private not a cluster member after 0 sec post_fail_delay\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ]      r(0) ip(10.10.90.2)\nJuly 1 16:31:33 hostname2 fenced[5990]: fencing node \"hostname1.domain.private\"\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Joined:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] CLM CONFIGURATION CHANGE\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] New Configuration:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ]      r(0) ip(10.10.90.1)\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Left:\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] Members Joined:\nJuly 1 16:31:33 hostname2 openais[5968]: [SYNC ] This node is within the primary component and will provide service.\nJuly 1 16:31:33 hostname2 openais[5968]: [TOTEM] entering OPERATIONAL state.\nJuly 1 16:31:33 hostname2 openais[5968]: [CLM  ] got nodejoin message 10.10.90.1\nJuly 1 16:31:33 hostname2 openais[5968]: [CPG  ] got joinlist message from node 1\nJuly 1 16:31:50 hostname2 fenced[5990]: fence \"hostname1.domain.private\" success\nJuly 1 16:31:50 hostname2 clurgmgrd[6229]:  notice  Taking over service service:ServiceName from down member hostname1.domain.private\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]:  notice  Owner of shared_vg1/shared_lv1 is not in the cluster\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]:  notice  Stealing shared_vg1/shared_lv1\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]:  notice  Activating shared_vg1/shared_lv1\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]:  notice  Making resilient : lvchange -ay shared_vg1/shared_lv1\nJuly 1 16:31:51 hostname2 clurgmgrd: [6229]:  notice  Resilient command: lvchange -ay shared_vg1/shared_lv1\n{wrapped} --config devices{filter=[\"a|/dev/mpath/mpshared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]}\nJuly 1 16:31:51 hostname2 multipathd: dm-12: devmap not registered, can't remove\nJuly 1 16:31:51 hostname2 multipathd: dm-12: add map (uevent)\nJuly 1 16:31:52 hostname2 kernel: kjournald starting.  Commit interval 5 seconds\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs warning: maximal mount count reached, running e2fsck is recommended\nJuly 1 16:31:52 hostname2 kernel: EXT3 FS on dm-12, internal journal\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs: dm-12: 1 orphan inode deleted\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs: recovery complete.\nJuly 1 16:31:52 hostname2 kernel: EXT3-fs: mounted filesystem with ordered data mode.\nJuly 1 16:31:54 hostname2 avahi-daemon[5564]: Registering new address record for 192.168.1.3 on eth0.\nJuly 1 16:33:30 hostname2 clurgmgrd[6229]:  notice  Service service:ServiceName started", 
            "title": "status of cluster after fencing and service migration"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_automatic_failover_manual_fence_override/", 
            "text": "RHEL cluster testing a fencing override using the fence_ack_manual command\n\n\ntriggering a kernel panic , trigger a crash\n\n\n[root@hostname2 ~]# echo c \n /proc/sysrq-trigger\n\n\n\ntailing the log on the second node\n\n\nJuly 1  22:57 hostname1 ccsd[5959]: Update of cluster.conf complete (version 40 -\n 41).\n\nJuly 1  22:10 hostname1 clurgmgrd[6229]: \nnotice\n Reconfiguring\nJuly 1  22:15 hostname1 clurgmgrd: [6229]: \nnotice\n Getting status\nJuly 1  22:01 hostname1 root: seamus simulating fencedevice\\site failure\nJuly 1  22:48 hostname1 root: seamus triggred kenel panic on hostname2\n\nJuly 1  22:40 hostname1 openais[5968]: [TOTEM] The token was lost in the OPERATIONAL state.\nJuly 1  22:40 hostname1 openais[5968]: [TOTEM] Receive multicast socket recv buffer size (320000 bytes).\nJuly 1  22:40 hostname1 openais[5968]: [TOTEM] Transmit multicast socket send buffer size (320000 bytes).\nJuly 1  22:40 hostname1 openais[5968]: [TOTEM] entering GATHER state from 2.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] entering GATHER state from 0.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] Creating commit token because I am the rep.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] Storing new sequence id for ring 1538\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] entering COMMIT state.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] entering RECOVERY state.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] position [0] member 10.10.90.1:\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] previous ring seq 5428 rep 10.10.90.1\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] aru 2f high delivered 2f received flag 1\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] Did not need to originate any messages in recovery.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] Sending initial ORF token\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] CLM CONFIGURATION CHANGE\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] New Configuration:\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ]      r(0) ip(10.10.90.1)\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] Members Left:\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ]      r(0) ip(10.10.90.2)\nJuly 1  22:42 hostname1 kernel: dlm: closing connection to node 2\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] Members Joined:\nJuly 1  22:42 hostname1 fenced[5990]: hostname2.domain.private not a cluster member after 0 sec post_fail_delay\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] CLM CONFIGURATION CHANGE\nJuly 1  22:42 hostname1 fenced[5990]: fencing node \"hostname2.domain.private\"\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] New Configuration:\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ]      r(0) ip(10.10.90.1)\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] Members Left:\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] Members Joined:\nJuly 1  22:42 hostname1 openais[5968]: [SYNC ] This node is within the primary component and will provide service.\nJuly 1  22:42 hostname1 openais[5968]: [TOTEM] entering OPERATIONAL state.\nJuly 1  22:42 hostname1 openais[5968]: [CLM  ] got nodejoin message 10.10.90.1\nJuly 1  22:42 hostname1 openais[5968]: [CPG  ] got joinlist message from node 1\n\nJuly 1  22:50 hostname1 fenced[5990]: agent \"fence_ipmilan\" reports: Rebooting machine @ IPMI:192.168.214.33...Failed\nJuly 1  22:50 hostname1 fenced[5990]: fence \"hostname2.domain.private\" failed\n\nJuly 1  22:18 hostname1 root: seamus fencing failing due to incorrect ipmi password\n\nJuly 1  22:48 hostname1 fenced[5990]: fencing node \"hostname2.domain.private\"\nJuly 1  22:56 hostname1 fenced[5990]: agent \"fence_ipmilan\" reports: Rebooting machine @ IPMI:192.168.214.33...Failed\nJuly 1  22:56 hostname1 fenced[5990]: fence \"hostname2.domain.private\" failed\n\n\n\nrunning the fence_ack_manual command\n\n\n[root@hostname1 ~]# fence_ack_manual -e -n hostname2.domain.private\n\nWarning:  If the node \"hostname2.domain.private\" has not been manually fenced\n(i.e. power cycled or disconnected from shared storage devices)\nthe GFS file system may become corrupted and all its data\nunrecoverable!  Please verify that the node shown above has\nbeen reset or disconnected from storage.\n\nAre you certain you want to continue? [yN] y\n\n\nJuly 1  22:06 hostname1 root: seamus running fence_ack_manual -e -n hostname2.domain.private\nJuly 1  22:23 hostname1 fenced[5990]: fence \"hostname2.domain.private\" overridden by administrator intervention", 
            "title": "HA LVM automatic failover manual fence override"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_cluster_build/", 
            "text": "RHEL 5.7 HA-LVM cluster setup guide\n\n\nversion 201600508\n\n\ncreated by Seamus Murray\n\n\nAfter manually building the server via multiple RDP sessions and KVM consoles you sometimes have typos or duplicate key strokes in config files.\nBefore going further, you should ensure that all the hostname's IP address gateways etc are correct..\n\n\n\n\nensure the host name is correct and a FQDN in /etc/sysconfig/network\n\n\nensure hostname is not in any of the ifcfg-ethx files \n\n\nensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default\n\n\nrestart network services, run hostname command manually or reboot if necessary\n\n\n\n\nEnsure NTP is working correctly,\n\n\nYou may need to manually sync the time\n\n\nservice ntp stop\nntpdate 192.168.100.1 \nntpdate 192.168.100.2 \nservice ntp start\n\n\n\nImportant\n , Due to the application vendor support requirements, the cluster servers need to remain on RHEL 5.7.\nie, they cannot be updated to RHEL 5.8 or 5.9\nTo ensure the hosts stay on RHEL 5.7 there has been a dedicated software channel created for this specific project.\nThe hosts have to be registered against this specific channel.... if they subscribe to the main RHEL 5.x channel they will be updated to the latest package versions which will make then unsupportable from the application vendors perspective.\n\n\n\n\nrhel-x86_64-server-5_7 \n\n\nClone Red Hat Network Tools for RHEL Server (v.5.7 64-bit x86_64) \n\n\nrhel-x86_64-server-cluster-5_7 \n\n\n\n\nDownload and install the certificates for the signed RPM packages\n\n\nwget --no-check-certificate https://satellite1.local/pub/rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm\nwget --no-check-certificate https://satellite1.local/pub/rpm-gpg-key-1.0-7.noarch.rpm\nrpm -Uvh rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm rpm-gpg-key-corp-1.0-7.noarch.rpm\nsed -i 's/^\\(sslCACert=\\).*/\\1\\/usr\\/share\\/rhn\\/RHN-ORG-TRUSTED-SSL-CERT/' /etc/sysconfig/rhn/up2date\n\n\n\nEnsure the that the RedHat certificate is installed # not installed by default\n\n\nrpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release\n\n\n\nif not you may get errors like...\n\n\nPublic key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed\n\n\n\nRegister the server with the RedHat satellite server using one of the following keys\n\n\nrhnreg_ks --activationkey 1-bc5fb717c87abcdefghijklm --serverUrl https://satellite1.local/XMLRPC\nrhnreg_ks --activationkey 1-5bbf25c4abcdefghijklm123 --serverUrl https://satellite1.local/XMLRPC\n\n\n\ncheck if the repo access is working\n\n\nyum clean all\nyum list\n\n\n\nInstall the addons...\n\n\nyum install java-1.6.0-openjdk\nyum install samba3x-winbind  iptables lsof krb5-workstation pam_passwdqc xauth\nyum install ipv6-disable\n\n\n\nFixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5\n\n\ncause\n\n\n\n\nRHEL 5 multipath bindings file is located in var\n\n\n/var isn't mounted before multipath is configured\n\n\nredhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot\n\n\n\n\nsolution (work around)\n\n\nEdit the /etc/multipath.conf\n\n\nadd the following lines to multipath config to set netapp specific settings\n\n\ndefaults {\n    user_friendly_names yes\n    bindings_file /etc/multipath/bindings ##changed \n    flush_on_last_del       yes\n    max_fds max\n    pg_prio_calc    avg\n    queue_without_daemon    no\n}\n\n\ndevices {\n\n    device {\n            vendor                  \"NETAPP\"\n            product                 \"LUN\"\n            path_checker            tur\n            path_selector           \"round-robin 0\"\n            getuid_callout          \"/sbin/scsi_id -g -u -s /block/%n\"\n#           prio_callout            \"/sbin/mpath_prio_ontap /dev/%n\"\n            prio_callout            \"/sbin/mpath_prio_alua /dev/%n\"\n#           features                \"1 queue_if_no_path\"\n            features                \"3 queue_if_no_path pg_init_retries 50\"\n#           hardware_handler        \"0\"\n            hardware_handler        \"1 alua\"\n            path_grouping_policy    group_by_prio\n            failback                immediate\n            rr_weight               uniform\n            rr_min_io               128\n    }\n\n}\n\n\n\n\nmkdir /etc/multipath\ncp /var/lib/multipath/bindings /etc/multipath/bindings\ncd /boot\ncp initrd*img initrd*img.multipath-bak\nmkinitrd -f initrd-`uname -r`.img `uname -r`\nls -ltr  #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size\n\n\n\nedit the multipath bindings and ensure the shared luns have the same friendly names across all nodes\nrecreate initial ram disk image\nreboot all nodes and ensure all nodes see the luns at same friendly name \n\n\ncat /etc/multipath/bindings\n# Multipath bindings, Version : 1.0\n# NOTE: this file is automatically maintained by the multipath program.\n# You should not need to edit this file in normal circumstances.\n#\n# Format:\n# alias wwid\n#\nmpsys 360a9800037542d73442443123456755\nmpath1 360a9800037542d73442443123456757\nmpath2 360a9800037542d73442443123456759\nmpath3 360a9800037542d7344244312345672f\nmpath4 360a9800037542d73442443123456762\nmpath5 360a9800037542d73442443123456764\nmpath6 360a9800037542d73442443123456766\n\n\n\nFinal multipath.conf file should look somthing like\n\n\ndefaults {\n        user_friendly_names yes\n        bindings_file /etc/multipath/bindings\n        flush_on_last_del       yes\n        max_fds max\n        pg_prio_calc    avg\n        queue_without_daemon    no\n    }\n\n\nblacklist {\n        devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\"\n        devnode \"^(hd|xvd|vd)[a-z]*\"\n#        wwid \"*\"\n}\n\n# Make sure our multipath devices are enabled.\n\nblacklist_exceptions {\n        wwid \"360a9800037542d73442443123456755\"\n}\n\nmultipaths {\n        multipath {\n        wwid  360a9800037542d73442443123456755\n        alias mpsys\n        }\n}\n\n\ndevices {\n\n       device {\n                vendor                  \"NETAPP\"\n                product                 \"LUN\"\n                path_checker            tur\n                path_selector           \"round-robin 0\"\n                getuid_callout          \"/sbin/scsi_id -g -u -s /block/%n\"\n#               prio_callout            \"/sbin/mpath_prio_ontap /dev/%n\"\n                prio_callout            \"/sbin/mpath_prio_alua /dev/%n\"\n#               features                \"1 queue_if_no_path\"\n                features                \"3 queue_if_no_path pg_init_retries 50\"\n#               hardware_handler        \"0\"\n                hardware_handler        \"1 alua\"\n                path_grouping_policy    group_by_prio\n                failback                immediate\n                rr_weight               uniform\n                rr_min_io               128\n       }\n\n}\n\n\n\nChange fstab\n\n\n#/dev/mapper/mpath0p1    /boot                   ext3    defaults        1 2\n/dev/mapper/mpsysp1    /boot                   ext3    defaults        1 2\n\n\n\nCluster preconfiguration...\n\n\nEnsure NTP is working correctly,\n\n\nyou may need to manually sync the time\n\n    service ntp stop\n    ntpdate 192.168.0.1\n    ntpdate 192.168.0.2 \n    service ntp start\n\n\nInstall cluster software and fencing tools\n\n\nyum install OpenIPMI-tools.x86_64 cman rgmanager lucci ricci\n\n\n\nDisable the cluster from starting up... until you have finished the config\n\n\nchkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off\n\n\n\nEdit firewall rules to allow cluster traffic\n\n\niptables -I INPUT --protocol tcp --dport 22 -j ACCEPT\n# Cluster\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT\n\n\n\nturn off firewall for private nic\n    iptables -I INPUT -i eth1 -j ACCEPT\n\n\nsave the iptables so that they will be persistant \n\n\nConfigure the private/heartbeat nic\n\n\n*add multicast route for private/heartbeat\n\n\n/etc/sysconfig/network-scripts/route-eth1\n239.0.0.0/4 dev eth1\n\n\n\nDefine which multicast address to use for the cluster\n\n\n239.192.0.1 for App 1\n239.192.0.2 for App 2\n239.192.0.3 for App 3\n\n\n\nAdd the private hostnames/domains to /etc/hosts\n\n\n# cluster nodes\n192.168.0.1 hostname1.local  hostname1\n10.10.10.1  hostname1.private  hostname1-priv\n192.168.0.2 hostname2.local  hostname2\n10.10.10.2  hostname2.private  hostname2-priv\n192.168.0.3 hostname-vip.local  hostname-vip\n\n\n\nif you want to use luci\n\n\nwarning\n due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs\nyou have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config\nYou can use ricci/luci to import an existing cluster\nTherefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci\n\n\nStarting ricci/luci\n\n\nif you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password\nstart up ricci on both servers and luci on one server \nsetup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci\n\n\nstart browser window https://127.0.0.1:80\n\n\nSimple cluster.conf file to get you started\n\n\n?xml version=\"1.0\"?\n\n\ncluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"\n\n        \nfence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/\n\n        \nclusternodes\n\n                \nclusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\"\n\n                        \nfence\n\n                                \nmethod name=\"1\"\n\n                                        \ndevice name=\"ipmi-hostname1\"/\n\n                                \n/method\n\n                        \n/fence\n\n                \n/clusternode\n\n                \nclusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\"\n\n                        \nfence\n\n                                \nmethod name=\"1\"\n\n                                        \ndevice name=\"ipmi-hostname2\"/\n\n                                \n/method\n\n                        \n/fence\n\n                \n/clusternode\n\n        \n/clusternodes\n\n        \ncman expected_votes=\"1\" two_node=\"1\"\n\n                \nmulticast addr=\"239.192.0.1\"/\n\n        \n/cman\n\n        \nfencedevices\n\n                \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/\n\n                \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/\n=\n        \n/fencedevices\n\n        \nrm\n\n                \nfailoverdomains\n\n                        \nfailoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"\n\n                                \nfailoverdomainnode name=\"hostname1.private\" priority=\"1\"/\n\n                                \nfailoverdomainnode name=\"hostname2.private\" priority=\"2\"/\n\n                        \n/failoverdomain\n\n                \n/failoverdomains\n\n                \nresources\n\n                \n/resources\n\n                \nservice autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"\n\n                \n/service\n\n        \n/rm\n\n\n/cluster\n\n\nscp cluster.conf hostname1:/etc/cluster/cluster.conf\n\nccs_tool update cluster.conf\n\n\n\nConfigure fencing\n\n\n\n\nSetup ipmi profile on the hardware \n\n\n\n\nTest to see if fencing works\n\n\necho -e \"ipaddr=192.168.1.1 \\nlogin=\n \\npasswd=\n \\naction=status\" | fence_ipmilan\n\n\n\n\n\n\nAdd the fencing details to the cluster.conf file\n    \n\n    \n\n\nservice rgmanager start\nservice cman start\nclustat\ncman_tool status\ntail -f /var/log/messages\n\n\n\n\n\n\nmanual fence override\n    fence_ack_manual -e -n hostname1.private\n\n\n\n\nmanaul service relocation\n    clusvcadm -r ServiceName\n\n\n\n\nifconfig will not display a VIP you have to run\n\n\nip address show\n\n\n\n\n\n\nyou may want to disable the acpi daemon otherwise your server may not switch off fast enough\n\n\nchkconfig --level 234 5 acpid off\nchkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off\n\n\n\nsetup HA-LVM\n\n\n...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide\n\n\nedit /etc/lvm/lvm.conf\n\n\n\n\n\n\nEnsure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'.\n\n\n\n\n\n\nEdit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file:\n\n\n\n\n\n\nvolume_list = [ \"vgsys\", \"@hostname1.private\" ]\n\n\nCreate the PV VG LV and filesystems \nfrom one of the nodes\n\n\npvcreate /dev/mapper/mpath1\npvcreate /dev/mapper/mpath2\npvcreate /dev/mapper/mpath3\npvcreate /dev/mapper/mpath4\npvcreate /dev/mapper/mpath5\npvcreate /dev/mapper/mpath6\n\n\nvgcreate vg_shared_pc /dev/mapper/mpath1\nvgcreate vg_shared_db /dev/mapper/mpath2\nvgcreate vg_shared_arch /dev/mapper/mpath3\nvgcreate vg_shared_logs /dev/mapper/mpath4\nvgcreate vg_shared_data /dev/mapper/mpath5\nvgcreate vg_shared_backup /dev/mapper/mpath6\n\n\nlvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc\nlvcreate -l 100%FREE -n lv_shared_db vg_shared_db\nlvcreate -l 100%FREE -n lv_shared_arch vg_shared_arch\nlvcreate -l 100%FREE -n lv_shared_logs vg_shared_logs\nlvcreate -l 100%FREE -n lv_shared_data vg_shared_data\nlvcreate -l 100%FREE -n lv_shared_backup vg_shared_backup\n\n\nmkfs.ext3 /dev/vg_shared_pc/lv_shared_pc\nmkfs.ext3 /dev/vg_shared_db/lv_shared_db\nmkfs.ext3 /dev/vg_shared_arch/lv_shared_arch\nmkfs.ext3 /dev/vg_shared_logs/lv_shared_logs\nmkfs.ext3 /dev/vg_shared_data/lv_shared_data\nmkfs.ext3 /dev/vg_shared_backup/lv_shared_backup\n\n\nmkdir /opt/pc\nmkdir /opt/pc_db\nmkdir /opt/pc_arch\nmkdir /opt/pc_logs\nmkdir /opt/pc_data\nmkdir /opt/pc_backup", 
            "title": "HA LVM cluster build"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_cluster_build/#rhel-57-ha-lvm-cluster-setup-guide", 
            "text": "version 201600508  created by Seamus Murray  After manually building the server via multiple RDP sessions and KVM consoles you sometimes have typos or duplicate key strokes in config files.\nBefore going further, you should ensure that all the hostname's IP address gateways etc are correct..   ensure the host name is correct and a FQDN in /etc/sysconfig/network  ensure hostname is not in any of the ifcfg-ethx files   ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default  restart network services, run hostname command manually or reboot if necessary   Ensure NTP is working correctly,  You may need to manually sync the time  service ntp stop\nntpdate 192.168.100.1 \nntpdate 192.168.100.2 \nservice ntp start  Important  , Due to the application vendor support requirements, the cluster servers need to remain on RHEL 5.7.\nie, they cannot be updated to RHEL 5.8 or 5.9\nTo ensure the hosts stay on RHEL 5.7 there has been a dedicated software channel created for this specific project.\nThe hosts have to be registered against this specific channel.... if they subscribe to the main RHEL 5.x channel they will be updated to the latest package versions which will make then unsupportable from the application vendors perspective.   rhel-x86_64-server-5_7   Clone Red Hat Network Tools for RHEL Server (v.5.7 64-bit x86_64)   rhel-x86_64-server-cluster-5_7    Download and install the certificates for the signed RPM packages  wget --no-check-certificate https://satellite1.local/pub/rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm\nwget --no-check-certificate https://satellite1.local/pub/rpm-gpg-key-1.0-7.noarch.rpm\nrpm -Uvh rhn-org-trusted-ssl-cert-1.0-1.noarch.rpm rpm-gpg-key-corp-1.0-7.noarch.rpm\nsed -i 's/^\\(sslCACert=\\).*/\\1\\/usr\\/share\\/rhn\\/RHN-ORG-TRUSTED-SSL-CERT/' /etc/sysconfig/rhn/up2date  Ensure the that the RedHat certificate is installed # not installed by default  rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release  if not you may get errors like...  Public key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed  Register the server with the RedHat satellite server using one of the following keys  rhnreg_ks --activationkey 1-bc5fb717c87abcdefghijklm --serverUrl https://satellite1.local/XMLRPC\nrhnreg_ks --activationkey 1-5bbf25c4abcdefghijklm123 --serverUrl https://satellite1.local/XMLRPC  check if the repo access is working  yum clean all\nyum list  Install the addons...  yum install java-1.6.0-openjdk\nyum install samba3x-winbind  iptables lsof krb5-workstation pam_passwdqc xauth\nyum install ipv6-disable  Fixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5  cause   RHEL 5 multipath bindings file is located in var  /var isn't mounted before multipath is configured  redhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot   solution (work around)  Edit the /etc/multipath.conf  add the following lines to multipath config to set netapp specific settings  defaults {\n    user_friendly_names yes\n    bindings_file /etc/multipath/bindings ##changed \n    flush_on_last_del       yes\n    max_fds max\n    pg_prio_calc    avg\n    queue_without_daemon    no\n}\n\n\ndevices {\n\n    device {\n            vendor                  \"NETAPP\"\n            product                 \"LUN\"\n            path_checker            tur\n            path_selector           \"round-robin 0\"\n            getuid_callout          \"/sbin/scsi_id -g -u -s /block/%n\"\n#           prio_callout            \"/sbin/mpath_prio_ontap /dev/%n\"\n            prio_callout            \"/sbin/mpath_prio_alua /dev/%n\"\n#           features                \"1 queue_if_no_path\"\n            features                \"3 queue_if_no_path pg_init_retries 50\"\n#           hardware_handler        \"0\"\n            hardware_handler        \"1 alua\"\n            path_grouping_policy    group_by_prio\n            failback                immediate\n            rr_weight               uniform\n            rr_min_io               128\n    }\n\n}\n\n\n\n\nmkdir /etc/multipath\ncp /var/lib/multipath/bindings /etc/multipath/bindings\ncd /boot\ncp initrd*img initrd*img.multipath-bak\nmkinitrd -f initrd-`uname -r`.img `uname -r`\nls -ltr  #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size  edit the multipath bindings and ensure the shared luns have the same friendly names across all nodes\nrecreate initial ram disk image\nreboot all nodes and ensure all nodes see the luns at same friendly name   cat /etc/multipath/bindings\n# Multipath bindings, Version : 1.0\n# NOTE: this file is automatically maintained by the multipath program.\n# You should not need to edit this file in normal circumstances.\n#\n# Format:\n# alias wwid\n#\nmpsys 360a9800037542d73442443123456755\nmpath1 360a9800037542d73442443123456757\nmpath2 360a9800037542d73442443123456759\nmpath3 360a9800037542d7344244312345672f\nmpath4 360a9800037542d73442443123456762\nmpath5 360a9800037542d73442443123456764\nmpath6 360a9800037542d73442443123456766  Final multipath.conf file should look somthing like  defaults {\n        user_friendly_names yes\n        bindings_file /etc/multipath/bindings\n        flush_on_last_del       yes\n        max_fds max\n        pg_prio_calc    avg\n        queue_without_daemon    no\n    }  blacklist {\n        devnode \"^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*\"\n        devnode \"^(hd|xvd|vd)[a-z]*\"\n#        wwid \"*\"\n}\n\n# Make sure our multipath devices are enabled.\n\nblacklist_exceptions {\n        wwid \"360a9800037542d73442443123456755\"\n}\n\nmultipaths {\n        multipath {\n        wwid  360a9800037542d73442443123456755\n        alias mpsys\n        }\n}\n\n\ndevices {\n\n       device {\n                vendor                  \"NETAPP\"\n                product                 \"LUN\"\n                path_checker            tur\n                path_selector           \"round-robin 0\"\n                getuid_callout          \"/sbin/scsi_id -g -u -s /block/%n\"\n#               prio_callout            \"/sbin/mpath_prio_ontap /dev/%n\"\n                prio_callout            \"/sbin/mpath_prio_alua /dev/%n\"\n#               features                \"1 queue_if_no_path\"\n                features                \"3 queue_if_no_path pg_init_retries 50\"\n#               hardware_handler        \"0\"\n                hardware_handler        \"1 alua\"\n                path_grouping_policy    group_by_prio\n                failback                immediate\n                rr_weight               uniform\n                rr_min_io               128\n       }\n\n}  Change fstab  #/dev/mapper/mpath0p1    /boot                   ext3    defaults        1 2\n/dev/mapper/mpsysp1    /boot                   ext3    defaults        1 2  Cluster preconfiguration...  Ensure NTP is working correctly,  you may need to manually sync the time \n    service ntp stop\n    ntpdate 192.168.0.1\n    ntpdate 192.168.0.2 \n    service ntp start  Install cluster software and fencing tools  yum install OpenIPMI-tools.x86_64 cman rgmanager lucci ricci  Disable the cluster from starting up... until you have finished the config  chkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off  Edit firewall rules to allow cluster traffic  iptables -I INPUT --protocol tcp --dport 22 -j ACCEPT\n# Cluster\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT  turn off firewall for private nic\n    iptables -I INPUT -i eth1 -j ACCEPT  save the iptables so that they will be persistant   Configure the private/heartbeat nic  *add multicast route for private/heartbeat  /etc/sysconfig/network-scripts/route-eth1\n239.0.0.0/4 dev eth1  Define which multicast address to use for the cluster  239.192.0.1 for App 1\n239.192.0.2 for App 2\n239.192.0.3 for App 3  Add the private hostnames/domains to /etc/hosts  # cluster nodes\n192.168.0.1 hostname1.local  hostname1\n10.10.10.1  hostname1.private  hostname1-priv\n192.168.0.2 hostname2.local  hostname2\n10.10.10.2  hostname2.private  hostname2-priv\n192.168.0.3 hostname-vip.local  hostname-vip  if you want to use luci  warning  due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs\nyou have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config\nYou can use ricci/luci to import an existing cluster\nTherefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci  Starting ricci/luci  if you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password\nstart up ricci on both servers and luci on one server \nsetup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci  start browser window https://127.0.0.1:80  Simple cluster.conf file to get you started  ?xml version=\"1.0\"?  cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\" \n         fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/ \n         clusternodes \n                 clusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\" \n                         fence \n                                 method name=\"1\" \n                                         device name=\"ipmi-hostname1\"/ \n                                 /method \n                         /fence \n                 /clusternode \n                 clusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\" \n                         fence \n                                 method name=\"1\" \n                                         device name=\"ipmi-hostname2\"/ \n                                 /method \n                         /fence \n                 /clusternode \n         /clusternodes \n         cman expected_votes=\"1\" two_node=\"1\" \n                 multicast addr=\"239.192.0.1\"/ \n         /cman \n         fencedevices \n                 fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/ \n                 fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/ =\n         /fencedevices \n         rm \n                 failoverdomains \n                         failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\" \n                                 failoverdomainnode name=\"hostname1.private\" priority=\"1\"/ \n                                 failoverdomainnode name=\"hostname2.private\" priority=\"2\"/ \n                         /failoverdomain \n                 /failoverdomains \n                 resources \n                 /resources \n                 service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\" \n                 /service \n         /rm  /cluster \n\nscp cluster.conf hostname1:/etc/cluster/cluster.conf\n\nccs_tool update cluster.conf  Configure fencing   Setup ipmi profile on the hardware    Test to see if fencing works  echo -e \"ipaddr=192.168.1.1 \\nlogin=  \\npasswd=  \\naction=status\" | fence_ipmilan    Add the fencing details to the cluster.conf file\n     \n      service rgmanager start\nservice cman start\nclustat\ncman_tool status\ntail -f /var/log/messages    manual fence override\n    fence_ack_manual -e -n hostname1.private   manaul service relocation\n    clusvcadm -r ServiceName   ifconfig will not display a VIP you have to run  ip address show    you may want to disable the acpi daemon otherwise your server may not switch off fast enough  chkconfig --level 234 5 acpid off\nchkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off  setup HA-LVM  ...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide  edit /etc/lvm/lvm.conf    Ensure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'.    Edit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file:    volume_list = [ \"vgsys\", \"@hostname1.private\" ]  Create the PV VG LV and filesystems  from one of the nodes  pvcreate /dev/mapper/mpath1\npvcreate /dev/mapper/mpath2\npvcreate /dev/mapper/mpath3\npvcreate /dev/mapper/mpath4\npvcreate /dev/mapper/mpath5\npvcreate /dev/mapper/mpath6\n\n\nvgcreate vg_shared_pc /dev/mapper/mpath1\nvgcreate vg_shared_db /dev/mapper/mpath2\nvgcreate vg_shared_arch /dev/mapper/mpath3\nvgcreate vg_shared_logs /dev/mapper/mpath4\nvgcreate vg_shared_data /dev/mapper/mpath5\nvgcreate vg_shared_backup /dev/mapper/mpath6\n\n\nlvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc\nlvcreate -l 100%FREE -n lv_shared_db vg_shared_db\nlvcreate -l 100%FREE -n lv_shared_arch vg_shared_arch\nlvcreate -l 100%FREE -n lv_shared_logs vg_shared_logs\nlvcreate -l 100%FREE -n lv_shared_data vg_shared_data\nlvcreate -l 100%FREE -n lv_shared_backup vg_shared_backup\n\n\nmkfs.ext3 /dev/vg_shared_pc/lv_shared_pc\nmkfs.ext3 /dev/vg_shared_db/lv_shared_db\nmkfs.ext3 /dev/vg_shared_arch/lv_shared_arch\nmkfs.ext3 /dev/vg_shared_logs/lv_shared_logs\nmkfs.ext3 /dev/vg_shared_data/lv_shared_data\nmkfs.ext3 /dev/vg_shared_backup/lv_shared_backup\n\n\nmkdir /opt/pc\nmkdir /opt/pc_db\nmkdir /opt/pc_arch\nmkdir /opt/pc_logs\nmkdir /opt/pc_data\nmkdir /opt/pc_backup", 
            "title": "RHEL 5.7 HA-LVM cluster setup guide"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_cluster_build_guide/", 
            "text": "RHEL 5.9 HA-LVM cluster install and setup guide\n\n\nversion 201600508\n\n\ncreated by Seamus Murray\n\n\nPrerequisites\n2 machines either physical or virtual\n\n\nRHEL OS packages\nRHEL Cluster packages\n\n\nthese can be sourced from\n  Note write guides for each of these methods\n * RedHat directly RHN\n * Local YUM Repository\n * Local Satellite/Spacewalk server\n * Local media cdrom/dvdrom/usb drive/iso image etc.\n\n\nIn this guide I will be using the ISO of the RHEL 5.9 install\n\nThis ISO was downloaded from RHN and contains the following Repositories \n\n\n\n\nServer\n\n\nCluster\n\n\nClusterStorage\n\n\nVT\n\n\n\n\nfor the initial OS install we are just going to use the Server repo\n\n\nBoot from the DVD/ISO \nStep though the installer\n\nnote the package choice is dependant on what you want to do with the servers later\n\nEg if you want to run a GUI and web browser you will have to install those Groups\npersonally i always try to install the minimum number of packages to get the specific job done.\n\n\n@base\n@core\n@editors\n@text-internet\n\n\n\nOnce you install has completed and you have booted into your new OS.\n\n\nsetup the hosts to connect to a \n\n\nRHEL OS packages\nRHEL Cluster packages\n\n\nthese can be sourced from\n \n * RedHat directy RHN\n * Local YUM Repository\n * Local Satellite/Spacewalk server\n * Local media cdrom/dvdrom/usb drive/iso image etc.\n\n\nInstall the required cluster utils for this example it is just \n * cman \n * rgamanger\n * openais\n\n\nNote: for a HA-LVM cluster these is no need for any distributed lockmanager GFS etc...\n    if these are installed its best to remove them now\n\n\nDisable the cluster from starting up... until you have finished the config\n\n\nchkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off\n\n\n\nBefore we setup the cluster..\n * ensure your hostnames IP addresses and interfaces are in order.\n__in this exaple each node has 2 physical interfaces\n 1. eth0 the public side, this is where the service that will be clusterd will run from https,nfs,ftp etc\n 2. eth1 the private interface that is just used for cluster communication broadcast,multicast or unicast\n * ensure the host name is correct and a FQDN in /etc/sysconfig/network\n * ensure hostname is not in any of the ifcfg-ethx files \n * ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default\n * restart network services, run hostname command manually or reboot if necessary*\n * ensure the hostnames for all IPs on all the cluster nodes are listed in the /etc/hosts file\n\n\nsample /etc/hosts file\n\n\n192.168.56.101  rhel59halvmn1.local     rhel59halvmn1\n10.10.100.101   rhel59halvmn1.private   rhel59halvmn1-priv\n192.168.56.102  rhel59halvmn2.local     rhel59halvmn2\n10.10.100.102   rhel59halvmn2.private   rhel59halvmn2-priv\n192.168.56.103  rhel59halvm-vip.local   rhel59halvm-vip\n\n\n\nSetup ssh keys between the nodes to make things easier\n\n\n# ssh-copy-id -i /root/.ssh/id_rsa.pub rhel59halvmn2\n\n\n\nEnsure the time on all cluster nodes are in sync\n\n\nyou may need to manually sync the time\n\n\nalias REMOTETIME='ssh rhel59halvmn1 -C date'\ndate -s \"`REMOTETIME`\"\n\nservice ntp stop\nntpdate 192.168.1.1 \nntpdate 192.168.1.1 \nservice ntp start\n\n\n\nConfigure iptables to allow the cluster traffic through##\n\n\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 16851 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 21064 -j ACCEPT\n\n\n\nCreate a /etc/cluster/cluster.conf file\n\n\nnote personaly I start with an existing complete cluste.conf file, edit it to match the desired config then start the cluster\n\nif you want a better understanding of the config follow the steps below\n\n\n?xml version=\"1.0\"?\n\n\ncluster name=\"rhel59halvm\" config_version=\"1\"\n\n    \nclusternodes\n\n        \nclusternode name=\"rhel59halvmn1.private\" nodeid=\"1\" /\n\n        \nclusternode name=\"rhel59halvmn2.private\" nodeid=\"2\" /\n\n    \n/clusternodes\n\n    \nfencedevices /\n\n    \nrm\n\n        \nfailoverdomains /\n\n        \nresources /\n\n    \n/rm\n\n\n/cluster\n\n\n\n\nNote: this is a very simple cluster.conf file there is no fencing resources services etc..\n\n\nStart the cluster to see if everything works\n\n\n[root@rhel59halvmn1 cluster]# service cman start\nStarting cluster:\n   Loading modules... done\n   Mounting configfs... done\n   Starting ccsd... done\n   Starting cman... done\n   Starting daemons... done\n   Starting fencing... done\n   Tuning DLM... done\n                                                           [  OK  ]\n[root@rhel59halvmn1 cluster]# clustat\nCluster Status for rhel59halvm @ Wed Jun 26 17:59:12 2013\nMember Status: Quorate\n\n Member Name                                       ID   Status\n ------ ----                                       ---- ------\n rhel59halvmn1.private                                 1 Online, Local\n rhel59halvmn2.private                                 2 Online\n\n\n\nAdd your fencing device\n\n\nNote: because we didnt specify a fence method with the original cluster/conf version 1\n\n\nwe will now have to add the fence device and a fence method for each node. Personally I'd rather do it by hand using a text editor but...\n \n\nin this example im using the fence_manual device\n\n\nnote this is not a real fence device do not usse this in production see\n man fence_manual for more info\n\n\nIf you want you can use the ccs_tool to delete the individual node's config then re-add the node's config with the fence method__\n\n\nccs_tool addfence manual fake-parameter\n\n\n\nNote: because the fence_manual is a fake agent it doesnt require any parameters but the ccs_tool requires atleast 1\n\n    ccs_tool addnode rhel59halvmn1 -n1 -f manual\n    ccs_tool delnode rhel59halvmn2.private\n    ccs_tool addnode rhel59halvmn2 -n2 -f manual\n    cat /etc/cluster/cluster.conf\n\n\n?xml version=\"1.0\"?\n\n\ncluster alias=\"rhel59halvm\" config_version=\"7\" name=\"rhel59halvm\"\n\n  \nfence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/\n\n  \nclusternodes\n\n    \nclusternode name=\"rhel59halvmn1\" votes=\"1\" nodeid=\"1\"\n\n      \nfence\n\n        \nmethod name=\"single\"\n\n          \ndevice name=\"manual\"/\n\n        \n/method\n\n      \n/fence\n\n    \n/clusternode\n\n    \nclusternode name=\"rhel59halvmn2\" votes=\"1\" nodeid=\"2\"\n\n      \nfence\n\n        \nmethod name=\"single\"\n\n          \ndevice name=\"manual\"/\n\n        \n/method\n\n      \n/fence\n\n    \n/clusternode\n\n  \n/clusternodes\n\n  \nfencedevices\n\n    \nfencedevice name=\"manual\" agent=\"fake-parameter\"/\n\n  \n/fencedevices\n\n  \nrm\n\n    \nfailoverdomains/\n\n    \nresources/\n\n  \n/rm\n\n\n/cluster\n\n\n\n\nsync the cluster.conf file using sthe ccs_tool\n\n\nccs_tool update  /etc/cluster/cluster.conf\n\n\n\nConfigure the private/heartbeat nic\n\n\n*add multicast route for private/heartbeat\n\n\n/etc/sysconfig/network-scripts/route-eth1\n239.0.0.0/4 dev eth1\n\n\n\nEnsure the that the RedHat certificate is installed # not installed by default\n\n\nrpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release\n\n\n\nif not you may get errors like...\n\n\nPublic key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed\n\n\n\nCheck if the repo access is working\n\n\nyum clean all\nyum list\n\n\n\nEdit firewall rules to allow cluster traffic\n\n\n#/opt/firewall-rules\niptables -I INPUT --protocol tcp --dport 22 -j ACCEPT\n# Cluster\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT\n# turn off firewall for private nic\niptables -I INPUT -i eth1 -j ACCEPT\n\n\n\nif you want to use luci\n\n\nwarning\n due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs\nyou have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config\nYou can use ricci/luci to import an existing cluster\nTherefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci\n\n\nStarting ricci/luci\n\n\nif you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password\nstart up ricci on both servers and luci on one server \nsetup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci\n\n\nstart browser window https://127.0.0.1:80\n\n\nSimple cluster.conf file to get you started\n\n\n?xml version=\"1.0\"?\n\n\ncluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"\n\n        \nfence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/\n\n        \nclusternodes\n\n                \nclusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\"\n\n                        \nfence\n\n                                \nmethod name=\"1\"\n\n                                        \ndevice name=\"ipmi-hostname1\"/\n\n                                \n/method\n\n                        \n/fence\n\n                \n/clusternode\n\n                \nclusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\"\n\n                        \nfence\n\n                                \nmethod name=\"1\"\n\n                                        \ndevice name=\"ipmi-hostname2\"/\n\n                                \n/method\n\n                        \n/fence\n\n                \n/clusternode\n\n        \n/clusternodes\n\n        \ncman expected_votes=\"1\" two_node=\"1\"\n\n                \nmulticast addr=\"239.192.0.1\"/\n\n        \n/cman\n\n        \nfencedevices\n\n                \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/\n\n                \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/\n=\n        \n/fencedevices\n\n        \nrm\n\n                \nfailoverdomains\n\n                        \nfailoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"\n\n                                \nfailoverdomainnode name=\"hostname1.private\" priority=\"1\"/\n\n                                \nfailoverdomainnode name=\"hostname2.private\" priority=\"2\"/\n\n                        \n/failoverdomain\n\n                \n/failoverdomains\n\n                \nresources\n\n                \n/resources\n\n                \nservice autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"\n\n                \n/service\n\n        \n/rm\n\n\n/cluster\n\n\nscp cluster.conf hostname1:/etc/cluster/cluster.conf\n\nccs_tool update cluster.conf\n\n\n\nConfigure fencing\n\n\n\n\nSetup ipmi profile on the hardware \n\n\n\n\nTest to see if fencing works\n\n\necho -e \"ipaddr=192.168.1.1 \\nlogin=\n \\npasswd=\n \\naction=status\" | fence_ipmilan\n\n\n\n\n\n\nAdd the fencing details to the cluster.conf file\n    \n\n    \n\n\nservice rgmanager start\n\n\nservice cman start\n\n\nclustat\ncman_tool status\ntail -f /var/log/messages\n\n\n\n\n\n\nmanual fence override\n    fence_ack_manual -e -n hostname1.private\n\n\n\n\nmanaul service relocation\n    clusvcadm -r ServiceName\n\n\nifconfig will not display a VIP you have to run \n\n    ip address show\n\n\n\n\nyou may want to disable the acpi daemon otherwise your server may not switch off fast enough\n\n\nchkconfig --level 234 5 acpid off\nchkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off\n\n\n\nsetup HA-LVM\n\n\n...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide\n\n\nedit /etc/lvm/lvm.conf\n\n\n\n\n\n\nEnsure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'.\n\n\n\n\n\n\nEdit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file:\n\n\n\n\n\n\nvolume_list = [ \"vgsys\", \"@rhel59halvm1.private\" ]\n\n\nCreate the PV VG LV and filesystems    \non one of the nodes\n\n\npvcreate /dev/mapper/mpath1\nvgcreate vg_shared_pc /dev/mapper/mpath1\nlvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc\nmkfs.ext3 /dev/vg_shared_pc/lv_shared_pc\nmkdir /mnt/pc", 
            "title": "HA LVM cluster build guide"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_cluster_build_guide/#rhel-59-ha-lvm-cluster-install-and-setup-guide", 
            "text": "version 201600508  created by Seamus Murray  Prerequisites\n2 machines either physical or virtual  RHEL OS packages\nRHEL Cluster packages  these can be sourced from   Note write guides for each of these methods\n * RedHat directly RHN\n * Local YUM Repository\n * Local Satellite/Spacewalk server\n * Local media cdrom/dvdrom/usb drive/iso image etc.  In this guide I will be using the ISO of the RHEL 5.9 install \nThis ISO was downloaded from RHN and contains the following Repositories    Server  Cluster  ClusterStorage  VT   for the initial OS install we are just going to use the Server repo  Boot from the DVD/ISO \nStep though the installer note the package choice is dependant on what you want to do with the servers later \nEg if you want to run a GUI and web browser you will have to install those Groups\npersonally i always try to install the minimum number of packages to get the specific job done.  @base\n@core\n@editors\n@text-internet  Once you install has completed and you have booted into your new OS.  setup the hosts to connect to a   RHEL OS packages\nRHEL Cluster packages  these can be sourced from  \n * RedHat directy RHN\n * Local YUM Repository\n * Local Satellite/Spacewalk server\n * Local media cdrom/dvdrom/usb drive/iso image etc.  Install the required cluster utils for this example it is just \n * cman \n * rgamanger\n * openais  Note: for a HA-LVM cluster these is no need for any distributed lockmanager GFS etc...\n    if these are installed its best to remove them now  Disable the cluster from starting up... until you have finished the config  chkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off  Before we setup the cluster..\n * ensure your hostnames IP addresses and interfaces are in order.\n__in this exaple each node has 2 physical interfaces\n 1. eth0 the public side, this is where the service that will be clusterd will run from https,nfs,ftp etc\n 2. eth1 the private interface that is just used for cluster communication broadcast,multicast or unicast\n * ensure the host name is correct and a FQDN in /etc/sysconfig/network\n * ensure hostname is not in any of the ifcfg-ethx files \n * ensure hostname is not listed against loopback address in /etc/hosts #RedHat installer put this in by default\n * restart network services, run hostname command manually or reboot if necessary*\n * ensure the hostnames for all IPs on all the cluster nodes are listed in the /etc/hosts file  sample /etc/hosts file  192.168.56.101  rhel59halvmn1.local     rhel59halvmn1\n10.10.100.101   rhel59halvmn1.private   rhel59halvmn1-priv\n192.168.56.102  rhel59halvmn2.local     rhel59halvmn2\n10.10.100.102   rhel59halvmn2.private   rhel59halvmn2-priv\n192.168.56.103  rhel59halvm-vip.local   rhel59halvm-vip  Setup ssh keys between the nodes to make things easier  # ssh-copy-id -i /root/.ssh/id_rsa.pub rhel59halvmn2  Ensure the time on all cluster nodes are in sync  you may need to manually sync the time  alias REMOTETIME='ssh rhel59halvmn1 -C date'\ndate -s \"`REMOTETIME`\"\n\nservice ntp stop\nntpdate 192.168.1.1 \nntpdate 192.168.1.1 \nservice ntp start  Configure iptables to allow the cluster traffic through##  iptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 16851 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 21064 -j ACCEPT  Create a /etc/cluster/cluster.conf file  note personaly I start with an existing complete cluste.conf file, edit it to match the desired config then start the cluster if you want a better understanding of the config follow the steps below  ?xml version=\"1.0\"?  cluster name=\"rhel59halvm\" config_version=\"1\" \n     clusternodes \n         clusternode name=\"rhel59halvmn1.private\" nodeid=\"1\" / \n         clusternode name=\"rhel59halvmn2.private\" nodeid=\"2\" / \n     /clusternodes \n     fencedevices / \n     rm \n         failoverdomains / \n         resources / \n     /rm  /cluster   Note: this is a very simple cluster.conf file there is no fencing resources services etc..  Start the cluster to see if everything works  [root@rhel59halvmn1 cluster]# service cman start\nStarting cluster:\n   Loading modules... done\n   Mounting configfs... done\n   Starting ccsd... done\n   Starting cman... done\n   Starting daemons... done\n   Starting fencing... done\n   Tuning DLM... done\n                                                           [  OK  ]\n[root@rhel59halvmn1 cluster]# clustat\nCluster Status for rhel59halvm @ Wed Jun 26 17:59:12 2013\nMember Status: Quorate\n\n Member Name                                       ID   Status\n ------ ----                                       ---- ------\n rhel59halvmn1.private                                 1 Online, Local\n rhel59halvmn2.private                                 2 Online  Add your fencing device  Note: because we didnt specify a fence method with the original cluster/conf version 1  we will now have to add the fence device and a fence method for each node. Personally I'd rather do it by hand using a text editor but...   in this example im using the fence_manual device  note this is not a real fence device do not usse this in production see  man fence_manual for more info  If you want you can use the ccs_tool to delete the individual node's config then re-add the node's config with the fence method__  ccs_tool addfence manual fake-parameter  Note: because the fence_manual is a fake agent it doesnt require any parameters but the ccs_tool requires atleast 1 \n    ccs_tool addnode rhel59halvmn1 -n1 -f manual\n    ccs_tool delnode rhel59halvmn2.private\n    ccs_tool addnode rhel59halvmn2 -n2 -f manual\n    cat /etc/cluster/cluster.conf  ?xml version=\"1.0\"?  cluster alias=\"rhel59halvm\" config_version=\"7\" name=\"rhel59halvm\" \n   fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/ \n   clusternodes \n     clusternode name=\"rhel59halvmn1\" votes=\"1\" nodeid=\"1\" \n       fence \n         method name=\"single\" \n           device name=\"manual\"/ \n         /method \n       /fence \n     /clusternode \n     clusternode name=\"rhel59halvmn2\" votes=\"1\" nodeid=\"2\" \n       fence \n         method name=\"single\" \n           device name=\"manual\"/ \n         /method \n       /fence \n     /clusternode \n   /clusternodes \n   fencedevices \n     fencedevice name=\"manual\" agent=\"fake-parameter\"/ \n   /fencedevices \n   rm \n     failoverdomains/ \n     resources/ \n   /rm  /cluster   sync the cluster.conf file using sthe ccs_tool  ccs_tool update  /etc/cluster/cluster.conf  Configure the private/heartbeat nic  *add multicast route for private/heartbeat  /etc/sysconfig/network-scripts/route-eth1\n239.0.0.0/4 dev eth1  Ensure the that the RedHat certificate is installed # not installed by default  rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release  if not you may get errors like...  Public key for mkinitrd-5.1.19.6-79.el5.i386.rpm is not installed  Check if the repo access is working  yum clean all\nyum list  Edit firewall rules to allow cluster traffic  #/opt/firewall-rules\niptables -I INPUT --protocol tcp --dport 22 -j ACCEPT\n# Cluster\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 11111 -j ACCEPT\niptables -I INPUT -p udp -m state --state NEW -m multiport --dports 50007 -j ACCEPT\niptables -I INPUT -p tcp -m state --state NEW -m multiport --dports 50008 -j ACCEPT\n# turn off firewall for private nic\niptables -I INPUT -i eth1 -j ACCEPT  if you want to use luci  warning  due to the way the RedHat Satellite server is configured you cannot perform rpm or yum group installs\nyou have to specify the packages individually, because of this you can not use ricci/luci to create a cluster config\nYou can use ricci/luci to import an existing cluster\nTherefore if you really need the GUI create a simple cluster with no resources or services first then import it into lucci  Starting ricci/luci  if you have not previously configured a luci account run \" /usr/sbin/luci_admin init\" and set a password\nstart up ricci on both servers and luci on one server \nsetup putty forwarding from the rdp jump host local port 80 to 127.0.0.1:8084 on the host you will run luci  start browser window https://127.0.0.1:80  Simple cluster.conf file to get you started  ?xml version=\"1.0\"?  cluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\" \n         fence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/ \n         clusternodes \n                 clusternode name=\"hostname1.private\" nodeid=\"1\" votes=\"1\" \n                         fence \n                                 method name=\"1\" \n                                         device name=\"ipmi-hostname1\"/ \n                                 /method \n                         /fence \n                 /clusternode \n                 clusternode name=\"hostname2.private\" nodeid=\"2\" votes=\"1\" \n                         fence \n                                 method name=\"1\" \n                                         device name=\"ipmi-hostname2\"/ \n                                 /method \n                         /fence \n                 /clusternode \n         /clusternodes \n         cman expected_votes=\"1\" two_node=\"1\" \n                 multicast addr=\"239.192.0.1\"/ \n         /cman \n         fencedevices \n                 fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"test-user\" name=\"ipmi-hostname1\" passwd=\"test-password\" delay=\"30\"/ \n                 fencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"test-user\" name=\"ipmi-hostname2\" passwd=\"test-password\"/ =\n         /fencedevices \n         rm \n                 failoverdomains \n                         failoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\" \n                                 failoverdomainnode name=\"hostname1.private\" priority=\"1\"/ \n                                 failoverdomainnode name=\"hostname2.private\" priority=\"2\"/ \n                         /failoverdomain \n                 /failoverdomains \n                 resources \n                 /resources \n                 service autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\" \n                 /service \n         /rm  /cluster \n\nscp cluster.conf hostname1:/etc/cluster/cluster.conf\n\nccs_tool update cluster.conf", 
            "title": "RHEL 5.9 HA-LVM cluster install and setup guide"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_cluster_build_guide/#configure-fencing", 
            "text": "Setup ipmi profile on the hardware    Test to see if fencing works  echo -e \"ipaddr=192.168.1.1 \\nlogin=  \\npasswd=  \\naction=status\" | fence_ipmilan    Add the fencing details to the cluster.conf file\n     \n      service rgmanager start  service cman start  clustat\ncman_tool status\ntail -f /var/log/messages    manual fence override\n    fence_ack_manual -e -n hostname1.private   manaul service relocation\n    clusvcadm -r ServiceName  ifconfig will not display a VIP you have to run  \n    ip address show   you may want to disable the acpi daemon otherwise your server may not switch off fast enough  chkconfig --level 234 5 acpid off\nchkconfig --levels 2345 cman off\nchkconfig --levels 2345 rgmanager off\nchkconfig --levels 2345 ricci off\nchkconfig --levels 2345 luci off  setup HA-LVM  ...refer to section F.2. Configuring HA-LVM Failover with Tagging from RHEL 5 Cluster admin guide  edit /etc/lvm/lvm.conf    Ensure that the locking_type parameter in the global section of the /etc/lvm/lvm.conf file is set to the value '1'.    Edit the volume_list field in the /etc/lvm/lvm.conf file. Include the name of your root volume group and your hostname as listed in the /etc/cluster/cluster.conf file preceded by @. The hostname to include here is the machine on which you are editing the lvm.conf file, not any remote hostname. Note that this string MUST match the node name given in the cluster.conf file. Below is a sample entry from the /etc/lvm/lvm.conf file:    volume_list = [ \"vgsys\", \"@rhel59halvm1.private\" ]  Create the PV VG LV and filesystems     on one of the nodes  pvcreate /dev/mapper/mpath1\nvgcreate vg_shared_pc /dev/mapper/mpath1\nlvcreate -l 100%FREE -n lv_shared_pc vg_shared_pc\nmkfs.ext3 /dev/vg_shared_pc/lv_shared_pc\nmkdir /mnt/pc", 
            "title": "Configure fencing"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_manual_failover.txt/", 
            "text": "status of both nodes before failover\n\n\n\n\n\n\n\n\n\n\n[root@node2 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:26:36 2013\nMember Status: Quorate\n\nMember Name    ID Status\n------ ----   --- ------\nnode1.private   1 Online, rgmanager\nnode2.private   2 Online, Local, rgmanager\n\nService Name          Owner (Last)  State\n------- ----          ----- ------  -----\nservice:ServiceName   node1.private started\n\n\n[root@node2 ~]# pvs\n PV                 VG       Fmt   Attr PSize \n /dev/mpath/shared  sharedVG lvm2  a-   60G \n /dev/mpath/mpsysp2 vgsys    lvm2  a-   49G \n\n\n[root@node2 ~]# vgs\n VG       #PV #LV #SN Attr   VSize  VFree\n sharedVG  1   1   0  wz--n- 60G    0\n vgsys     1   7   0  wz--n- 49G   8G\n\n\n[root@node2 ~]# lvs\n LV            VG         Attr   LSize\n sharedLV    sharedVG -wi--- 60G\n home          vgsys      -wi-ao  2.00G\n root          vgsys      -wi-ao 16.00G\n swap          vgsys      -wi-ao 10.00G\n tmp           vgsys      -wi-ao  5.00G\n var           vgsys      -wi-ao  5.00G\n var_log       vgsys      -wi-ao  2.00G\n var_log_audit vgsys      -wi-ao  1.00G\n\n\n\n\n\n\n\n\n\n\n\n[root@node1 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:26:39 2013\nMember Status: Quorate\n\nMember Name    ID Status\n------ ----   --- ------\nnode1.private   1 Online, rgmanager\nnode2.private   2 Online, Local, rgmanager\n\nService Name          Owner (Last)  State\n------- ----          ----- ------  -----\nservice:ServiceName   node1.private started\n\n\n[root@node2 ~]# pvs\n PV                 VG       Fmt   Attr PSize\n /dev/mpath/shared  sharedVG lvm2  a-   60G\n /dev/mpath/mpsysp2 vgsys    lvm2  a-   49G\n\n\n[root@node2 ~]# vgs\n VG       #PV #LV #SN Attr   VSize  VFree\n sharedVG  1   1   0  wz--n- 60G    0\n vgsys     1   7   0  wz--n- 49G    8G\n\n\n[root@node1 ~]# lvs\n LV            VG         Attr   LSize\n sharedLV    sharedVG -wi-ao 60G\n home          vgsys      -wi-ao  2.00G\n root          vgsys      -wi-ao 14.62G\n swap          vgsys      -wi-ao 10.00G\n tmp           vgsys      -wi-ao  5.00G\n var           vgsys      -wi-ao  5.00G\n var_log       vgsys      -wi-ao  2.00G\n var_log_audit vgsys      -wi-ao  1.00G\n\n\n\n\n\n\n\n\n\n\n\nsyslog of both nodes during failover\n\n\nclusvcadm -r ServiceName\n\n\n\n\n\n\n\n\n\nMay 31 15:40:36 node2 root: seamus running clusvcadm -r ServiceName from node1\nMay 31 15:43:25 node2 clurgmgrd[6214]: \n Starting stopped service service:ServiceName\nMay 31 15:43:26 node2 clurgmgrd: [6214]: \n Activating sharedVG/sharedLV\nMay 31 15:43:26 node2 clurgmgrd: [6214]: \n Making resilient : lvchange -ay sharedVG/sharedLV\nMay 31 15:43:26 node2 clurgmgrd: [6214]: \n Resilient command: lvchange -ay sharedVG/sharedLV \n{wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]}\nMay 31 15:43:26 node2 multipathd: dm-12: devmap not registered, can't remove\nMay 31 15:43:26 node2 multipathd: dm-12: add map (uevent)\nMay 31 15:43:26 node2 kernel: kjournald starting.  Commit interval 5 seconds\nMay 31 15:43:26 node2 kernel: EXT3 FS on dm-12, internal journal\nMay 31 15:43:26 node2 kernel: EXT3-fs: mounted filesystem with ordered data mode.\nMay 31 15:43:28 node2 avahi-daemon[5577]: Registering new address record for 192.168.1.3 on eth0.\nMay 31 15:44:57 node2 clurgmgrd[6214]: \n Service service:ServiceName started\n\n\n\n\n\n\n\n\n\n\n\nMay 31 15:40:36 node1 root: seamus running clusvcadm -r ServiceName\nMay 31 15:40:45 node1 clurgmgrd[6229]: \n Stopping service service:ServiceName\nMay 31 15:43:14 node1 avahi-daemon[5564]: Withdrawing address record for 192.168.1.3 on eth0.\nMay 31 15:43:24 node1 multipathd: dm-12: umount map (uevent)\nMay 31 15:43:24 node1 clurgmgrd: [6229]: \n Deactivating sharedVG/sharedLV\nMay 31 15:43:24 node1 clurgmgrd: [6229]: \n Making resilient : lvchange -an sharedVG/sharedLV\nMay 31 15:43:24 node1 clurgmgrd: [6229]: \n Resilient command: lvchange -an sharedVG/sharedLV\n{wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]}\nMay 31 15:43:24 node1 multipathd: dm-12: remove map (uevent)\nMay 31 15:43:25 node1 clurgmgrd: [6229]: \n Removing ownership tag (node1.private) from sharedVG/sharedLV\nMay 31 15:43:25 node1 clurgmgrd[6229]: \n Service service:ServiceName is stopped\nMay 31 15:44:57 node1 clurgmgrd[6229]: \n Service service:ServiceName is now running on member 2\n\n\n\n\n\n\n\n\n\n\n\nstatus of both nodes after failover\n\n\n\n\n\n\n\n\n\n[root@node2 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:47:29 2013\nMember Status: Quorate\n\nMember Name    ID   Status\n------ ----   --- ------\nnode1.private   1 Online, rgmanager\nnode2.private   2 Online, Local, rgmanager\n\nService Name          Owner (Last)   State\n------- ----          ----- ------   -----\nservice:ServiceName   node2.private  started\n[root@node2 ~]# pvs\n  PV                  VG     Fmt  Attr PSize\n  /dev/mpath/shared sharedVG lvm2 a-   60G\n  /dev/mpath/mpsysp2  vgsys  lvm2 a-   49G\n[root@node2 ~]# vgs\n  VG         #PV #LV #SN Attr   VSize  VFree\n  sharedVG   1   1   0   wz--n- 60G    0\n  vgsys      1   7   0   wz--n- 49G    8G\n[root@node2 ~]# lvs\n  LV            VG         Attr   LSize\n  sharedLV    sharedVG -wi-ao 60G\n  home          vgsys      -wi-ao  2.00G\n  root          vgsys      -wi-ao 16.00G\n  swap          vgsys      -wi-ao 10.00G\n  tmp           vgsys      -wi-ao  5.00G\n  var           vgsys      -wi-ao  5.00G\n  var_log       vgsys      -wi-ao  2.00G\n  var_log_audit vgsys      -wi-ao  1.00G\n\n\n\n\n\n\n\n\n[root@node1 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:48:09 2013\nMember Status: Quorate\n\nMember Name    ID Status\n------ ----   --- ------\nnode1.private  1 Online, Local, rgmanager\nnode2.private  2 Online, rgmanager\n\nService Name          Owner (Last)   State\n ------- ----         ----- ------   -----\nservice:ServiceName   node2.private  started\n[root@node1 ~]# pvs\n  PV                  VG     Fmt  Attr PSize\n  /dev/mpath/shared sharedVG lvm2 a-   60G\n  /dev/mpath/mpsysp2  vgsys  lvm2 a-   49G \n[root@node1 ~]# vgs\n  VG         #PV #LV #SN Attr   VSize  VFree\n  sharedVG   1   1   0   wz--n- 60G    0\n  vgsys      1   7   0   wz--n- 49G    10.25G\n[root@node1 ~]# lvs\n  LV            VG         Attr   LSize\n  sharedLV    sharedVG -wi--- 60G\n  home          vgsys      -wi-ao  2.00G\n  root          vgsys      -wi-ao 14.62G\n  swap          vgsys      -wi-ao 10.00G\n  tmp           vgsys      -wi-ao  5.00G\n  var           vgsys      -wi-ao  5.00G\n  var_log       vgsys      -wi-ao  2.00G\n  var_log_audit vgsys      -wi-ao  1.00G", 
            "title": "HA LVM manual failover.txt"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_manual_failover.txt/#status-of-both-nodes-before-failover", 
            "text": "[root@node2 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:26:36 2013\nMember Status: Quorate\n\nMember Name    ID Status\n------ ----   --- ------\nnode1.private   1 Online, rgmanager\nnode2.private   2 Online, Local, rgmanager\n\nService Name          Owner (Last)  State\n------- ----          ----- ------  -----\nservice:ServiceName   node1.private started\n\n\n[root@node2 ~]# pvs\n PV                 VG       Fmt   Attr PSize \n /dev/mpath/shared  sharedVG lvm2  a-   60G \n /dev/mpath/mpsysp2 vgsys    lvm2  a-   49G \n\n\n[root@node2 ~]# vgs\n VG       #PV #LV #SN Attr   VSize  VFree\n sharedVG  1   1   0  wz--n- 60G    0\n vgsys     1   7   0  wz--n- 49G   8G\n\n\n[root@node2 ~]# lvs\n LV            VG         Attr   LSize\n sharedLV    sharedVG -wi--- 60G\n home          vgsys      -wi-ao  2.00G\n root          vgsys      -wi-ao 16.00G\n swap          vgsys      -wi-ao 10.00G\n tmp           vgsys      -wi-ao  5.00G\n var           vgsys      -wi-ao  5.00G\n var_log       vgsys      -wi-ao  2.00G\n var_log_audit vgsys      -wi-ao  1.00G    \n[root@node1 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:26:39 2013\nMember Status: Quorate\n\nMember Name    ID Status\n------ ----   --- ------\nnode1.private   1 Online, rgmanager\nnode2.private   2 Online, Local, rgmanager\n\nService Name          Owner (Last)  State\n------- ----          ----- ------  -----\nservice:ServiceName   node1.private started\n\n\n[root@node2 ~]# pvs\n PV                 VG       Fmt   Attr PSize\n /dev/mpath/shared  sharedVG lvm2  a-   60G\n /dev/mpath/mpsysp2 vgsys    lvm2  a-   49G\n\n\n[root@node2 ~]# vgs\n VG       #PV #LV #SN Attr   VSize  VFree\n sharedVG  1   1   0  wz--n- 60G    0\n vgsys     1   7   0  wz--n- 49G    8G\n\n\n[root@node1 ~]# lvs\n LV            VG         Attr   LSize\n sharedLV    sharedVG -wi-ao 60G\n home          vgsys      -wi-ao  2.00G\n root          vgsys      -wi-ao 14.62G\n swap          vgsys      -wi-ao 10.00G\n tmp           vgsys      -wi-ao  5.00G\n var           vgsys      -wi-ao  5.00G\n var_log       vgsys      -wi-ao  2.00G\n var_log_audit vgsys      -wi-ao  1.00G", 
            "title": "status of both nodes before failover"
        }, 
        {
            "location": "/linux/Clustering/HA_LVM_manual_failover.txt/#syslog-of-both-nodes-during-failover", 
            "text": "clusvcadm -r ServiceName     \nMay 31 15:40:36 node2 root: seamus running clusvcadm -r ServiceName from node1\nMay 31 15:43:25 node2 clurgmgrd[6214]:   Starting stopped service service:ServiceName\nMay 31 15:43:26 node2 clurgmgrd: [6214]:   Activating sharedVG/sharedLV\nMay 31 15:43:26 node2 clurgmgrd: [6214]:   Making resilient : lvchange -ay sharedVG/sharedLV\nMay 31 15:43:26 node2 clurgmgrd: [6214]:   Resilient command: lvchange -ay sharedVG/sharedLV \n{wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]}\nMay 31 15:43:26 node2 multipathd: dm-12: devmap not registered, can't remove\nMay 31 15:43:26 node2 multipathd: dm-12: add map (uevent)\nMay 31 15:43:26 node2 kernel: kjournald starting.  Commit interval 5 seconds\nMay 31 15:43:26 node2 kernel: EXT3 FS on dm-12, internal journal\nMay 31 15:43:26 node2 kernel: EXT3-fs: mounted filesystem with ordered data mode.\nMay 31 15:43:28 node2 avahi-daemon[5577]: Registering new address record for 192.168.1.3 on eth0.\nMay 31 15:44:57 node2 clurgmgrd[6214]:   Service service:ServiceName started    \nMay 31 15:40:36 node1 root: seamus running clusvcadm -r ServiceName\nMay 31 15:40:45 node1 clurgmgrd[6229]:   Stopping service service:ServiceName\nMay 31 15:43:14 node1 avahi-daemon[5564]: Withdrawing address record for 192.168.1.3 on eth0.\nMay 31 15:43:24 node1 multipathd: dm-12: umount map (uevent)\nMay 31 15:43:24 node1 clurgmgrd: [6229]:   Deactivating sharedVG/sharedLV\nMay 31 15:43:24 node1 clurgmgrd: [6229]:   Making resilient : lvchange -an sharedVG/sharedLV\nMay 31 15:43:24 node1 clurgmgrd: [6229]:   Resilient command: lvchange -an sharedVG/sharedLV\n{wrapped line} --config devices{filter=[\"a|/dev/mpath/shared|\",\"a|/dev/mpath/mpsysp2|\",\"r|.*|\"]}\nMay 31 15:43:24 node1 multipathd: dm-12: remove map (uevent)\nMay 31 15:43:25 node1 clurgmgrd: [6229]:   Removing ownership tag (node1.private) from sharedVG/sharedLV\nMay 31 15:43:25 node1 clurgmgrd[6229]:   Service service:ServiceName is stopped\nMay 31 15:44:57 node1 clurgmgrd[6229]:   Service service:ServiceName is now running on member 2     status of both nodes after failover     \n[root@node2 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:47:29 2013\nMember Status: Quorate\n\nMember Name    ID   Status\n------ ----   --- ------\nnode1.private   1 Online, rgmanager\nnode2.private   2 Online, Local, rgmanager\n\nService Name          Owner (Last)   State\n------- ----          ----- ------   -----\nservice:ServiceName   node2.private  started\n[root@node2 ~]# pvs\n  PV                  VG     Fmt  Attr PSize\n  /dev/mpath/shared sharedVG lvm2 a-   60G\n  /dev/mpath/mpsysp2  vgsys  lvm2 a-   49G\n[root@node2 ~]# vgs\n  VG         #PV #LV #SN Attr   VSize  VFree\n  sharedVG   1   1   0   wz--n- 60G    0\n  vgsys      1   7   0   wz--n- 49G    8G\n[root@node2 ~]# lvs\n  LV            VG         Attr   LSize\n  sharedLV    sharedVG -wi-ao 60G\n  home          vgsys      -wi-ao  2.00G\n  root          vgsys      -wi-ao 16.00G\n  swap          vgsys      -wi-ao 10.00G\n  tmp           vgsys      -wi-ao  5.00G\n  var           vgsys      -wi-ao  5.00G\n  var_log       vgsys      -wi-ao  2.00G\n  var_log_audit vgsys      -wi-ao  1.00G    \n[root@node1 ~]# clustat\nCluster Status for round-and-round @ Fri May 31 15:48:09 2013\nMember Status: Quorate\n\nMember Name    ID Status\n------ ----   --- ------\nnode1.private  1 Online, Local, rgmanager\nnode2.private  2 Online, rgmanager\n\nService Name          Owner (Last)   State\n ------- ----         ----- ------   -----\nservice:ServiceName   node2.private  started\n[root@node1 ~]# pvs\n  PV                  VG     Fmt  Attr PSize\n  /dev/mpath/shared sharedVG lvm2 a-   60G\n  /dev/mpath/mpsysp2  vgsys  lvm2 a-   49G \n[root@node1 ~]# vgs\n  VG         #PV #LV #SN Attr   VSize  VFree\n  sharedVG   1   1   0   wz--n- 60G    0\n  vgsys      1   7   0   wz--n- 49G    10.25G\n[root@node1 ~]# lvs\n  LV            VG         Attr   LSize\n  sharedLV    sharedVG -wi--- 60G\n  home          vgsys      -wi-ao  2.00G\n  root          vgsys      -wi-ao 14.62G\n  swap          vgsys      -wi-ao 10.00G\n  tmp           vgsys      -wi-ao  5.00G\n  var           vgsys      -wi-ao  5.00G\n  var_log       vgsys      -wi-ao  2.00G\n  var_log_audit vgsys      -wi-ao  1.00G", 
            "title": "syslog of both nodes during failover"
        }, 
        {
            "location": "/linux/Clustering/RHEL4_cluster_installation/", 
            "text": "Red Hat 4 Cluster Install\n\n\n#!/bin/bash\nexit # this is here encase you run this as a script\n# install cluster suite for rhes4u3 last updated 17th April 2007 by Seamus\n\n# Below details how install the redhat cluster suite and GFS on any redhat ES4 Update 3 box\n# the prerequisites are based upon the RHEL4u3 kickstart build prepared by Seamus\n# fileserver.example.com:/u1/Distros/KickStart_rhes4u3/RHEL4u3_32bit.iso\n# All of the software below is installed from an NFS server\n# Simply cut and paste the sections into a root shell,\n# WARNING there are no error checks in this file so please don't run as a script...\n# just watch the terminal for any errors such as cant find rpm, cant mount NFS volume etc.\n# I have divided up the commands based on the source of the software\n# There are a few comments down the bottom which need to be actioned manually\n\nNFS_SERVER=fileserver.example.com\nNFS_SHARE=/u1/Distros\nVERSION=rhes4u3\nNFS_PATH=$NFS_SHARE/$VERSION\nTEMP_MOUNT=/tmp/software\nmkdir $TEMP_MOUNT\nmount $NFS_SERVER:$NFS_SHARE $TEMP_MOUNT\n\n#######################################\n# Install cluster suite prerequisites #\n#######################################\n# The following RedHat RPM's are not part of the standard build prepared by Seamus\n# and therefore will need to be installed prior to the installation of the cluster suite\n\nmkdir /tmp/rhcs_install\nmount -o ro,loop -t iso9660 /tmp/software/rhes4u3/rhel-4-u3-rhcs-i386.iso /tmp/rhcs_install\nRPM_PATH=/tmp/software/rhes4u3/install/RedHat/RPMS/\n\nrpm -ivh $RPM_PATH/libidn-0.5.6-1.i386.rpm\nrpm -ivh $RPM_PATH/curl-7.12.1-8.rhel4.i386.rpm\nrpm -ivh $RPM_PATH/php-pear-4.3.9-3.9.i386.rpm $RPM_PATH/php-4.3.9-3.9.i386.rpm\nrpm -ivh $RPM_PATH/device-mapper-multipath-0.4.5-12.0.RHEL4.i386.rpm\n\n#######################################\n# Install cluster suite               #\n#######################################\n# not all of the following are required to get the cluster running\n# I figured its easier to have them here just in-case you need them in the future\n\nRHCS_PATH=/tmp/rhcs_install/RedHat/RPMS/\n\nrpm -ivh $RHCS_PATH/ipvsadm-1.24-6.i386.rpm\nrpm -ivh $RHCS_PATH/piranha-0.8.2-1.i386.rpm\nrpm -ivh $RHCS_PATH/perl-Net-Telnet-3.03-3.noarch.rpm\nrpm -ivh $RHCS_PATH/magma-1.0.4-0.i686.rpm\nrpm -ivh $RHCS_PATH/ccs-1.0.3-0.i686.rpm\nrpm -ivh $RHCS_PATH/gulm-1.0.6-0.i686.rpm\nrpm -ivh $RHCS_PATH/cman-kernel-2.6.9-43.8.i686.rpm\nrpm -ivh $RHCS_PATH/cman-1.0.4-0.i686.rpm\nrpm -ivh $RHCS_PATH/cman-kernel-smp-2.6.9-43.8.i686.rpm\nrpm -ivh $RHCS_PATH/fence-1.32.18-0.i686.rpm\nrpm -ivh $RHCS_PATH/rgmanager-1.9.46-0.i386.rpm\nrpm -ivh $RHCS_PATH/system-config-cluster-1.0.25-1.0.noarch.rpm\nrpm -ivh $RHCS_PATH/iddev-2.0.0-3.i686.rpm\n\n# add these to the production servers\nrpm -ivh $RHCS_PATH/dlm-1.0.0-5.i686.rpm\nrpm -ivh $RHCS_PATH/dlm-kernel-2.6.9-41.7.i686.rpm\nrpm -ivh $RHCS_PATH/dlm-kernheaders-2.6.9-41.7.i686.rpm\nrpm -ivh $RHCS_PATH/dlm-kernel-smp-2.6.9-41.7.i686.rpm\nrpm -ivh $RHCS_PATH/magma-plugins-1.0.6-0.i386.rpm\n\n######################################\n# Install the few GFS packages       #\n######################################\n\nmkdir /tmp/rhgfs_install\n\nmount -o ro,loop -t iso9660 /tmp/software/rhes4u3/rhel-4-u3-rhgfs-i386.iso /tmp/rhgfs_install\n\nRHGFS_PATH=/tmp/rhgfs_install/RedHat/RPMS/\n\nrpm -ivh $RHGFS_PATH/GFS-6.1.5-0.i386.rpm\nrpm -ivh $RHGFS_PATH/GFS-kernel-smp-2.6.9-49.1.i686.rpm\nrpm -ivh $RHGFS_PATH/lvm2-cluster-2.02.01-1.2.RHEL4.i386.rpm\n\n#########################################################\n#                                                       #\n# This is the end of the cluster software installation  #\n# All further steps should be performed manually        #\n#                                                       #\n#########################################################\n\n# You need to setup the host file on each cluster node\n# either cut and paste into a shell or paste in a VI session\n# watch out for tabs vs white space when you cut and paste\n\nTIME=`date +%Y_%m_%d_%H%M`\ncp /etc/hosts /etc/hosts_$TIME.bak\ncat \n/etc/hosts\n\n# Do not remove the following line, or various programs\n# that require network functionality will fail.\n127.0.0.1 localhost.localdomain localhost\n\n# production search engine host file last updated 13th April 2007\n\n#temporary blue IPs\n10.10.10.50 example10.example.com example10\n10.10.10.51 example11.example.com example11\n10.10.10.52 example12.example.com example12\n10.10.10.53 example13.example.com example13\n10.10.10.54 example14.example.com example14\n10.10.10.55 example15.example.com example15\n10.10.10.56 example16.example.com example16\n10.10.10.57 example17.example.com example17\n10.10.10.58 example18.example.com example18\n10.10.10.59 example19.example.com example19\n\n# ilo interfaces\n# warning if you move a blade the ilo ip will change\n10.10.10.24 example10-ilo\n10.10.10.25 example11-ilo\n10.10.10.26 example12-ilo\n10.10.10.27 example13-ilo\n10.10.10.28 example14-ilo\n10.10.10.40 example15-ilo\n10.10.10.41 example16-ilo\n10.10.10.42 example17-ilo\n10.10.10.43 example18-ilo\n10.10.10.44 example19-ilo\n\n#end of hosts file\n\nEOF\n\n################################################\n#                                              #\n# Setting up the fibre channel cards and paths #\n#                                              #\n################################################\n\n# make backup of multipath.conf file\nTIME=`date +%Y_%m_%d_%H%M`\ncp /etc/multipath.conf /etc/multipath.conf_$TIME.bak\n\n#To enable the mutipathd to scan for luns,\n#you need to comment out the following 3 lines in /etc/multipath.conf\n\n#devnode_blacklist {\n# devnode \"*\"\n#}\n\n# at this point its easiest to reboot you can lookup the rescan method\n# for you particular HBA driver but these always change\n# once the box comes back up run\nmultipath -l\n\n#you should see somtheing like\n\nmpath1 (360060e80000000000000000000000000000)\n[size=500 GB][features=\"0\"][hwhandler=\"0\"]\n\\_ round-robin 0 [active]\n\\_ 0:0:0:0 sda 8:0 [active][ready]\n\\_ round-robin 0 [enabled]\n\\_ 1:0:0:0 sdb 8:16 [active][ready]\n\n#edit /etc/ssh/sshd_config and change the permit root login just\n\nmkdir /root/scripts\ntouch /root/scripts/cluster_services.sh\nchmod 700 /root/scripts/cluster_services.sh\nvi /root/scripts/cluster_services.sh\n\n\n\n#!/bin/bash\n# this script makes it easy to enable and disable all the cluster services from automatically starting\n\nACTION=$1\nif [ -z \"${ACTION}\" ]; then\necho \"Usage: $0 on|off|start|stop|status\"\nexit 1\nfi\n\nif [ ${ACTION} = \"on\" ]; then\nprintf \"Setting cluster services to start on runlevels 2345\\n\"\n\nchkconfig --level 2345 ccsd on\nchkconfig --level 2345 cman on\nchkconfig --level 2345 fenced on\nchkconfig --level 2345 clvmd on\nchkconfig --level 2345 gfs on\nchkconfig --level 2345 rgmanager on\n\nelif [ ${ACTION} = \"off\" ]; then\nprintf \"Turning cluster services off for runlevels 2345\\n\"\n\nchkconfig --level 2345 ccsd off\nchkconfig --level 2345 cman off\nchkconfig --level 2345 fenced off\nchkconfig --level 2345 clvmd off\nchkconfig --level 2345 gfs off\nchkconfig --level 2345 rgmanager off\n\nelif [ ${ACTION} = \"status\" ]; then\n\n/usr/sbin/clustat\n\nelif [ ${ACTION} = \"stop\" ]; then\nprintf \"Run the following commands manually in order\";echo\n\necho \"service rgmanager stop\"\necho \"service gfs stop\"\necho \"service clvmd stop\"\necho \"service fenced stop\"\necho \"service cman stop\"\necho \"service ccsd stop\"\n\nelif [ ${ACTION} = \"start\" ]; then\nprintf \"Run the following commands manually in order\";echo\n\necho \"service ccsd start\"\necho \"service cman start\"\necho \"service fenced start\"\necho \"service clvmd start\"\necho \"service gfs start\"\necho \"service rgmanager start\"\n\nfi\n\n#end of file\n\n\n\n\n# Below is the initial cluster configuration file that I created, the only deviation from standard is..\n# I setup a unique ILO FENCE account on each host node, these account name's are based on the host name.\n# This step is very important when dealing with blade enclosures\n\n# The reason for having unique names is due the cluster fencing mechanism.\n# The problem stems from the way the ILO's IP addresses are assigned via DHCP,\n# The IP addresses are assigned based upon the physical location of the blade within the enclosure cabinet.\n# note they are not permanently assigned to a particular blade.\n# ie of you move a blade from slot 9 to slot 10, its ILO IP address will also change, and what ever blade is put\n# back into slot 9 will inherit its old IP. Who cares i hear you say. What if you put someone else's server in the old slot 9\n# and the cluster tries to fence the the blade you have just moved to slot 10.\n# If you failed to update the /etc/hosts file The fencing mechanism will shut down the wrong node (if it could log in).\n\n# So when ever a blade is physically moved make sure you update the hosts file on each cluster node and manually test login via ssh\n\n# There is a bug in the ILOs ssh daemon that prevents you from logging in, a work around is to create a ssh conf file with\n# ForwardAgent no then call it when you ssh by ssh -F -u\n\nmkdir /root/.ssh\necho \"ForwardAgent no\" \n .ssh/ilo_bug\n\nssh -F .ssh/ilo_bug xxxxxx_test_fence@10.10.10.29\n# to restart server typr\n\nFencing via the HP ILO\nField Description\nName A name for the server with HP iLO support.\nLogin The login name used to access the device.\nPassword The password used to authenticate the connection to the device.\nHostname The hostname assigned to the device.\n\n###############################################\n#                                             #\n# sample initial /etc/cluster/cluster.conf    #\n#                                             #\n###############################################\n\n\n?xml version=\"1.0\" ?\n\nremoved   \n\n\n\n\n\nThis change is made by the installation of the vm2-cluster-2.02.01-1.2.RHEL4.i386.rpm\n\n\ndiff /etc/lvm/lvm.conf /etc/lvm/lvm.conf.lvmconfold\n172,173d171\n\n library_dir = \"/usr/lib\"\n\n locking_library = \"liblvm2clusterlock.so\"\n215c213\n\n locking_type = 2\n---\n\n locking_type = 1\n\n\n\n\n\n\nThe following directory's need to be shared between all nodes via GFS or NFS\n\n\n/home/search\n/big/search\n/var/www/html/search\n\nmanually replicate this file between the nodes\n/etc/httpd/conf/funnelback-httpd.conf", 
            "title": "RHEL4 cluster installation"
        }, 
        {
            "location": "/linux/Clustering/cluster_website/", 
            "text": "https://alteeve.ca/w/Cluster.conf", 
            "title": "Cluster website"
        }, 
        {
            "location": "/linux/Clustering/homemade_cluster/", 
            "text": "To mitigate a site failure at the primary site the 4 custom_application applications are failed over to the secondary site.\n\n\nA windows 2008 tie-breaker server located at a third site  monitors the primary cluster via SSH every 5 seconds. \n\n\nIf it detects a failure it asks the DR custom_application server to confirm that the primary site has failed.\n\n\nIf both the tie-breaker and the DR server agree that the primary site is down. Then the tie-breaker server executes the failover scripts located on the DR servers which perform the failover.\n\n\nOperating Systems\n\n\n\n\nApplication servers are RHEL 5.9\n\n\nSteward server is windows 2008\n\n\n\n\nThe tie-breaker server requires...\n\n\n\n\npowershell \n\n\ndotnet \n\n\nuser account \n\n\nscheduled task for each monitored cluster\n\n\npowershell script\n\n\nssh wrapper script (powershell to dotnet) ssh.net library\n\n\nlog directory\n\n\npublic ssh key of the primary and the DR servers (to enable paswordlSERVER login)\n\n\n\n\nFiles\n\n\nWindows Steward server\n\n\nid_rsa              ## ssh private key to allow loginto linux servers\nid_rsa.pub          ## ssh public key to allow loginto linux servers\nRenci.SshNet.dll    ## SSH.net library from http://sshnet.codeplex.com/ \nSSH-SSERVERions.psd1   ## powershell wrapper for SSH.net from http://www.powershelladmin.com/wiki/SSH_from_PowerShell_using_the_SSH.NET_library\nSSH-SSERVERions.psm1   ## licence file\nssh-fail.ps1        ## powershell script to login and checl custom_application servers from Seamus Murray\nschedule.xml        ## exported windows shedual task to run the script\n\n\n\nApplication Primary Server\n\n\nPrimaryTest.sh      ## bash script locally execute on the Application Primary servers from Seamus Murray\n\n\n\nApplication DR Server\n\n\nConfirmPriDown.sh   ## remotely connects to Application Primary Server and runs PrimaryTest.sh from Seamus Murray\nMountStart.sh       ## Starts the Application application\nBreak-n-Mount.sh    ## only on the index servers remotely connects to netapp controller, breaks the mirror, maps the lun and mounts it\n\n\n\nNetApp controller\n\n\nid_rsa.pub          ## ssh public key of SnapMirror-user on the 2 custom_application index DR servers\n\n\n\n\n\nDiagrams\n\n\n\n\nScript contents.................\n\n\nssh-fail.ps1\n\n\n#ssh-fail.ps1        \n#powershell script to login and check custom_application servers from Seamus Murray\n\nParam(\n  [string]$cluster  \n)\nImport-Module  SSH-SSERVERions\n\n\n#Start-Transcript -path C:\\ssh-fail2\\logs\\transcript.txt -append\n\n\n##set dubug level\n\n#$debug=1\n#if ( $debug -eq 1 ) {\n#               # Debug log file\n#               $deboutfile=$scriptdir+\"\\outputlog.\"+$siteid+\".log\"\n#               start-transcript -path $deboutfile -force\n#               # Shows a trace of each line being run with variables as variables\n#               set-psdebug -trace 1\n#\n#}\n\nif ( $cluster -eq 'Indexer' -Or  $cluster -eq 'DataBase' -Or $cluster -eq 'FrontEndA' -Or $cluster -eq 'FrontEndB')\n{\n\n}\nelse{\nWrite-Host \"you must specify the cluster to test using an argument\"\nbreak\n}\n\n##           IP                    Function             Hostname          Role    \nSwitch ($cluster)\n{\nFrontEndA {\n$server_P1='10.10.10.51' #a    Application FrontEnd     SERVERFW1TS    QLD-APP-5\n$server_P2='10.10.10.52' #a    Application FrontEnd     SERVERFW2TS    QLD-APP-6\n$server_VIP='10.10.10.53'#a    Application FrontEnd     VIPA\n$server_DR='10.10.11.32' #a    Application FrontEnd     SERVERFW3TS    SYD-APP-3\n}\nFrontEndB{\n$server_P1='10.10.10.55' #b    Application FrontEnd     SERVERFW3TS    QLD-APP-7\n$server_P2='10.10.10.56' #b    Application FrontEnd     SERVERFW4TS    QLD-APP-8\n$server_VIP='10.10.10.57'#b    Application FrontEnd     VIPB \n$server_DR='10.10.11.33' #b    Application FrontEnd     SERVERFW2TS    SYD-APP-4\n}\nIndexer{\n$server_P1='10.10.10.77' #c    Application Indexer      SERVERIX1TS    QLD-APP-1\n$server_P2='10.10.10.78' #c    Application Indexer      SERVERIX2TS    QLD-APP-2\n$server_VIP='10.10.10.76'#c    Application Indexer      VIPI\n$server_DR='10.10.11.17' #c    Application Indexer      SERVERIX3TS    SYD-APP-1\n}\nDataBase{\n$server_P1='10.10.10.85' #d    Application DataBase     SERVERHD1TS    QLD-APP-3\n$server_P2='10.10.10.86' #d    Application DataBase     SERVERHD2TS    QLD-APP-4\n$server_VIP='10.10.10.84'#d    Application DataBase     VIPD\n$server_DR='10.10.11.18' #d    Application DataBase     SERVERHD3TS    SYD-APP-2\n}\n}\n\n##set log file to local directory  eg..2012-01-1_0000_10.10.10.51_custom_applicationfail\n$Logfile = \"c:\\ssh-fail2\\logs\\$(get-date -uformat %Y-%m-%d_%H%M)\"+\"_$server_P1\"+\"_custom_applicationfail.log\"\n\n#Write_Host $Logfile\n#LogWrite  $env:PSModulePath\nFunction LogWrite\n{\n   Param ([string]$logstring)\n\n   Add-content $Logfile -value $logstring\n}\n\n$stamped  = \"$(Get-Date)\" + \" starting script \"\nLogWrite $stamped\n\n\n\n\n\n#Remote Scripts executed on the linux servers but called from this script\n#You must specify the argument \"Up\" case sensitive for this test to succeed \n#If you want to simulate this test failing just change the argument\n$ApplicationPrimaryTest='/home/failover-user/custom_applicationfailover0.1/primary-test.sh Up'\n\n#Specifiy which server the DR should test by assigning a single argument....P1 P2 or VIP\n#the Various IPs are stored both locally in this file and in   ApplicationConfirmPriDown.sh on the respective DR servers\n#If you want to simulate this test failing just change the argument to something else\n$ApplicationConfirmPriDown='/home/failover-user/custom_applicationfailover0.1/primary-confirm-fail.sh VIP'\n\n#this command needs to execute the start up script via sudo this either requires a tty which \"SSH-SSERVERions\" doesn't provide or..editing sudo to disable the requiretty in /etc/sudoers\n#This script varies slightly between the Application FrontEnds and the Application Indexers\n#On the Application indexers the NetApp mirrored lun's need to be broken and mounted this is handled by the..\n#Break-n-Mount.sh script called from within the ApplicationMountStart.sh executed from the DR servers\n$ApplicationMountStart='/home/failover-user/custom_applicationfailover0.1/initiate-dr.sh Start'\n\n#called from within $ApplicationMountStart on the Indexer DR servers\n#$Break-n-Mount='/home/failover-user/custom_applicationfailover0.1/break-snap-mirror.sh RESYNC'\n\n\n\n\n\nwhile(\"forever\")\n{\n    New-SshSSERVERion -ComputerName $server_P1 -Username 'failover-user'  -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null \n    New-SshSSERVERion -ComputerName $server_DR -Username 'failover-user'  -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null\n\n\ntry\n{\n #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 1st loop\"\n $stamped = \"$(Get-Date)\" + \" Testing ApplicationPrimaryTest on $server_P1 1st loop\"\n LogWrite  $stamped\n $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet\n}\ncatch [Exception]\n{ \n  $CmdOutput1 = \"SSH_SSERVERION_FAILED\" \n  #Write_Host\"ERROR: $CmdOutput1 during 1st loop\" -foregroundcolor white -backgroundcolor red\n  $stamped  = \"$(Get-Date)\" + \" ERROR: $CmdOutput1 during 1st loop\"\n  LogWrite $stamped\n}\n\n\n    #Check Primary Server for status\n    if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { \n        #Write_Host \"ERROR: Primary Failure Detected\"\n        #Write_Host \"Waiting 10 seconds before retrying\"\n        $stamped =  \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up   Waiting 10 seconds before retrying\"        \n        LogWrite $stamped\n        sleep 10\n\n\n        try\n       {\n           #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 2nd loop\"\n           $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet\n       }\n       catch [Exception]\n           {\n             $CmdOutput1 = \"SSH_SSERVERION_FAILED\"\n             #Write_Host\"$CmdOutput1 during 2nd loop\" -foregroundcolor white -backgroundcolor red\n             $stamped =  \"$(Get-Date)\" + \" $CmdOutput1 during 2nd loop\"\n             LogWrite $stamped\n       }\n\n                  #Check Primary Server for status after a previous failure\n                  if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { \n                  #Write_Host \"ERROR: Primary Failure Detected 2 times\"\n                $stamped =  \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up after 2 tries\"\n                LogWrite $stamped\n\n           try\n              {\n                  $CmdOutput2 = Invoke-SshCommand -ComputerName $server3 -Command $ApplicationPrimaryTest -Quiet\n              }\n              catch [Exception]\n                  {\n                  #Write_Host \"ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\"\n               $stamped =  \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\"\n               LogWrite $stamped\n              }\n\n                    #If this host fails 2 time to determine if Primary_App_Is_Up, then ask DR server to also run the check          \n                    if ( $CmdOutput2 -ne 'Primary_App_Is_Up' ) {     \n                    #Write_Host \"ERROR: DR Server $server3 is also reporting Primary Failure....... Need to Initiate DR\"\n                    $stamped =  \"$(Get-Date)\" + \" ERROR: DR Server is also reporting Primary Failure....... Need to Initiate DR\"\n                    LogWrite $stamped\n\n\n                try\n            {\n             $CmdOutput3 = Invoke-SshCommand -ComputerName $server_DR -Command $ApplicationMountStart  -Quiet\n            }\n            catch [Exception]\n            {\n            $CmdOutput3 = \"SSH_SSERVERION_FAILED\"\n              #Write_Host \"ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\"\n            $stamped =  \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\"\n            LogWrite $stamped\n\n                        }\n\n\n                if ( $CmdOutput3 -ne 'App_Started' ) {     \n                #Write_Host \"ERROR: DR server $server_DR Failed to start the App\"\n                $stamped =  \"$(Get-Date)\" + \" ERROR: DR server $server_DR Failed to start the App\"\n                LogWrite $stamped\n\n\n                }\n                else {\n                   #Write_Host \"DR server $server_DR has started the App\"\n                   $stamped =  \"$(Get-Date)\" + \" DR server $server_DR has started the App\"\n                   LogWrite $stamped\n                   $stamped =  \"$(Get-Date)\" + \" Nothing else to do...Failover script self terminating\"\n                   LogWrite $stamped\n                   break\n                }\n\n        }\n        else {\n            #Write_Host \"DR server $server_DR is reporting Primary $server_P1 is OK: Nothing To Do\"\n            #Write_Host \"Assuming the link between me and the Primary server has failed\"\n            $stamped =  \"$(Get-Date)\" + \" Assuming the link between me and the Primary server has failed\"\n            LogWrite $stamped\n        } \n    }\n              else {\n                #Write_Host \"Primary server $server_P1 is OK: Nothing To Do             2nd test\"   \n                $stamped =  \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do             2nd test\" \n                LogWrite $stamped\n                   } \n    }\n    else {\n        #Write_Host \"Primary server $server_P1 is OK: Nothing To Do             1st test\"   \n        $stamped =  \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do             1st test\" \n        LogWrite $stamped\n    }\n\n    sleep 5\n\n}\n\nRemove-SshSSERVERion -RemoveAll\n\n#Stop-Transcript\n\n\n\nschedule.xml\n\n\n#schedule.xml\n#exported windows shedual task to run the script\n\n?xml version=\"1.0\" encoding=\"UTF-16\"?\n\n\nTask version=\"1.2\" xmlns=\"http://schemas.microsoft.com/windows/2004/02/mit/task\"\n\n  \nRegistrationInfo\n\n    \nDate\n2012-01-1T12:30:10\n/Date\n\n    \nAuthor\nABCDEFG123\\seamus\n/Author\n\n  \n/RegistrationInfo\n\n  \nTriggers\n\n    \nRegistrationTrigger\n\n      \nRepetition\n\n        \nInterval\nPT15M\n/Interval\n\n        \nStopAtDurationEnd\nfalse\n/StopAtDurationEnd\n\n      \n/Repetition\n\n      \nExecutionTimeLimit\nPT1H\n/ExecutionTimeLimit\n\n      \nEnabled\ntrue\n/Enabled\n\n    \n/RegistrationTrigger\n\n  \n/Triggers\n\n  \nPrincipals\n\n    \nPrincipal id=\"Author\"\n\n      \nUserId\nABCDEFG123\\Administrator\n/UserId\n\n      \nLogonType\nPassword\n/LogonType\n\n      \nRunLevel\nHighestAvailable\n/RunLevel\n\n    \n/Principal\n\n  \n/Principals\n\n  \nSettings\n\n    \nMultipleInstancesPolicy\nStopExisting\n/MultipleInstancesPolicy\n\n    \nDisallowStartIfOnBatteries\nfalse\n/DisallowStartIfOnBatteries\n\n    \nStopIfGoingOnBatteries\ntrue\n/StopIfGoingOnBatteries\n\n    \nAllowHardTerminate\ntrue\n/AllowHardTerminate\n\n    \nStartWhenAvailable\ntrue\n/StartWhenAvailable\n\n    \nRunOnlyIfNetworkAvailable\nfalse\n/RunOnlyIfNetworkAvailable\n\n    \nIdleSettings\n\n      \nStopOnIdleEnd\ntrue\n/StopOnIdleEnd\n\n      \nRestartOnIdle\nfalse\n/RestartOnIdle\n\n    \n/IdleSettings\n\n    \nAllowStartOnDemand\ntrue\n/AllowStartOnDemand\n\n    \nEnabled\ntrue\n/Enabled\n\n    \nHidden\nfalse\n/Hidden\n\n    \nRunOnlyIfIdle\nfalse\n/RunOnlyIfIdle\n\n    \nWakeToRun\nfalse\n/WakeToRun\n\n    \nExecutionTimeLimit\nPT1H\n/ExecutionTimeLimit\n\n    \nPriority\n7\n/Priority\n\n  \n/Settings\n\n  \nActions Context=\"Author\"\n\n    \nExec\n\n      \nCommand\nC:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\n/Command\n\n      \nArguments\n-File \"C:\\ssh-fail2\\slc-tie-breaker.ps1\" FrontEndA -ExecutionPolicy RemoteSigned -noprofile -noninteractive\n/Arguments\n\n      \nWorkingDirectory\nC:\\ssh-fail2\\\n/WorkingDirectory\n\n    \n/Exec\n\n  \n/Actions\n\n\n/Task\n\n\n\n\nPrimaryTest.sh...bash script locally executed on the Application Primary servers\n\n\n#!/bin/bash\n\n    if [ \"$1\" = \"Up\" ]; then\n                Is_App_Up=\"Primary_App_Is_Up\"\n    else\n                Is_App_Up=\"Primary_App_Is_Down\"\n    fi\necho $Is_App_Up\necho `date` \n /tmp/log\n\n\n\nConfirmPriDown.sh....bash script remotely connects to Application Primary Server and runs PrimaryTest.sh\n\n\n#!/bin/bash\n\nVIP=10.10.10.53\nP1=10.10.10.51\nP2=10.10.10.52\n\nif [[ \"$1\" = \"VIP\" ]]; then TEST=\"$VIP\"\n   elif [[ \"$1\" = \"P1\" ]]; then TEST=\"$P1\"\n   elif [[ \"$1\" = \"P2\" ]]; then TEST=\"$P2\"\nelse\necho \"Usage: you much define which node to test {VIP,P1,P2}\"\nexit 0\nfi\n\nif [[ `ssh $TEST -q -C /home/failover-user/custom_applicationfailover0.1/primary-test.sh Up` = \"Primary_App_Is_Up\"  ]]  ; then\n  echo Primary_App_Is_Up\nelse\n   echo test-failed\nfi\n\n\n#!/bin/bash\n\necho  \n /home/failover-user/custom_applicationfailover0.1/startlog\necho `date`  start \n /home/failover-user/custom_applicationfailover0.1/startlog\n\nif [ \"$1\" = \"Start\" ] ;\n   then\n  # Start_App=\"App_Started\"\n\n       echo `date` \" \" $0\" \" $1 \n /home/failover-user/custom_applicationfailover0.1/startlog\n        /usr/bin/sudo /etc/init.d/custom_application-heavy restart \n /home/failover-user/custom_applicationfailover0.1/startlog\n        echo \"sudo /etc/init.d/custom_application-heavy restart\" \n /home/failover-user/custom_applicationfailover0.1/startlog\n\n        if  [[ $? = \"0\"  ]]\n                then\n                 Start_App=\"App_Started\"\n                 echo $Start_App\n                else\n                 echo $0 Failed to restart app\n                 echo $Start_App\n\n        fi\n   else\n   echo `date` Usage $0 Start \n /home/failover-user/custom_applicationfailover0.1/startlog\nfi\n\necho `date`  finsihed \n /home/failover-user/custom_applicationfailover0.1/startlog\n\n#/usr/bin/sudo /etc/init.d/custom_application-heavy restart", 
            "title": "Homemade cluster"
        }, 
        {
            "location": "/linux/Clustering/homemade_cluster/#operating-systems", 
            "text": "Application servers are RHEL 5.9  Steward server is windows 2008", 
            "title": "Operating Systems"
        }, 
        {
            "location": "/linux/Clustering/homemade_cluster/#the-tie-breaker-server-requires", 
            "text": "powershell   dotnet   user account   scheduled task for each monitored cluster  powershell script  ssh wrapper script (powershell to dotnet) ssh.net library  log directory  public ssh key of the primary and the DR servers (to enable paswordlSERVER login)", 
            "title": "The tie-breaker server requires..."
        }, 
        {
            "location": "/linux/Clustering/homemade_cluster/#files", 
            "text": "Windows Steward server  id_rsa              ## ssh private key to allow loginto linux servers\nid_rsa.pub          ## ssh public key to allow loginto linux servers\nRenci.SshNet.dll    ## SSH.net library from http://sshnet.codeplex.com/ \nSSH-SSERVERions.psd1   ## powershell wrapper for SSH.net from http://www.powershelladmin.com/wiki/SSH_from_PowerShell_using_the_SSH.NET_library\nSSH-SSERVERions.psm1   ## licence file\nssh-fail.ps1        ## powershell script to login and checl custom_application servers from Seamus Murray\nschedule.xml        ## exported windows shedual task to run the script  Application Primary Server  PrimaryTest.sh      ## bash script locally execute on the Application Primary servers from Seamus Murray  Application DR Server  ConfirmPriDown.sh   ## remotely connects to Application Primary Server and runs PrimaryTest.sh from Seamus Murray\nMountStart.sh       ## Starts the Application application\nBreak-n-Mount.sh    ## only on the index servers remotely connects to netapp controller, breaks the mirror, maps the lun and mounts it  NetApp controller  id_rsa.pub          ## ssh public key of SnapMirror-user on the 2 custom_application index DR servers   Diagrams", 
            "title": "Files"
        }, 
        {
            "location": "/linux/Clustering/homemade_cluster/#script-contents", 
            "text": "ssh-fail.ps1  #ssh-fail.ps1        \n#powershell script to login and check custom_application servers from Seamus Murray\n\nParam(\n  [string]$cluster  \n)\nImport-Module  SSH-SSERVERions\n\n\n#Start-Transcript -path C:\\ssh-fail2\\logs\\transcript.txt -append\n\n\n##set dubug level\n\n#$debug=1\n#if ( $debug -eq 1 ) {\n#               # Debug log file\n#               $deboutfile=$scriptdir+\"\\outputlog.\"+$siteid+\".log\"\n#               start-transcript -path $deboutfile -force\n#               # Shows a trace of each line being run with variables as variables\n#               set-psdebug -trace 1\n#\n#}\n\nif ( $cluster -eq 'Indexer' -Or  $cluster -eq 'DataBase' -Or $cluster -eq 'FrontEndA' -Or $cluster -eq 'FrontEndB')\n{\n\n}\nelse{\nWrite-Host \"you must specify the cluster to test using an argument\"\nbreak\n}\n\n##           IP                    Function             Hostname          Role    \nSwitch ($cluster)\n{\nFrontEndA {\n$server_P1='10.10.10.51' #a    Application FrontEnd     SERVERFW1TS    QLD-APP-5\n$server_P2='10.10.10.52' #a    Application FrontEnd     SERVERFW2TS    QLD-APP-6\n$server_VIP='10.10.10.53'#a    Application FrontEnd     VIPA\n$server_DR='10.10.11.32' #a    Application FrontEnd     SERVERFW3TS    SYD-APP-3\n}\nFrontEndB{\n$server_P1='10.10.10.55' #b    Application FrontEnd     SERVERFW3TS    QLD-APP-7\n$server_P2='10.10.10.56' #b    Application FrontEnd     SERVERFW4TS    QLD-APP-8\n$server_VIP='10.10.10.57'#b    Application FrontEnd     VIPB \n$server_DR='10.10.11.33' #b    Application FrontEnd     SERVERFW2TS    SYD-APP-4\n}\nIndexer{\n$server_P1='10.10.10.77' #c    Application Indexer      SERVERIX1TS    QLD-APP-1\n$server_P2='10.10.10.78' #c    Application Indexer      SERVERIX2TS    QLD-APP-2\n$server_VIP='10.10.10.76'#c    Application Indexer      VIPI\n$server_DR='10.10.11.17' #c    Application Indexer      SERVERIX3TS    SYD-APP-1\n}\nDataBase{\n$server_P1='10.10.10.85' #d    Application DataBase     SERVERHD1TS    QLD-APP-3\n$server_P2='10.10.10.86' #d    Application DataBase     SERVERHD2TS    QLD-APP-4\n$server_VIP='10.10.10.84'#d    Application DataBase     VIPD\n$server_DR='10.10.11.18' #d    Application DataBase     SERVERHD3TS    SYD-APP-2\n}\n}\n\n##set log file to local directory  eg..2012-01-1_0000_10.10.10.51_custom_applicationfail\n$Logfile = \"c:\\ssh-fail2\\logs\\$(get-date -uformat %Y-%m-%d_%H%M)\"+\"_$server_P1\"+\"_custom_applicationfail.log\"\n\n#Write_Host $Logfile\n#LogWrite  $env:PSModulePath\nFunction LogWrite\n{\n   Param ([string]$logstring)\n\n   Add-content $Logfile -value $logstring\n}\n\n$stamped  = \"$(Get-Date)\" + \" starting script \"\nLogWrite $stamped\n\n\n\n\n\n#Remote Scripts executed on the linux servers but called from this script\n#You must specify the argument \"Up\" case sensitive for this test to succeed \n#If you want to simulate this test failing just change the argument\n$ApplicationPrimaryTest='/home/failover-user/custom_applicationfailover0.1/primary-test.sh Up'\n\n#Specifiy which server the DR should test by assigning a single argument....P1 P2 or VIP\n#the Various IPs are stored both locally in this file and in   ApplicationConfirmPriDown.sh on the respective DR servers\n#If you want to simulate this test failing just change the argument to something else\n$ApplicationConfirmPriDown='/home/failover-user/custom_applicationfailover0.1/primary-confirm-fail.sh VIP'\n\n#this command needs to execute the start up script via sudo this either requires a tty which \"SSH-SSERVERions\" doesn't provide or..editing sudo to disable the requiretty in /etc/sudoers\n#This script varies slightly between the Application FrontEnds and the Application Indexers\n#On the Application indexers the NetApp mirrored lun's need to be broken and mounted this is handled by the..\n#Break-n-Mount.sh script called from within the ApplicationMountStart.sh executed from the DR servers\n$ApplicationMountStart='/home/failover-user/custom_applicationfailover0.1/initiate-dr.sh Start'\n\n#called from within $ApplicationMountStart on the Indexer DR servers\n#$Break-n-Mount='/home/failover-user/custom_applicationfailover0.1/break-snap-mirror.sh RESYNC'\n\n\n\n\n\nwhile(\"forever\")\n{\n    New-SshSSERVERion -ComputerName $server_P1 -Username 'failover-user'  -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null \n    New-SshSSERVERion -ComputerName $server_DR -Username 'failover-user'  -KeyFile 'C:\\ssh-fail2\\id_rsa' # | out-null\n\n\ntry\n{\n #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 1st loop\"\n $stamped = \"$(Get-Date)\" + \" Testing ApplicationPrimaryTest on $server_P1 1st loop\"\n LogWrite  $stamped\n $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet\n}\ncatch [Exception]\n{ \n  $CmdOutput1 = \"SSH_SSERVERION_FAILED\" \n  #Write_Host\"ERROR: $CmdOutput1 during 1st loop\" -foregroundcolor white -backgroundcolor red\n  $stamped  = \"$(Get-Date)\" + \" ERROR: $CmdOutput1 during 1st loop\"\n  LogWrite $stamped\n}\n\n\n    #Check Primary Server for status\n    if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { \n        #Write_Host \"ERROR: Primary Failure Detected\"\n        #Write_Host \"Waiting 10 seconds before retrying\"\n        $stamped =  \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up   Waiting 10 seconds before retrying\"        \n        LogWrite $stamped\n        sleep 10\n\n\n        try\n       {\n           #Write_Host \"Testing ApplicationPrimaryTest on $server_P1 2nd loop\"\n           $CmdOutput1 = Invoke-SshCommand -ComputerName $server_P1 -Command $ApplicationPrimaryTest -Quiet\n       }\n       catch [Exception]\n           {\n             $CmdOutput1 = \"SSH_SSERVERION_FAILED\"\n             #Write_Host\"$CmdOutput1 during 2nd loop\" -foregroundcolor white -backgroundcolor red\n             $stamped =  \"$(Get-Date)\" + \" $CmdOutput1 during 2nd loop\"\n             LogWrite $stamped\n       }\n\n                  #Check Primary Server for status after a previous failure\n                  if ( $CmdOutput1 -ne 'Primary_App_Is_Up' ) { \n                  #Write_Host \"ERROR: Primary Failure Detected 2 times\"\n                $stamped =  \"$(Get-Date)\" + \" ERROR: Check Primary did not return Primary_App_Is_Up after 2 tries\"\n                LogWrite $stamped\n\n           try\n              {\n                  $CmdOutput2 = Invoke-SshCommand -ComputerName $server3 -Command $ApplicationPrimaryTest -Quiet\n              }\n              catch [Exception]\n                  {\n                  #Write_Host \"ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\"\n               $stamped =  \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_P1 has failed. Unable to execute ApplicationPrimaryTest\"\n               LogWrite $stamped\n              }\n\n                    #If this host fails 2 time to determine if Primary_App_Is_Up, then ask DR server to also run the check          \n                    if ( $CmdOutput2 -ne 'Primary_App_Is_Up' ) {     \n                    #Write_Host \"ERROR: DR Server $server3 is also reporting Primary Failure....... Need to Initiate DR\"\n                    $stamped =  \"$(Get-Date)\" + \" ERROR: DR Server is also reporting Primary Failure....... Need to Initiate DR\"\n                    LogWrite $stamped\n\n\n                try\n            {\n             $CmdOutput3 = Invoke-SshCommand -ComputerName $server_DR -Command $ApplicationMountStart  -Quiet\n            }\n            catch [Exception]\n            {\n            $CmdOutput3 = \"SSH_SSERVERION_FAILED\"\n              #Write_Host \"ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\"\n            $stamped =  \"$(Get-Date)\" + \" ERROR:ssh sSERVERion to $server_DR has failed. Unable to execute ApplicationMountStart\"\n            LogWrite $stamped\n\n                        }\n\n\n                if ( $CmdOutput3 -ne 'App_Started' ) {     \n                #Write_Host \"ERROR: DR server $server_DR Failed to start the App\"\n                $stamped =  \"$(Get-Date)\" + \" ERROR: DR server $server_DR Failed to start the App\"\n                LogWrite $stamped\n\n\n                }\n                else {\n                   #Write_Host \"DR server $server_DR has started the App\"\n                   $stamped =  \"$(Get-Date)\" + \" DR server $server_DR has started the App\"\n                   LogWrite $stamped\n                   $stamped =  \"$(Get-Date)\" + \" Nothing else to do...Failover script self terminating\"\n                   LogWrite $stamped\n                   break\n                }\n\n        }\n        else {\n            #Write_Host \"DR server $server_DR is reporting Primary $server_P1 is OK: Nothing To Do\"\n            #Write_Host \"Assuming the link between me and the Primary server has failed\"\n            $stamped =  \"$(Get-Date)\" + \" Assuming the link between me and the Primary server has failed\"\n            LogWrite $stamped\n        } \n    }\n              else {\n                #Write_Host \"Primary server $server_P1 is OK: Nothing To Do             2nd test\"   \n                $stamped =  \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do             2nd test\" \n                LogWrite $stamped\n                   } \n    }\n    else {\n        #Write_Host \"Primary server $server_P1 is OK: Nothing To Do             1st test\"   \n        $stamped =  \"$(Get-Date)\" + \" Primary server $server_P1 is OK: Nothing To Do             1st test\" \n        LogWrite $stamped\n    }\n\n    sleep 5\n\n}\n\nRemove-SshSSERVERion -RemoveAll\n\n#Stop-Transcript  schedule.xml  #schedule.xml\n#exported windows shedual task to run the script ?xml version=\"1.0\" encoding=\"UTF-16\"?  Task version=\"1.2\" xmlns=\"http://schemas.microsoft.com/windows/2004/02/mit/task\" \n   RegistrationInfo \n     Date 2012-01-1T12:30:10 /Date \n     Author ABCDEFG123\\seamus /Author \n   /RegistrationInfo \n   Triggers \n     RegistrationTrigger \n       Repetition \n         Interval PT15M /Interval \n         StopAtDurationEnd false /StopAtDurationEnd \n       /Repetition \n       ExecutionTimeLimit PT1H /ExecutionTimeLimit \n       Enabled true /Enabled \n     /RegistrationTrigger \n   /Triggers \n   Principals \n     Principal id=\"Author\" \n       UserId ABCDEFG123\\Administrator /UserId \n       LogonType Password /LogonType \n       RunLevel HighestAvailable /RunLevel \n     /Principal \n   /Principals \n   Settings \n     MultipleInstancesPolicy StopExisting /MultipleInstancesPolicy \n     DisallowStartIfOnBatteries false /DisallowStartIfOnBatteries \n     StopIfGoingOnBatteries true /StopIfGoingOnBatteries \n     AllowHardTerminate true /AllowHardTerminate \n     StartWhenAvailable true /StartWhenAvailable \n     RunOnlyIfNetworkAvailable false /RunOnlyIfNetworkAvailable \n     IdleSettings \n       StopOnIdleEnd true /StopOnIdleEnd \n       RestartOnIdle false /RestartOnIdle \n     /IdleSettings \n     AllowStartOnDemand true /AllowStartOnDemand \n     Enabled true /Enabled \n     Hidden false /Hidden \n     RunOnlyIfIdle false /RunOnlyIfIdle \n     WakeToRun false /WakeToRun \n     ExecutionTimeLimit PT1H /ExecutionTimeLimit \n     Priority 7 /Priority \n   /Settings \n   Actions Context=\"Author\" \n     Exec \n       Command C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe /Command \n       Arguments -File \"C:\\ssh-fail2\\slc-tie-breaker.ps1\" FrontEndA -ExecutionPolicy RemoteSigned -noprofile -noninteractive /Arguments \n       WorkingDirectory C:\\ssh-fail2\\ /WorkingDirectory \n     /Exec \n   /Actions  /Task   PrimaryTest.sh...bash script locally executed on the Application Primary servers  #!/bin/bash\n\n    if [ \"$1\" = \"Up\" ]; then\n                Is_App_Up=\"Primary_App_Is_Up\"\n    else\n                Is_App_Up=\"Primary_App_Is_Down\"\n    fi\necho $Is_App_Up\necho `date`   /tmp/log  ConfirmPriDown.sh....bash script remotely connects to Application Primary Server and runs PrimaryTest.sh  #!/bin/bash\n\nVIP=10.10.10.53\nP1=10.10.10.51\nP2=10.10.10.52\n\nif [[ \"$1\" = \"VIP\" ]]; then TEST=\"$VIP\"\n   elif [[ \"$1\" = \"P1\" ]]; then TEST=\"$P1\"\n   elif [[ \"$1\" = \"P2\" ]]; then TEST=\"$P2\"\nelse\necho \"Usage: you much define which node to test {VIP,P1,P2}\"\nexit 0\nfi\n\nif [[ `ssh $TEST -q -C /home/failover-user/custom_applicationfailover0.1/primary-test.sh Up` = \"Primary_App_Is_Up\"  ]]  ; then\n  echo Primary_App_Is_Up\nelse\n   echo test-failed\nfi\n\n\n#!/bin/bash\n\necho    /home/failover-user/custom_applicationfailover0.1/startlog\necho `date`  start   /home/failover-user/custom_applicationfailover0.1/startlog\n\nif [ \"$1\" = \"Start\" ] ;\n   then\n  # Start_App=\"App_Started\"\n\n       echo `date` \" \" $0\" \" $1   /home/failover-user/custom_applicationfailover0.1/startlog\n        /usr/bin/sudo /etc/init.d/custom_application-heavy restart   /home/failover-user/custom_applicationfailover0.1/startlog\n        echo \"sudo /etc/init.d/custom_application-heavy restart\"   /home/failover-user/custom_applicationfailover0.1/startlog\n\n        if  [[ $? = \"0\"  ]]\n                then\n                 Start_App=\"App_Started\"\n                 echo $Start_App\n                else\n                 echo $0 Failed to restart app\n                 echo $Start_App\n\n        fi\n   else\n   echo `date` Usage $0 Start   /home/failover-user/custom_applicationfailover0.1/startlog\nfi\n\necho `date`  finsihed   /home/failover-user/custom_applicationfailover0.1/startlog\n\n#/usr/bin/sudo /etc/init.d/custom_application-heavy restart", 
            "title": "Script contents................."
        }, 
        {
            "location": "/linux/Clustering/simple.cluster.conf/", 
            "text": "?xml version=\"1.0\"?\n\n    \ncluster alias=\"cluster1\" config_version=\"1\" name=\"cluster1\"\n\n            \nfence_daemon clean_start=\"0\" post_fail_delay=\"0\" post_join_delay=\"3\"/\n\n            \nclusternodes\n\n                    \nclusternode name=\"hostname1.example.private\" nodeid=\"1\" votes=\"1\"\n\n                            \nfence\n\n                                    \nmethod name=\"1\"\n\n                                            \ndevice name=\"ipmi-hostname1\"/\n\n                                    \n/method\n\n                            \n/fence\n\n                    \n/clusternode\n\n                    \nclusternode name=\"hostname2.example.private\" nodeid=\"2\" votes=\"1\"\n\n                            \nfence\n\n                                    \nmethod name=\"1\"\n\n                                            \ndevice name=\"ipmi-hostname2\"/\n\n                                    \n/method\n\n                            \n/fence\n\n                    \n/clusternode\n\n            \n/clusternodes\n\n            \ncman expected_votes=\"1\" two_node=\"1\"\n\n                    \nmulticast addr=\"239.192.0.1\"/\n\n            \n/cman\n\n            \nfencedevices\n\n                    \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.1\" login=\"ucsipmi_pmpricen\" name=\"ipmi-hostname1\" passwd=\"password\" delay=\"30\"/\n\n                    \nfencedevice agent=\"fence_ipmilan\" auth=\"password\" ipaddr=\"192.168.1.2\" login=\"ucsipmi_pmpricen\" name=\"ipmi-hostname2\" passwd=\"password\"/\n\n            \n/fencedevices\n\n            \nrm\n\n                    \nfailoverdomains\n\n                            \nfailoverdomain name=\"FailDomain\" ordered=\"1\" restricted=\"1\"\n\n                                    \nfailoverdomainnode name=\"hostname1.example.private\" priority=\"1\"/\n\n                                    \nfailoverdomainnode name=\"hostname2.example.private\" priority=\"2\"/\n\n                            \n/failoverdomain\n\n                    \n/failoverdomains\n\n                    \nresources\n\n                    \n/resources\n\n                    \nservice autostart=\"1\" domain=\"FailDomain\" exclusive=\"0\" name=\"servicel\"\n\n                    \n/service\n\n            \n/rm\n\n    \n/cluster", 
            "title": "Simple.cluster.conf"
        }, 
        {
            "location": "/linux/Time/Adventures_in_the_land_of_symmetric_ntp_authentication/", 
            "text": "To configure symmetric key NTP authentication for the RedHat NTP client, the following 2 files need to be configured.\n\n\n/etc/ntp.conf\n\n\n#symmetric key authentication\nrestrict -6 default kod nomodify nopeer notrap noquery\nrestrict 127.0.0.1\nrestrict -6 ::1\ntinker panic 0\ndriftfile /var/lib/ntp/drift\nkeys /etc/ntp/keys\nserver 10.10.10.1 version 2 key 2\nserver 10.0.10.1 version 2 key 2\ntrustedkey 2\nrequestkey 2\ncontrolkey 2\n\n\n\n/etc/ntp/keys\n\n\n#key_number key_type Key\n2           M        !@#$%^\n*()\n\n\n\nUnfortunately the default logging level and debug mode of the redhat ntp client does not output any useful messages to syslog regarding symmetric key authentication.\nTherefore to fault find you will have to use a combination of... \n\n\n\n\nntpq -p  ## print list of peers and their state\n\n\nntpq -c as ## print a list of association identifiers and peer statuses\n\n\nntpd in debug mode  ## probably best to just ignore this option or anything else in syslog besides the message indicating a successful sync\n\n\n\n\nExample of outputs when everything is working\n\n\nWhen using the correct key, after starting the ntpd you should immediately see the output for\n\n\n[root@linuxserver1 ~]# ntpq -c as\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 16777  f014   yes   yes   ok     reject   reachable  1\n  2 16778  f014   yes   yes   ok     reject   reachable  1\n\n\n\n\n\nreach = yes\n\n\nauth = ok\n\n\ncondition = reject  \n\n\nlast_event = reachable \n\n\n\n\nThe ntp -q wont show an asterix (sys.peer) until the client has had enough time to validate the stability of the ntp server and sync its clock\n\n\n[root@linuxserver1 ~]# ntpq -p\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. 10.10.0.1     2 u   17   64    1    0.622    0.502   0.001\n router2. 10.11.0.1     2 u   16   64    1   16.264   -0.395   0.001\n\n\n\nAfter a few minutes (depending on the time disparity) the clocks should sync and the outputs should be...\n\n\n[root@linuxserver1 ~]# ntpq -c as\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 16777  f414   yes   yes   ok   candidat   reachable  1\n  2 16778  f614   yes   yes   ok   sys.peer   reachable  1\n\n  [root@linuxserver1 ~]# ntpq -p\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n+router1. 10.10.0.1     2 u   56   64  377    0.615    0.767   0.127\n*router2. 10.11.0.1     2 u   59   64  377   15.865   -0.095   0.134\n\n\n\nWhen the client has been synchronized you should see a message similar to the following\n\n\nsyslog message\nAug  5 12:22:53 linuxserver1 ntpd[32065]: synchronized to 10.10.10.1, stratum 2\n\n\n\n\n\nPossible error scenarios\n\n\n1. Specifying a key number that doesn't match on the ntp server,\n\n\neg... client specifies key number 1  server has key number 2\n\n\n[root@linuxserver1 ~]# ntpq -p\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. .AUTH.          16 u    -   64    0    0.000    0.000   0.000\n router2. .AUTH.          16 u    -   64    0    0.000    0.000   0.000\n\n [root@linuxserver1 ~]# ntpq -c as\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 62393  c000   yes   yes   bad    reject\n  2 62394  c000   yes   yes   bad    reject\n\nsyslog message\nAug  8 11:30:30 linuxserver1 ntpd[20387]: transmit: 10.10.0.1  key 1 not found\n\n\n\n2. When using an incorrect or corrupted key (eg...if using a script to deploy the key have you escaped the extended ascii values in the key)\n\n\n[root@linuxserver1 ~]# ntpq -p\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. .AUTH.          16 u    -   64    0    0.000    0.000   0.001\n router2. .AUTH.          16 u    -   64    0    0.000    0.000   0.001\n\n [root@linuxserver1 ~]# ntpq -c as\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 62393  c000   yes   yes   bad    reject\n  2 62394  c000   yes   yes   bad    reject\n\n~~Note there is no indication in /var/log/messages that the key is bad~~\n\n\n\n3. Specifying an NTP server device IP when that device dose not have symmetric authentication turned on\n\n\n~~ the client authenticates even though it should not ~~\n\nAug  9 13:55:17 linuxserver1 ntpd[4101]: synchronized to router3, stratum 2\n\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n+router3. 10.10.0.1     2 u    2   64  377    0.566   -1.045   1.085\n*router4. 10.11.0.1     2 u    6   64  377   15.751    1.584   1.122\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 61738  f414   yes   yes   ok   candidat   reachable  1\n  2 61739  f614   yes   yes   ok   sys.peer   reachable  1\n~~ it appears that the device may authenticate to the local router that is using symmetric authentication to its upstream device\n\n\n\n4. When specifying a windows domain controller that does not support symmetric key authentication\n\n\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. .INIT.          16 u    -   64    0    0.000    0.000   0.000\n router2. .INIT.          16 u    -   64    0    0.000    0.000   0.000\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 30681  c000   yes   yes   bad    reject\n  2 30682  c000   yes   yes   bad    reject\n\n\n\n5. Specifying the incorrect IP or the IP of a non reachable NTP device\n\n\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n 100.80.88.255   .INIT.          16 u    -   64    0    0.000    0.000   0.000\n 100.80.88.254   .INIT.          16 u    -   64    0    0.000    0.000   0.000\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 47581  c000   yes   yes   bad    reject\n  2 47582  c000   yes   yes   bad    reject", 
            "title": "Adventures in the land of symmetric ntp authentication"
        }, 
        {
            "location": "/linux/Time/Adventures_in_the_land_of_symmetric_ntp_authentication/#possible-error-scenarios", 
            "text": "1. Specifying a key number that doesn't match on the ntp server,  eg... client specifies key number 1  server has key number 2  [root@linuxserver1 ~]# ntpq -p\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. .AUTH.          16 u    -   64    0    0.000    0.000   0.000\n router2. .AUTH.          16 u    -   64    0    0.000    0.000   0.000\n\n [root@linuxserver1 ~]# ntpq -c as\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 62393  c000   yes   yes   bad    reject\n  2 62394  c000   yes   yes   bad    reject\n\nsyslog message\nAug  8 11:30:30 linuxserver1 ntpd[20387]: transmit: 10.10.0.1  key 1 not found  2. When using an incorrect or corrupted key (eg...if using a script to deploy the key have you escaped the extended ascii values in the key)  [root@linuxserver1 ~]# ntpq -p\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. .AUTH.          16 u    -   64    0    0.000    0.000   0.001\n router2. .AUTH.          16 u    -   64    0    0.000    0.000   0.001\n\n [root@linuxserver1 ~]# ntpq -c as\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 62393  c000   yes   yes   bad    reject\n  2 62394  c000   yes   yes   bad    reject\n\n~~Note there is no indication in /var/log/messages that the key is bad~~  3. Specifying an NTP server device IP when that device dose not have symmetric authentication turned on  ~~ the client authenticates even though it should not ~~\n\nAug  9 13:55:17 linuxserver1 ntpd[4101]: synchronized to router3, stratum 2\n\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n+router3. 10.10.0.1     2 u    2   64  377    0.566   -1.045   1.085\n*router4. 10.11.0.1     2 u    6   64  377   15.751    1.584   1.122\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 61738  f414   yes   yes   ok   candidat   reachable  1\n  2 61739  f614   yes   yes   ok   sys.peer   reachable  1\n~~ it appears that the device may authenticate to the local router that is using symmetric authentication to its upstream device  4. When specifying a windows domain controller that does not support symmetric key authentication       remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n router1. .INIT.          16 u    -   64    0    0.000    0.000   0.000\n router2. .INIT.          16 u    -   64    0    0.000    0.000   0.000\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 30681  c000   yes   yes   bad    reject\n  2 30682  c000   yes   yes   bad    reject  5. Specifying the incorrect IP or the IP of a non reachable NTP device       remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n 100.80.88.255   .INIT.          16 u    -   64    0    0.000    0.000   0.000\n 100.80.88.254   .INIT.          16 u    -   64    0    0.000    0.000   0.000\n\nind assID status  conf reach auth condition  last_event cnt\n===========================================================\n  1 47581  c000   yes   yes   bad    reject\n  2 47582  c000   yes   yes   bad    reject", 
            "title": "Possible error scenarios"
        }, 
        {
            "location": "/linux/Time/Setting_time/", 
            "text": "ntpdate time.nist.gov\nhwclock -w\n\n\n\nIf you dont have an NTP service available you can set  the clock based on the current time on another server\n\n\ndate +%Y%m%d%T -s \"`ssh user@remoteserver 'date \"+%Y%m%d%T\"'`\"", 
            "title": "Setting time"
        }, 
        {
            "location": "/linux/Time/linux_clocksource_benchmark-2.c/", 
            "text": "A short program written in C and in-line X86 assembler for experimenting the the various clocksources available on an Intel X86 system.\n\n\nSample execution.. \n\n\nInital ClockSource setting is : tsc \nAvailable ClockSources : tsc hpet acpi_pm\n+--------------------------------------------------------------------------------------------------+\n|ClockSrc|Exec time | Start  | Finish  |RDSTC start |RDTSC finish| RDTSCP start   | RDTSCP finish  |\n+--------+----------+--------+---------+------------+------------+----------------+----------------+\n|tsc     | 0.032044 | 1032   | 33076   | 1159606212 | 1269023424 | 1856585477990  | 1856694895364  |\n|hpet    | 0.596099 | 33218  | 629317  | 1269654044 | 3304412637 | 1856695525864  | 1858730284571  |\n|acpi_pm | 0.734672 | 629399 | 1364071 | 3304881771 | 1517682755 | 1858730753603  | 1861238521977  |\n+--------------------------------------------------------------------------------------------------+\n\n\n\nSource Code..\n\n\n#include \nstdio.h\n\n#include \nsys/time.h\n\n#include \ntime.h\n\n#include \nstdlib.h\n\n#include \nstdint.h\n\n\n\n\nunsigned long get_rdtscp(int *chip, int *core);\nstatic unsigned long long get_rdtsc();\n\n\n\nint main()\n\n{\n//array of strings for the clocksource names\nchar clocksource[4][15];\nclocksource[0][0] = '\\0';\nclocksource[1][0] = '\\0';\nclocksource[2][0] = '\\0';\nclocksource[3][0] = '\\0';\nclocksource[4][0] = '\\0';\nchar initial_clocksource[15] = {'\\0'};\nstruct timeval mytime;\nstruct timezone mytimezone;\nclock_t start, end;\nint x = 0;\nint y = 0;\n\n//special numbers to store the values retrieved from the TSC calls\nuint64_t starting_tsc = 0;\nuint64_t finishing_tsc = 0;\nuint64_t starting_tscp = 0;\nuint64_t finishing_tscp = 0;\n\n//cpu values to send the rdtscp command to the same cpu core\nint my_chip = 0;\nint my_core = 3;\nint * chip = \nmy_chip;\nint * core = \nmy_core;\n\n//saving the initial clocksource value so we can set it back when we are finished\nFILE *get_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"r\");\nfscanf(get_clocksource, \"%s\", initial_clocksource); //saving initial value \nfclose(get_clocksource);\nprintf(\"Initial ClockSource setting is : %s \\n\",initial_clocksource);\n\n//test if we have the permissions to change the current ClockSource\n      FILE *current_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\");\n      if (current_clocksource == NULL) {\n          perror(\"failed to open /sys/devices/system/clocksource/clocksource0/current_clocksource for writing\"\n                 \"\\n are you running as root or via sudo ?\\n\");\n          exit(1);\n          }\n      fclose(current_clocksource);\n\n\n//reading in all the available clocksource options, this seems to vary between 2 and 4 values \nFILE *available_clocksources  = fopen(\"/sys/devices/system/clocksource/clocksource0/available_clocksource\", \"r\");\n\n      if (available_clocksources == NULL) {\n          perror(\"failed to open /sys/devices/system/clocksource/clocksource0/available_clocksource for reading\"\n                 \"\\n are you running as root ?\\n\");\n          exit(1);\n          }\n       else {\n         fscanf(available_clocksources, \"%s %s %s %s %s\", clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]);\n         printf(\"Available ClockSources : %s %s %s %s %s\\n\",clocksource[0], clocksource[1], clocksource[2], clocksource[3], clocksource[4]);\n            }\n\n\nprintf(\"+----------------------------------------------------------------------------------------------------------+\\n\");\nprintf(\"|ClockSrc|Exec time | Start  | Finish  | RDSTC start    |  RDTSC finish  | RDTSCP start   | RDTSCP finish  |\\n\");\nprintf(\"+--------+----------+--------+---------+----------------+----------------+----------------+----------------+\\n\");\nwhile ( (x \n= 4 ) \n (clocksource[x][0] != '\\0') ){ //iterate through the available clocksource values and set them as the current value\n      FILE *current_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\");\n      if (current_clocksource == NULL) {\n          perror(\"failed to open /sys/devices/system/clocksource/clocksource0/current_clocksource for writing\"\n                 \"\\n are you running as root ?\\n\");\n          exit(1);\n          }\n      else {\n         fprintf(current_clocksource,\"%s\", clocksource[x]);\n         fclose(current_clocksource);\n          }\n         y = 0 ;//execute the gettimeofday call \n         starting_tscp = get_rdtscp(chip,core);\n         starting_tsc = get_rdtsc(); \n         start = clock();\n         while ( y \n 1000000 ) {\n             gettimeofday(\nmytime, \nmytimezone);\n             y++;\n             }\n\n         end = clock();\n         finishing_tsc = get_rdtsc();\n         finishing_tscp = get_rdtscp(chip,core);\n         printf(\"|%-7s | %-7f | %-6ld | %-7ld | %ld | %ld | %ld | %ld |\\n\",clocksource[x],((double) (end - start)) / CLOCKS_PER_SEC,start,end,starting_tsc,finishing_tsc,starting_tscp,finishing_tscp);\n       x++;\n     }\n\n//setting the clocksource back to the initial value\nFILE *set_clocksource = fopen(\"/sys/devices/system/clocksource/clocksource0/current_clocksource\", \"w\");\nif (set_clocksource == NULL) {\n          perror(\"failed to open /sys/devices/system/clocksource/clocksource0/current_clocksource for writing\"\n                 \"\\n are you running as root ?\\n\");\n          exit(1);\n          }\n       else {\n              fprintf(set_clocksource,\"%s\", initial_clocksource); //setting clocksource back to  initial value\n              fclose(set_clocksource);\n            }\n\nprintf(\"+----------------------------------------------------------------------------------------------------------+\\n\");\nreturn 0;\n\n}\n\n//2 functions to retrieve the counter values from the TSC\n\n\n//this function just calls which ever TSC that the kernel or cpu assigns, i.e it varies\nstatic __inline__ unsigned long long get_rdtsc(void)\n{\n    unsigned tsc_val_lo, tsc_val_hi;\n    __asm__ __volatile__ (\"rdtsc\" : \"=a\"(tsc_val_lo), \"=d\"(tsc_val_hi));\n    return ( (unsigned long long)tsc_val_lo)|( ((unsigned long long)tsc_val_hi)\n32 );\n}\n\n//with this function we can hard set the CPU and core\nunsigned long get_rdtscp(int *chip, int *core)\n{\n    unsigned a, d, c;\n    __asm__ volatile(\"rdtscp\" : \"=a\" (a), \"=d\" (d), \"=c\" (c));\n\n    *chip = (c \n 0xFFF000)\n12;\n\n    *core = c \n 0xFFF;\n    return ((unsigned long)a) | (((unsigned long)d) \n 32);;\n\n}", 
            "title": "Linux clocksource benchmark 2.c"
        }, 
        {
            "location": "/linux/RedHat/kickstart/", 
            "text": "#  kickstart file created by Seamus Nov 29 2006\n\n\n#  The following tasks are AutoMagicly configured with this script\n\n\n\n#  The hard disks will be zeroed but you will be prompted for a partition scheme\n#  User and Group accounts for the unix team will be added\n#  The Unix team will be granted FULL Sudo access\n#  The Message Of The Day will be set\n#  the (CTRL/ALT/DEL) key combination will be disabled on the physical console \n#  Xinetd will be disabled (wont start up automatically)\n#  Various Services will be disabled (wont start up automatically)\n#  Various default redhat user accounts will be deleted\n#  Remote loggin to the root account via SSH will be disabled\n#  The ntpdaemon will be configured started and synced\n#  The mondo packages and configuration files will be installed\n#  The nagios agents will be installed\n#  The strange label created by the installer for the swap partition is changed in the fstab file\n#  A custom grub splash screen is copied\n\n\n\n#  There is a separate file for configuring host for oracle\n\n# dont forget that during the post install script phase you can switch to virtual terminal 3 and 4 \n# and see what the script is doing. This is why there are so many echo's in this file\n\n\n\n\n\n#System  language\nlang en_AU\n\n#Language modules to install\nlangsupport en_AU\n\n#System keyboard\nkeyboard us\n\n#System mouse\nmouse\n\n#Sytem timezone\ntimezone Moon/Crater\n\n#Root password\nrootpw --iscrypted xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n#Reboot after installation\nreboot\n\n#Use text mode install\ntext\n\n#Install OS instead of upgrade\ninstall\n\n#Use NFS installation Media\n\nnfs --server=xxx.xxx.com  --dir=/u1/Distros/rhes4u3/install\n\n#System bootloader configuration\nbootloader --location=mbr\n\n#Clear the Master Boot Record\nzerombr yes\n\n\n# the following line will cause KickStart to ignor SAN disks during the OS install\nignoredisk --drives=sda,sdb,sdc,sdd,sde,sdf,sdg,sdh\n\n\n#Partition clearing information\nclearpart --all --initlabel \npart / --fstype ext3 --size 20480 --asprimary \npart /boot --fstype ext3 --size 150 --asprimary \n#part swap --size 4096\n#part swap --size 8192\npart swap --size 5120\npart /var --fstype ext3 --size 5120\npart /opt --fstype ext3 --size 25600\npart /usr --fstype ext3 --size 5120 --asprimary \npart /tmp --fstype ext3 --size 5120\npart /backup  --fstype ext3 --size 1 --grow\n\n#System authorization infomation\nauth  --useshadow  --enablemd5\n\n#Network information\nnetwork --bootproto=dhcp --device=eth0\n\n#Firewall configuration\nfirewall --disabled\n\n#Do not configure XWindows\nskipx\n\n#Package install information\n%packages --resolvedeps\n@ base-x\n@ text-internet\n#@ ftp-server\n#@ web-server\n#@ development-tools\n#@ admin-tools\n#@ system-tools\nkernel-smp\n-system-config-httpd\n-webalizer\nlvm2\ngrub\n-postfix\n-squid\n-spamassassin\n-cadaver\n-fetchmail\ne2fsprogs\nmkisofs\nbusybox\ncdrecord\n\n#oracle needs the following\nbinutils\ncompat-db\ncontrol-center\ngcc\ngcc-c++\nglibc\nglibc-common\ngnome-libs\nlibstdc++\nlibstdc++-devel\nmake\npdksh\nsysstat\nxscreensaver\nlibaio\n\n# docuemntum needs the following\n#compat-libstdc++-296\n#compat-libstdc++-33\ncompat-gcc-32\ncompat-gcc-32-c++\n\n\n\n########### POST CONFIGURATIONS BEGIN AT THIS POINT ##############\n\n\n%post\n#\n\necho \"setting time\"\nntpdate ntp.ntp.com\n\n\n\necho \"# disable the startup of unnecessary services\"\n\nchkconfig microcode_ctl off\nchkconfig netfs off\nchkconfig saslauthd off\nchkconfig mdmonitor off\nchkconfig mdmpd off\nchkconfig irda off\nchkconfig psacct off\nchkconfig isdn off\nchkconfig pcmcia off\nchkconfig autofs off\n#chkconfig portmap off\n#chkconfig nfs off\n#chkconfig nfslock off\nchkconfig cups off\nchkconfig dc_client off\nchkconfig arptables_jf off\nchkconfig dc_server off\nchkconfig bcm5820 off\nchkconfig squid off\nchkconfig named off\nchkconfig tux off\nchkconfig cups off\n\n\necho  \"removing unnecessary user accounts\"\n\nuserdel lp\nuserdel sync\nuserdel shutdown\nuserdel halt\nuserdel mail\nuserdel news\nuserdel uucp\nuserdel operator\nuserdel games\nuserdel gopher\nuserdel ftp\nuserdel nscd\nuserdel rpc\nuserdel rpcuser\nuserdel mailnull\nuserdel xfs\nuserdel gdm\nuserdel desktop\nuserdel squid\nuserdel named\n\nuserdel ldap\nuserdel netdump\n\necho \"#\"\necho \"# Disabling SSH ROOT login\"\necho \"#\"\ncp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak\nsed -e \"s/\\#PermitRootLogin/PermitRootLogin\\ no\\#/\" /etc/ssh/sshd_config.bak \n /etc/ssh/sshd_config\necho;echo\n\n\n\necho \"# Creating /etc/ntp.conf file...\"\n\ncp /etc/ntp.conf /etc/ntp.conf.bak\n\n\n\n\ncat \nEOF \n/etc/ntp.conf\n\n# this file was created by the XXXX Custom kickstart script\n# Permit time synchronization with our time source, but do not\n# permit the source to query or modify the service on this system.\nrestrict default nomodify notrap noquery\n\nrestrict 127.0.0.1\ndriftfile /var/lib/ntp/drift\nbroadcastdelay  0.008\nkeys            /etc/ntp/key\n\n# add Time servers\nserver 10.10.10.10\n\nEOF\n\n\n\necho \"# SYNC WITH NTP SERVER\"\nservice ntpd stop\nntpdate 10.10.10.10\n\necho \"# STARTING NTP SERVICE\"\n\n/etc/init.d/ntpd start\n\n\necho \"# Configuring Console access rights\"\n\ncat \nEOF \n/etc/default/console\nCONSOLE=/dev/console\nPASSREQ=YES\nPATH=/usr/bin:\nRETRIES=2\nSLEEPTIME=4\nSUPATH=/usr/sbin:/usr/bin:\nSYSLOG=YES\nSYSLOG_FAILED_LOGINS=2\nTIMEOUT=60\nUMASK=027\nEOF\n\n\necho \"# Configuring Password Parameters\"\n\ncat \nEOF \n/etc/default/passwd\nMAXWEEKS=5\nMINWEEKS=0\nPASSLENGTH=7\nEOF\n\n\n\ntouch /.rhosts /.netrc /etc/hosts.equiv\nchmod 0 /.rhosts /.netrc /etc/hosts.equiv\n\n\necho \"# Setup admin user accounts and passwords\"\n\n\ngroupadd -g 2000 UNIX\nuseradd XXXXX -u 99999 -c \"XXXXX XXXXX \" -p 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx' -d /home/XXXXX\n\n\n\n\n\necho \"# Creating SUDO permissions for UNIX team...\"\n\ncat \nEOF \n/etc/sudoers\n\n# sudoers file.\n#\n# This file MUST be edited with the 'visudo' command as root.\n#\n# See the sudoers man page for the details on how to write a sudoers file.\n#\n\n# Host alias specification\n\n# User alias specification\n\n# Cmnd alias specification\n\n# Defaults specification\n\n# User privilege specification\nroot    ALL=(ALL) ALL\nXXXXX   ALL=(ALL) ALL\n\n\n# Uncomment to allow people in group wheel to run all commands\n# %wheel    ALL=(ALL)   ALL\n\n# Same thing without a password\n# %wheel    ALL=(ALL)   NOPASSWD: ALL\n\n# Samples\n# %users  ALL=/sbin/mount /cdrom,/sbin/umount /cdrom\n# %users  localhost=/sbin/shutdown -h now\n\n#aaron  ALL= /usr/bin /sbin/mount /cdrom,/sbin/umount /cdrom\n\nEOF\n\n\n\n\n# the following is to stop windows admins accidentally rebooting linux boxes when they share a KVM with windows\necho \"#\"\necho \"# Disabling shutdown (CTRL/ALT/DEL)\"\necho \"#\"\nmv /etc/inittab /etc/inittab.bak\nsed -e \"s/^pf:/#pf:/\" -e \"s/^pr:/#pr:/\" -e \"s/^ca:/#ca:/\"  /etc/inittab.bak \n /etc/inittab\nchmod 700 /etc/inittab\nchown root:root /etc/inittab\necho;echo\n\n\n#echo \"#\"\n#echo \"# Disable xinetd\"\n#echo \"#\"\n#for service in `ls -1 /etc/xinetd.d`; do chkconfig $service off; done\n#echo ;echo\n\n\n\necho \" creating MOTD\"\necho \"\" \n /etc/motd\necho \"+----------------------------------------------------+\" \n /etc/motd\necho \"| This system is the property of ME                  |\" \n /etc/motd\necho \"+----------------------------------------------------+\" \n /etc/motd\necho \"\"\n /etc/motd\n\n\n\n\n\n\n\n\n\n# This will install mondo from an NFS mount and copy the config file\n\n#local mount point for NFS\nTEMP_MOUNT=/tmp/software\nmkdir $TEMP_MOUNT\n\n# Source of NSF share\nNFS_SERVER=xxxxx.xxxxx.com\nNFS_SHARE=/u1/software\nAPP_VERSION=mondo-v2.2\nNFS_PATH=$NFS_SHARE/$APP_VERSION/\n\n\necho \"mounting nfs share to install extra software\"\n\nmount $NFS_SERVER:$NFS_SHARE  $TEMP_MOUNT\n\n# create diectories for mondo cd images\nmkdir /u1\nmkdir /u1/iso\nmkdir /u1/iso/tmp\nmkdir /u1/mondo\n\n\n\necho \"# install of mondo packages\"\n\n#echo \"installing mkisofs\"\n#rpm -ivh  $TEMP_MOUNT/$APP_VERSION/mkisofs-2.0-11.i386.rpm\n\n#echo \"installing cdrecord\"\n#rpm -ivh  $TEMP_MOUNT/$APP_VERSION/cdrecord-2.0-11.i386.rpm\n\necho \"installing buffer\"\nrpm -ivh  $TEMP_MOUNT/$APP_VERSION/buffer-1.19-4.i386.rpm\n\necho \"installing afio\"\nrpm -ivh  $TEMP_MOUNT/$APP_VERSION/afio-2.4.7-1.i386.rpm\n\necho \"installing mindi\"\nrpm -ivh  $TEMP_MOUNT/$APP_VERSION/mindi-1.06-266.rhel3.i386.rpm\n\necho \"installing mondo\"\nrpm -ivh  $TEMP_MOUNT/$APP_VERSION/mondo-2.2.0-2.rhel3.i586.rpm\n\n# copy mondo run script\nmkdir /usr/local/admin\ncp  $TEMP_MOUNT/$APP_VERSION/mondo.sh /usr/local/admin/mondo\nchmod 744 /usr/local/admin/mondo\n\n\necho \"# Install of Nagios Agent\"\n\n# swap this variable from mondo to nagios\nAPP_VERSION=nagios-agent-2.5.2\n\n\n\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/fping-2.4-1.b2.2.el4.rf.i386.rpm\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Crypt-DES-2.05-3.2.el4.rf.i386.rpm\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Digest-SHA1-2.07-5.i386.rpm\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Digest-HMAC-1.01-13.noarch.rpm\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Socket6-0.19-1.2.el4.rf.i386.rpm\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/perl-Net-SNMP-5.2.0-1.2.el4.rf.noarch.rpm\nrpm -Uvh $TEMP_MOUNT/$APP_VERSION/nagios-plugins-1.4-2.2.el4.rf.i386.rpm\n\ncp $TEMP_MOUNT/$APP_VERSION/nrpe /usr/sbin\ncp $TEMP_MOUNT/$APP_VERSION/nrpe.cfg /etc/nagios\ncp $TEMP_MOUNT/$APP_VERSION/nrpe.xinetd /etc/xinetd.d/nrpe\necho \"nrpe        5666/tcp     #nagios agent\" \n /etc/services\nuseradd nagios\ncp $TEMP_MOUNT/$APP_VERSION/check_nrpe /usr/lib/nagios/plugins/\ncp $TEMP_MOUNT/$APP_VERSION/check_procs /usr/lib/nagios/plugins/\n\n\n\n\necho \"editing the fstab file to remove the strange LABEL for the swap partition\"\n# with out changing this mondo restore gets confused\n\ncp /etc/fstab /etc/fstab.bak\ncat /etc/fstab.bak | sed s/LABEL\\=SW\\-/\\\\/dev\\\\// \n /etc/fstab\n\n\n\necho \"fstab has been edited\"\n\n\n\n\necho \"installing XXXX custom grub splash screen\"\nAPP_VERSION=grub_custom_XXXX\n\n\necho \"swapping the redhat grub splash screen with custom logo\"\ncp /boot/grub/splash.xpm.gz /boot/grub/splash.xpm.gz.orig\ncp $TEMP_MOUNT/$APP_VERSION/XXXX_grub_grey.xpm.gz /boot/grub/splash.xpm.gz", 
            "title": "Kickstart"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/perl_script_to_calulate_uptime_of_satellite_clients/", 
            "text": "#!/usr/bin/perl\n#This unfinished script will poll a satellite/spacewalk server for each registered host and return the uptime of each hosts\n\n##Start of Sample output\n###########\n#Host Name       = centos64small01.localdomain\n#last_boot       = 20140426T04:42:41\n#last_checkin    = 20140426T08:53:03\n#lastboot_epoc :1398487320\n#time now      :1398502383\n# seconds 15063\n# minutes 251.05\n# hours   4.18416666666667\n#\n###########\n#Host Name       = centos64small02.localdomain\n#last_boot       = 20130808T01:34:21\n#last_checkin    = 20130808T10:18:03\n#lastboot_epoc :1375925640\n#time now      :1398502383\n# seconds 22576743\n# minutes 376279.05\n# hours   6271.3175\n##End of Sample output\n\n\nuse strict;\nuse warnings;\n\nuse Frontier::Client;\nuse boolean;\nuse Time::Local;\n#use Data::Dumper;\n#use DateTime::Format::ISO8601;\n#use POSIX;\n\nmy $HOST = 'spacewalk';\nmy $user = 'admin';\nmy $pass = 'password';\n\nmy $client = new Frontier::Client(url =\n \"http://$HOST/rpc/api\");\nmy $session = $client-\ncall('auth.login',$user, $pass) or die;\n\nmy $systems = $client-\ncall('system.listSystems', $session);\n\n\nforeach my $system (@$systems) {\n\n        my %client = %{$system};\n        my $lastcheckin = ${$client{'last_checkin'}};\n        my $systemid = $system-\n{'id'};\n        my $hostname = $system-\n{'name'};\n        my $Detail = $client-\ncall('system.getDetails', $session, $systemid);\n        my %clientdetails = %{$Detail};\n        my $lastboot = ${$clientdetails{'last_boot'}};\n            print \"\\n###########\\n\";\n            print \"Host Name       = $hostname\\n\";\n            print \"last_boot       = $lastboot\\n\";\n            print \"last_checkin    = $lastcheckin\\n\";\n\n\n        ## convert the non standard ISO8601 date format to a series of variables the to epoc.\n        my $year = substr(\"$lastboot\", 0,4);\n        my $month = substr(\"$lastboot\", 4,2);\n        $month--; ## timegm counts months from 0 ie... Jan = 0 Dec =11\n        my $day = substr(\"$lastboot\", 6,2);\n        my $hour = substr(\"$lastboot\", 9,2);\n        my $min = substr(\"$lastboot\", 12,2);\n        ## print \" year  = $year\\n month = $month -1\\n day   = $day\\n hour  = $hour\\n min   = $min\\n\";\n        my $lastboot_epoc = timegm(00,$min,$hour,$day,$month,$year);\n        print \"lastboot_epoc :$lastboot_epoc\\n\";\n\n\n\n##calculating the difference between lastboot_epoc and now\nmy $timenow = time;\nprint \"time now      :$timenow\\n\";\nmy $timediff =  $timenow - $lastboot_epoc ;\nprint \" seconds $timediff\\n\";\nmy $minutes = $timediff / 60 ;\nmy $hours = $minutes / 60 ;\nprint \" minutes $minutes\\n\";\nprint \" hours   $hours\\n\";\n}", 
            "title": "Perl script to calulate uptime of satellite clients"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/replacing_your_satellite_certificate/", 
            "text": "replacing your satellite certificate\n\n\n\n\n\n\nlogin to the Red Hat Customer Portal - self-service Satellite Certificate generation tool \n\n\nhttps://access.redhat.com/management/distributors/\n\n\n\n\n\n\nGenerate/update your certificate.\n     \nfurther information can be found here\n\n\nhttps://access.redhat.com/site/articles/477863\n\n\n\n\n\n\nDownload your new certificate\n\n\n\n\n\n\nInstall/Activate the certificate\n\n\n\n\n\n\nRestart the satellite server\n\n\n\n\n\n\n\n\n[root@server ~]# rhn-satellite-activate   -vvv --rhn-cert=/home/seamusmurray/MySat.xml\nRHN_PARENT: satellite.rhn.redhat.com\nHTTP_PROXY: None\nHTTP_PROXY_USERNAME: None\nHTTP_PROXY_PASSWORD: \npassword\n\nCA_CERT: /usr/share/rhn/RHNS-CA-CERT\nChecking cert XML sanity and GPG signature: '/usr/bin/validate-sat-cert.pl --keyring /etc/webapp-keyring.gpg     /etc/sysconfig/rhn/rhn-entitlement-cert.xml-XXXX'\nDatabase connectioned initialized: refer to /etc/rhn/rhn.conf\nAttempting local RHN Certificate push (and therefore activation)\nExecuting: remote XMLRPC deactivation (if necessary).\nExecuting: remote XMLRPC activation call.\nExecuting: '/usr/bin/satellite-sync --list-channels --ca-cert /usr/share/rhn/RHNS-CA-CERT'\n\n\n\n[root@server ~]# /usr/sbin/rhn-satellite restart\nShutting down spacewalk services...\nStopping RHN Taskomatic...\nWaiting for RHN Taskomatic to exit...\nWaiting for RHN Taskomatic to exit...\nWaiting for RHN Taskomatic to exit...\nStopped RHN Taskomatic.\nStopping cobbler daemon:                                   [  OK  ]\nStopping rhn-search...\nWaiting for rhn-search to exit...\nWaiting for rhn-search to exit...\nWaiting for rhn-search to exit...\nStopped rhn-search.\nStopping MonitoringScout ...\n[ OK ]\nStopping Monitoring ...\n[ OK ]\nShutting down osa-dispatcher:                              [  OK  ]\nStopping httpd:                                            [  OK  ]\nStopping tomcat5:                                          [  OK  ]\nTerminating jabberd processes ...\nStopping s2s:                                              [  OK  ]\nStopping c2s:                                              [  OK  ]\nStopping sm:                                               [  OK  ]\nStopping router:                                           [  OK  ]\nDone.\nStarting spacewalk services...\nInitializing jabberd processes ...\nStarting router:                                           [  OK  ]\nStarting sm:                                               [  OK  ]\nStarting c2s:                                              [  OK  ]\nStarting s2s:                                              [  OK  ]\nStarting tomcat5:                                          [  OK  ]\nWaiting for tomcat to be ready ...\nStarting httpd:                                            [  OK  ]\nStarting osa-dispatcher:                                   [  OK  ]\nStarting Monitoring ...\n[ OK ]\nStarting MonitoringScout ...\n[ OK ]\nStarting rhn-search...\nStarting cobbler daemon:                                   [  OK  ]\nStarting RHN Taskomatic...\nDone.", 
            "title": "Replacing your satellite certificate"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/satellite_certificate_licence_install/", 
            "text": "replacing your satellite certificate\n\n\n\n\n\n\nlogin to the Red Hat Customer Portal - self-service Satellite Certificate generation tool \n\n\nhttps://access.redhat.com/management/distributors/\n\n\n\n\n\n\nGenerate/update your certificate.\n     \nfurther information can be found here\n\n\nhttps://access.redhat.com/site/articles/477863\n\n\n\n\n\n\nDownload your new certificate\n\n\n\n\n\n\nInstall/Activate the certificate\n\n\n\n\n\n\nRestart the satellite server\n\n\n\n\n\n\n\n\n[root@server ~]# rhn-satellite-activate   -vvv --rhn-cert=/home/seamusmurray/MySat.xml\nRHN_PARENT: satellite.rhn.redhat.com\nHTTP_PROXY: None\nHTTP_PROXY_USERNAME: None\nHTTP_PROXY_PASSWORD: \npassword\n\nCA_CERT: /usr/share/rhn/RHNS-CA-CERT\nChecking cert XML sanity and GPG signature: '/usr/bin/validate-sat-cert.pl --keyring /etc/webapp-keyring.gpg     /etc/sysconfig/rhn/rhn-entitlement-cert.xml-ZsgzKl'\nDatabase connection initialized: refer to /etc/rhn/rhn.conf\nAttempting local RHN Certificate push (and therefore activation)\nExecuting: remote XMLRPC deactivation (if necessary).\nExecuting: remote XMLRPC activation call.\nExecuting: '/usr/bin/satellite-sync --list-channels --ca-cert /usr/share/rhn/RHNS-CA-CERT'\n\n\n\n\n[root@server ~]# /usr/sbin/rhn-satellite restart\nShutting down spacewalk services...\nStopping RHN Taskomatic...\nWaiting for RHN Taskomatic to exit...\nWaiting for RHN Taskomatic to exit...\nWaiting for RHN Taskomatic to exit...\nStopped RHN Taskomatic.\nStopping cobbler daemon:                                   [  OK  ]\nStopping rhn-search...\nWaiting for rhn-search to exit...\nWaiting for rhn-search to exit...\nWaiting for rhn-search to exit...\nStopped rhn-search.\nStopping MonitoringScout ...\n[ OK ]\nStopping Monitoring ...\n[ OK ]\nShutting down osa-dispatcher:                              [  OK  ]\nStopping httpd:                                            [  OK  ]\nStopping tomcat5:                                          [  OK  ]\nTerminating jabberd processes ...\nStopping s2s:                                              [  OK  ]\nStopping c2s:                                              [  OK  ]\nStopping sm:                                               [  OK  ]\nStopping router:                                           [  OK  ]\nDone.\nStarting spacewalk services...\nInitializing jabberd processes ...\nStarting router:                                           [  OK  ]\nStarting sm:                                               [  OK  ]\nStarting c2s:                                              [  OK  ]\nStarting s2s:                                              [  OK  ]\nStarting tomcat5:                                          [  OK  ]\nWaiting for tomcat to be ready ...\nStarting httpd:                                            [  OK  ]\nStarting osa-dispatcher:                                   [  OK  ]\nStarting Monitoring ...\n[ OK ]\nStarting MonitoringScout ...\n[ OK ]\nStarting rhn-search...\nStarting cobbler daemon:                                   [  OK  ]\nStarting RHN Taskomatic...\nDone.", 
            "title": "Satellite certificate licence install"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/", 
            "text": "Installation on RHEL 5\n\n\nwget http://fedora.mirror.uber.com.au/epel/5/x86_64/spacecmd-1.9.4-1.el5.noarch.rpm\nsudo yum install python-simplejson.x86_64\nsudo rpm -i spacecmd-1.9.4-1.el5.noarch.rpm\n\n\n\nConfigure account credentials\n\n\n[USER@SERVER ~]$ cat .spacecmd/config\n[spacecmd]\nserver=localhost\nusername=admin\npassword=PASSWORD\nnossl=1\n\n\n\nRunning spacecmd\n\n\nspacecmd can be run either interactively (with tab completion and a help\nmenu) or non interactive both modes enable you to run a single\nspacecmd-command plus wither a single or multiple operand\n\n\nInteractive mode\n\n\n[USER@SERVER ~]$ spacecmd\nWelcome to spacecmd, a command-line interface to Spacewalk.\n\nType: 'help' for a list of commands\n      'help \ncmd\n' for command-specific help\n      'quit' to quit\n\nINFO: Connected to http://localhost/rpc/api as admin\nspacecmd {SSM:0}\n system_details server1.localhost\nspacecmd {SSM:0}\n system_details server1.localhost server2.localhost\n\n\n\nNonInteractive from the shell (bash, csh etc...)\n\n\n[USER@SERVER ~]$ spacecmd system_details server1.localhost   ## output goes to standard out\n[USER@SERVER ~]$ spacecmd system_details server1.localhost server2.localhost \n {ouput file}   ##redirected the output to a file\n\n\n\nGroup creation and manipulation\n\n\nInteractive mode\n\n\nUSER@SERVER ~]spacecmd\n\nlog in\n\nspacecmd {SSM:41}\n group_create newgroup\nspacecmd {SSM:41}\n group_addsystems newgroup server1.local server2.local \nspacecmd {SSM:41}\n group_addsystems newgroup search:name:server\n\n\n\nNonInteractive from the shell (bash, csh etc...)\n\n\nUSER@SERVER ~]spacecmd group_create newgroup   # you will be prompted for a group description\nUSER@SERVER ~]spacecmd group_addsystems newgroup server1.local server2.local\nUSER@SERVER ~]for i in ``cat list-of-servers`` ; do spacecmd group_addsystems newgroup  $i ;done\n\n\n\nNote:\n\n\nUsing the output redirection is handy when dealing with lots of data, if\nyou use sed/awk/grep/perl to parse the output you can save your self a\nlot of time.--\n\n\nlist all systems registered to the spacewalk server\n\n\nUSER@SERVER ~]spacecmd system_list \n all-systems\n\n\n\nList all systems with a certain package+version installed\n\n\nUSER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64 \n fakepackagename-1.2.3-4.x86_64_installed\n\n\n\nComparing the installed packages on 2 servers\n\n\n[USER@SERVER ~]$ spacecmd system_comparepackages server1.local server2.local\nINFO: Connected to http://localhost/rpc/api as admin\nPackage                               This System              Other System            Difference\n------------------------------------  -----------------------  ----------------------  ----------\ninitscripts                           8.45.42-1.0.1.el5        8.45.42-1.0.1.el5_8.1   Newer there\nkernel                                2.6.18-308.24.1.0.1.el5  None                    Only here\nkernel-fake                           None                     2.6.32-400.11.1.el5     Only there\n\n\n\nlisting negative delta lists..\n\n\nUnfortunately you cannot query satellite/spacewalk for a list of\nservers where something does not exist\n\n\nI work around this limitation by in 3 steps\n\n\nFull-list - With-list = WithOut-list\n\n\nlist all systems registered to the spacewalk server\n\n\nUSER@SERVER ~]spacecmd system_list \n all-systems\n\n\n\nlist all systems with a certain package+version installed\n\n\nUSER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64 \n fakepackagename-1.2.3-4.x86_64_installed\n\n\n\nGenerate a list of the servers that do not have the package installed\n\n\nfor i in `cat all-systems` \ndo if grep -q $i fakepackagename-1.2.3-4.x86_64_installed \n   then true\n   else echo $i \n   fi \ndone \n fakepackagename-1.2.3-4.x86_64_not_installed\n\n\n\nyou now have a delta list\n\n\n## note before you compare outputs from spacecmd you have to clean up\nthe files first eg...remove the extra headers from the output files and\nthe MS dos line feeds \\^M etc....\n\n\nscheduling - keeping track of a scheduled job initiated from the spacewalk web interface\n\n\nInteractive mode\n\n\n[USER@SERVER ~]$ spacecmd\nWelcome to spacecmd, a command-line interface to Spacewalk.\n\nType: 'help' for a list of commands\n      'help \ncmd\n' for command-specific help\n      'quit' to quit\n\nINFO: Connected to http://localhost/rpc/api as admin\nspacecmd {SSM:0}\n schedule_details 243013\n\n\n\nNonInteractive from the shell (bash, csh etc...)\n\n\n[USER@SERVER ~]$ spacecmd schedule_details 243013   ## output goes to standard out\n[USER@SERVER ~]$ spacecmd schedule_details 243013 \n {output file}   ##redirected the output to a file\n\n\n\nInteractive mode\n\n\n[USER@SERVER ~]$ spacecmd\nWelcome to spacecmd, a command-line interface to Spacewalk.\n\nType: 'help' for a list of commands\n      'help \ncmd\n' for command-specific help\n      'quit' to quit\n\nINFO: Connected to http://localhost/rpc/api as admin\nspacecmd {SSM:0}\n schedule_getoutput 243013\n\n\n\nNonInteractive from the shell (bash, csh etc...)\n\n\n[USER@SERVER ~]$ spacecmd schedule_getoutput 243013   ## output goes to standard out\n[USER@SERVER ~]$ spacecmd schedule_getoutput 243013 \n {output file}   ##redirected the output to a file\n\n\n\nRunning scripts on multiple severs.\n\n\nInstead of trying to parse relatively unstructured output I prefer to\nstructure the the output before it goes into spacewalk\n\n\nMy preferred method is echo the server name followed by the output of\nthe command, this way it is easy to grep out the server you want\n\n\neg... if you want to run the following\n\n\nscript\n\n\n#!/bin/sh\n/usr/sbin/dmidecode | grep -m 1 Product\n\n\n\noutput from spacemcd\n_\n\n\nspacecmd {SSM:0}\n schedule_getoutput 245295\nSystem:      server1.local\nStart Time:  20131118T15:22:52\nStop Time:   20131118T15:22:52\nReturn Code: 0\n\nOutput\n------\n        Product Name: VMware Virtual Platform\n\n\n##############################\n\nSystem:      server2.local\nStart Time:  20131118T15:24:38\nStop Time:   20131118T15:24:38\nReturn Code: 0\n\nOutput\n------\n        Product Name: VMware Virtual Platform\n\n\n\nscript\n\n\n#!/bin/sh\nproduct=`/usr/sbin/dmidecode | grep -m 1 Product`\nservername=`hostname`\necho $servername  $product\n\n\n\noutput from spacemcd\n_\n\n\nspacecmd {SSM:0}\n schedule_getoutput 245295\nSystem:      server1.local\nStart Time:  20131118T15:22:52\nStop Time:   20131118T15:22:52\nReturn Code: 0\n\nOutput\n------\nserver1.local        Product Name: VMware Virtual Platform\n\n\n##############################\n\nSystem:      server2.local\nStart Time:  20131118T15:24:38\nStop Time:   20131118T15:24:38\nReturn Code: 0\n\nOutput\n------\nserver2.local        Product Name: VMware Virtual Platform\n\n\n\n__with the second method it is very easy to just grep for\nserver*local or Vmware__\n\n\nscript to capture the IP details across all Ethernet interfaces\n\n\nrun across all hosts to capture all IP's on all hosts\n\n\n#!/bin/bash\n\nHOST_NAME=`hostname -s`\nNICS=`ls /sys/class/net | grep -v lo`\n\nfor i in $NICS\n  do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://`\ndone\n\n\n\nyou could achieve the same results by querying the oracle database directly or using the satellite API. I chose to use spacecmd, grep and awk because that\u2019s what I'm more comfortable with\n\n\nthis method seems to put less load on the satellite server and the back end database\n\n\n(it creates hash tables locally in .spacecmd/localhost/) which I assume reduces hits on the database.\n\n\nIf the batch \"list_PackageName-6.1_not_installed\" is too large to run in one go..\n\n\nyou can divide it up based on server name prefix/suffix or simply batch size eg....\n\n\nname based divide\n\n\ngrep servername list_PackageName-6.1_not_installed \n list_PackageName-6.1_not_installed_servernames\ngrep '[0-9]pr' list_PackageName-6.1_not_installed \n list_PackageName-6.1_not_installed_pr\n\n\n\nbatch size divide\n\n\nSTARTNUM=100\nLASTNUM=200\nawk '$1 == '$STARTNUM' , $1 == '$LASTNUM'' list_PackageName-6.1_not_installed \n list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM\n\n\n\nCreate a system group to run the job against\n\n\nGROUP_NAME=PackageName_install \nspacecmd group_create  $GROUP_NAME\n\n\n\nPopulate the group\n\n\n           for i in `cat list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM\n           do spacecmd group_addsystems $GROUP_NAME $i \n           done\n\n\n\nschedule the job from the GUI\n\n\nyou can either keep track of the scheduled job from the GUI or from spacecmd\n\n\nspacecmd schedule_details {schedule number}\n\n\n\nIf you need to schedule a job from satellite at a precise time, you can use satellite to run an arbitrary script to set up an at job for the client to check in with the satellite server at a specific time (within 60 seconds)\n\n\nYou can schedule a second job in satellite to perform the the actual work.\n\n\nExample.\n\n\nSuppose you want to execute the the following command at 6pm today\n\n\nps -Aef \n\n\nSchedule an \"at\" job to execute the /usr/sbin/rhn_check at your desired time\n\n\nNote: To ensure that all the clients have polled the satellite server and picked up this job you will have to schedule this at least an hour before your \"desired time\"\n\n\n#!/bin/sh\nPreciseTime=1800\ncat \n /tmp/sat-precision-strike.sh \nEOF\necho `date` \" starting the at job\" \n /tmp/sat-precision-strike.log\n/usr/sbin/rhn_check\necho `date` \" finished the at job\" \n /tmp/sat-precision-strike.log\nrm -f /tmp/sat-precision-strike.sh\nEOF\nsleep 5\nat -f /tmp/sat-precision-strike.sh $PreciseTime\necho \"added at job at -f /root/sat-precision-strike $PreciseTime\" \n /tmp/sat-precision-strike.log\nsleep 5\n\n\n\nSchedule a second Job detailing the command that you need to run\n\n\nensure you set the \"Schedule no sooner than:\" time to 1 minute before your desired time\n\n\n#!/bin/bash\n/bin/ps -Aef", 
            "title": "Using spacecmd"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#installation-on-rhel-5", 
            "text": "wget http://fedora.mirror.uber.com.au/epel/5/x86_64/spacecmd-1.9.4-1.el5.noarch.rpm\nsudo yum install python-simplejson.x86_64\nsudo rpm -i spacecmd-1.9.4-1.el5.noarch.rpm", 
            "title": "Installation on RHEL 5"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#configure-account-credentials", 
            "text": "[USER@SERVER ~]$ cat .spacecmd/config\n[spacecmd]\nserver=localhost\nusername=admin\npassword=PASSWORD\nnossl=1", 
            "title": "Configure account credentials"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#running-spacecmd", 
            "text": "spacecmd can be run either interactively (with tab completion and a help\nmenu) or non interactive both modes enable you to run a single\nspacecmd-command plus wither a single or multiple operand  Interactive mode  [USER@SERVER ~]$ spacecmd\nWelcome to spacecmd, a command-line interface to Spacewalk.\n\nType: 'help' for a list of commands\n      'help  cmd ' for command-specific help\n      'quit' to quit\n\nINFO: Connected to http://localhost/rpc/api as admin\nspacecmd {SSM:0}  system_details server1.localhost\nspacecmd {SSM:0}  system_details server1.localhost server2.localhost  NonInteractive from the shell (bash, csh etc...)  [USER@SERVER ~]$ spacecmd system_details server1.localhost   ## output goes to standard out\n[USER@SERVER ~]$ spacecmd system_details server1.localhost server2.localhost   {ouput file}   ##redirected the output to a file", 
            "title": "Running spacecmd"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#group-creation-and-manipulation", 
            "text": "Interactive mode  USER@SERVER ~]spacecmd log in \nspacecmd {SSM:41}  group_create newgroup\nspacecmd {SSM:41}  group_addsystems newgroup server1.local server2.local \nspacecmd {SSM:41}  group_addsystems newgroup search:name:server  NonInteractive from the shell (bash, csh etc...)  USER@SERVER ~]spacecmd group_create newgroup   # you will be prompted for a group description\nUSER@SERVER ~]spacecmd group_addsystems newgroup server1.local server2.local\nUSER@SERVER ~]for i in ``cat list-of-servers`` ; do spacecmd group_addsystems newgroup  $i ;done  Note:  Using the output redirection is handy when dealing with lots of data, if\nyou use sed/awk/grep/perl to parse the output you can save your self a\nlot of time.--", 
            "title": "Group creation and manipulation"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#list-all-systems-registered-to-the-spacewalk-server", 
            "text": "USER@SERVER ~]spacecmd system_list   all-systems", 
            "title": "list all systems registered to the spacewalk server"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#list-all-systems-with-a-certain-packageversion-installed", 
            "text": "USER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64   fakepackagename-1.2.3-4.x86_64_installed", 
            "title": "List all systems with a certain package+version installed"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#comparing-the-installed-packages-on-2-servers", 
            "text": "[USER@SERVER ~]$ spacecmd system_comparepackages server1.local server2.local\nINFO: Connected to http://localhost/rpc/api as admin\nPackage                               This System              Other System            Difference\n------------------------------------  -----------------------  ----------------------  ----------\ninitscripts                           8.45.42-1.0.1.el5        8.45.42-1.0.1.el5_8.1   Newer there\nkernel                                2.6.18-308.24.1.0.1.el5  None                    Only here\nkernel-fake                           None                     2.6.32-400.11.1.el5     Only there", 
            "title": "Comparing the installed packages on 2 servers"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#listing-negative-delta-lists", 
            "text": "Unfortunately you cannot query satellite/spacewalk for a list of\nservers where something does not exist  I work around this limitation by in 3 steps", 
            "title": "listing negative delta lists.."
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#full-list-with-list-without-list", 
            "text": "list all systems registered to the spacewalk server  USER@SERVER ~]spacecmd system_list   all-systems  list all systems with a certain package+version installed  USER@SERVER ~]spacecmd package_listinstalledsystems fakepackagename-1.2.3-4.x86_64   fakepackagename-1.2.3-4.x86_64_installed  Generate a list of the servers that do not have the package installed  for i in `cat all-systems` \ndo if grep -q $i fakepackagename-1.2.3-4.x86_64_installed \n   then true\n   else echo $i \n   fi \ndone   fakepackagename-1.2.3-4.x86_64_not_installed  you now have a delta list  ## note before you compare outputs from spacecmd you have to clean up\nthe files first eg...remove the extra headers from the output files and\nthe MS dos line feeds \\^M etc....  scheduling - keeping track of a scheduled job initiated from the spacewalk web interface  Interactive mode  [USER@SERVER ~]$ spacecmd\nWelcome to spacecmd, a command-line interface to Spacewalk.\n\nType: 'help' for a list of commands\n      'help  cmd ' for command-specific help\n      'quit' to quit\n\nINFO: Connected to http://localhost/rpc/api as admin\nspacecmd {SSM:0}  schedule_details 243013  NonInteractive from the shell (bash, csh etc...)  [USER@SERVER ~]$ spacecmd schedule_details 243013   ## output goes to standard out\n[USER@SERVER ~]$ spacecmd schedule_details 243013   {output file}   ##redirected the output to a file  Interactive mode  [USER@SERVER ~]$ spacecmd\nWelcome to spacecmd, a command-line interface to Spacewalk.\n\nType: 'help' for a list of commands\n      'help  cmd ' for command-specific help\n      'quit' to quit\n\nINFO: Connected to http://localhost/rpc/api as admin\nspacecmd {SSM:0}  schedule_getoutput 243013  NonInteractive from the shell (bash, csh etc...)  [USER@SERVER ~]$ spacecmd schedule_getoutput 243013   ## output goes to standard out\n[USER@SERVER ~]$ spacecmd schedule_getoutput 243013   {output file}   ##redirected the output to a file", 
            "title": "Full-list - With-list = WithOut-list"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#running-scripts-on-multiple-severs", 
            "text": "Instead of trying to parse relatively unstructured output I prefer to\nstructure the the output before it goes into spacewalk  My preferred method is echo the server name followed by the output of\nthe command, this way it is easy to grep out the server you want  eg... if you want to run the following  script  #!/bin/sh\n/usr/sbin/dmidecode | grep -m 1 Product  output from spacemcd _  spacecmd {SSM:0}  schedule_getoutput 245295\nSystem:      server1.local\nStart Time:  20131118T15:22:52\nStop Time:   20131118T15:22:52\nReturn Code: 0\n\nOutput\n------\n        Product Name: VMware Virtual Platform\n\n\n##############################\n\nSystem:      server2.local\nStart Time:  20131118T15:24:38\nStop Time:   20131118T15:24:38\nReturn Code: 0\n\nOutput\n------\n        Product Name: VMware Virtual Platform  script  #!/bin/sh\nproduct=`/usr/sbin/dmidecode | grep -m 1 Product`\nservername=`hostname`\necho $servername  $product  output from spacemcd _  spacecmd {SSM:0}  schedule_getoutput 245295\nSystem:      server1.local\nStart Time:  20131118T15:22:52\nStop Time:   20131118T15:22:52\nReturn Code: 0\n\nOutput\n------\nserver1.local        Product Name: VMware Virtual Platform\n\n\n##############################\n\nSystem:      server2.local\nStart Time:  20131118T15:24:38\nStop Time:   20131118T15:24:38\nReturn Code: 0\n\nOutput\n------\nserver2.local        Product Name: VMware Virtual Platform  __with the second method it is very easy to just grep for\nserver*local or Vmware__  script to capture the IP details across all Ethernet interfaces  run across all hosts to capture all IP's on all hosts  #!/bin/bash\n\nHOST_NAME=`hostname -s`\nNICS=`ls /sys/class/net | grep -v lo`\n\nfor i in $NICS\n  do echo -e $HOST_NAME $i `ifconfig $i | grep \"inet addr\" | awk -F\" \" '{print $2 \" \" $4}' | sed s/addr\\://`\ndone  you could achieve the same results by querying the oracle database directly or using the satellite API. I chose to use spacecmd, grep and awk because that\u2019s what I'm more comfortable with  this method seems to put less load on the satellite server and the back end database  (it creates hash tables locally in .spacecmd/localhost/) which I assume reduces hits on the database.  If the batch \"list_PackageName-6.1_not_installed\" is too large to run in one go..  you can divide it up based on server name prefix/suffix or simply batch size eg....  name based divide  grep servername list_PackageName-6.1_not_installed   list_PackageName-6.1_not_installed_servernames\ngrep '[0-9]pr' list_PackageName-6.1_not_installed   list_PackageName-6.1_not_installed_pr", 
            "title": "Running scripts on multiple severs."
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#batch-size-divide", 
            "text": "STARTNUM=100\nLASTNUM=200\nawk '$1 == '$STARTNUM' , $1 == '$LASTNUM'' list_PackageName-6.1_not_installed   list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM", 
            "title": "batch size divide"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#create-a-system-group-to-run-the-job-against", 
            "text": "GROUP_NAME=PackageName_install \nspacecmd group_create  $GROUP_NAME", 
            "title": "Create a system group to run the job against"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#populate-the-group", 
            "text": "for i in `cat list_PackageName-6.1_not_installed-numbered-$STARTNUM-$LASTNUM\n           do spacecmd group_addsystems $GROUP_NAME $i \n           done", 
            "title": "Populate the group"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#schedule-the-job-from-the-gui", 
            "text": "you can either keep track of the scheduled job from the GUI or from spacecmd  spacecmd schedule_details {schedule number}", 
            "title": "schedule the job from the GUI"
        }, 
        {
            "location": "/linux/RedHat/satellite_spacewalk/using_spacecmd/#if-you-need-to-schedule-a-job-from-satellite-at-a-precise-time-you-can-use-satellite-to-run-an-arbitrary-script-to-set-up-an-at-job-for-the-client-to-check-in-with-the-satellite-server-at-a-specific-time-within-60-seconds", 
            "text": "You can schedule a second job in satellite to perform the the actual work.  Example.  Suppose you want to execute the the following command at 6pm today  ps -Aef   Schedule an \"at\" job to execute the /usr/sbin/rhn_check at your desired time  Note: To ensure that all the clients have polled the satellite server and picked up this job you will have to schedule this at least an hour before your \"desired time\"  #!/bin/sh\nPreciseTime=1800\ncat   /tmp/sat-precision-strike.sh  EOF\necho `date` \" starting the at job\"   /tmp/sat-precision-strike.log\n/usr/sbin/rhn_check\necho `date` \" finished the at job\"   /tmp/sat-precision-strike.log\nrm -f /tmp/sat-precision-strike.sh\nEOF\nsleep 5\nat -f /tmp/sat-precision-strike.sh $PreciseTime\necho \"added at job at -f /root/sat-precision-strike $PreciseTime\"   /tmp/sat-precision-strike.log\nsleep 5  Schedule a second Job detailing the command that you need to run  ensure you set the \"Schedule no sooner than:\" time to 1 minute before your desired time  #!/bin/bash\n/bin/ps -Aef", 
            "title": "If you need to schedule a job from satellite at a precise time, you can use satellite to run an arbitrary script to set up an at job for the client to check in with the satellite server at a specific time (within 60 seconds)"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/create_local_repos_from_redhat_install_media/", 
            "text": "How to create a local yum REPO from the RedHat install media\n\n\nRHEL 7 local repo\n\n\ncat \n /etc/yum.repos.d/dvd-iso.repo \n EOF\n[InstallMedia]\nname=RHEL-7.0 Server.x86_64\nmetadata_expire=-1\ngpgcheck=0\ncost=500\nbaseurl=file:///media/repo\nEOF\n\n\n\n\n\nmkdir /media/repo\nmount /dev/sr0 /media/repo\n\n\n\n\n\nRHEL 6 local repo\n\n\nmkdir -p /mnt/DVD-ISO-REPO/\nmount /dev/dvd /mnt/DVD-ISO-REPO/\n\n\n\ncopy the media.repo file from the ISO\n\n\ncp /mnt/DVD-ISO-REPO/media.repo /etc/yum.repos.d/media.repo\n\necho \"baseurl=file:///mnt/DVD-ISO-REPO/\" \n /etc/yum.repos.d/media.repo\n\n\n\n\n\nRHEL 5 local repo\n\n\ncat \n /etc/yum.repos.d/dvd-iso.repo \n EOF\n##repo file for mounting the RHEL 5.x DVD ISO\n\n\n[local_Cluster]\nname=Local Cluster\nbaseurl=file:///mnt/DVD-ISO-REPO/Cluster/\nenabled=1\ngpgcheck=0\n\n\n[local_ClusterStorage]\nname=Local Cluster Storage\nbaseurl=file:///mnt/DVD-ISO-REPO/ClusterStorage/\nenabled=1\ngpgcheck=0\n\n\n[Local_Server]\nname= Local Server\nbaseurl=file:///mnt/DVD-ISO-REPO/Server\nenabled=1\ngpgcheck=0\n\nEOF\n\n\n\n\n\nmkdir -p /mnt/DVD-ISO-REPO\nmount /dev/dvd /mnt/DVD-ISO-REPO/\nyum repolist\n\n\n\n\n\nRHEL 4 local repo\n\n\nMount the RHEL 4 installation ISO.\n\n\nmount -o loop,ro RHEL4.8-x86_64-AS-DVD.iso /media/cdrom\n\n\n\nComment out the following line in /etc/sysconfig/rhn/sources\n\n\nThis will prevent up2date from connecting to Red Hat Network\n\n\nup2date default\n\n\n\nChange to:\n\n\n#up2date default\n\n\n\ncreate a line like the following in /etc/sysconfig/rhn/sources\n\n\ndir mydvdrepo /media/cdrom/RedHat/RPMS\n\n\n\n\n\nList the REPOS\n\n\nyum repolist\n\n\n\nepel repo install\n\n\nrpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm", 
            "title": "Create local repos from redhat install media"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/working_with_yum_and_rpm/", 
            "text": "working with yum and rpm\n\n\n\n\ndownload the latest repodata\n\n\nyum clean\nyum repolist\n\n\n\nlist current repos\n\n\nyum repolist\n\n\n\nQuerying data from the RPM database\n\n\nList the available tags\n\n\nrpm --querytags\n\n\n\nyou can specify 1 or more tags plus what ever formatting you want /n /t etc..\n\n\nrpm -q -qf \"%{TAGNAME}\\n\" fake-rpm\nor\nrpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm\n\n\n\ndetermine which rpm has the largest file on the server\n\n\nrpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on thrid field\n\n\n\ndetermine when an rpm was installed, displayed in human readable time\n\n\nrpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm\n\n\n\nfixing python on RHEL when someone breaks it\n\n\nfor i in `rpm -qa | grep python` ; do yum reinstall $i -y ; done\n\n\n\ndisplay a list of the last 10 rpms to be installed\n\n\nrpm -q --all --last  | head -10\n\n\n\ndetermine inter dependencies\n\n\nrpm -q --whatrequires /usr/lib/libstdc++.so.5\n\nrpm -q --whatprovides /usr/lib/libstdc++.so.5\n\n\n\ndisable yum plugins on RHEL if your not registering the server with a satellite server\n\n\n\n\nResigning a vendor package with the your own GPG key\n\n\n[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 14214 License: GPLv2+\nSignature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.\n\n[root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nEnter pass phrase:\nPass phrase is good.\nspacewalk-selinux-1.4.1-1.el5-test.src.rpm:\ngpg: WARNING: standard input reopened\ngpg: WARNING: standard input reopened\n\n[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 14214 License: GPLv2+\nSignature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.", 
            "title": "Working with yum and rpm"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/working_with_yum_and_rpm/#working-with-yum-and-rpm", 
            "text": "download the latest repodata  yum clean\nyum repolist  list current repos  yum repolist", 
            "title": "working with yum and rpm"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/working_with_yum_and_rpm/#querying-data-from-the-rpm-database", 
            "text": "List the available tags  rpm --querytags  you can specify 1 or more tags plus what ever formatting you want /n /t etc..  rpm -q -qf \"%{TAGNAME}\\n\" fake-rpm\nor\nrpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm  determine which rpm has the largest file on the server  rpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on thrid field  determine when an rpm was installed, displayed in human readable time  rpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm  fixing python on RHEL when someone breaks it  for i in `rpm -qa | grep python` ; do yum reinstall $i -y ; done  display a list of the last 10 rpms to be installed  rpm -q --all --last  | head -10  determine inter dependencies  rpm -q --whatrequires /usr/lib/libstdc++.so.5\n\nrpm -q --whatprovides /usr/lib/libstdc++.so.5  disable yum plugins on RHEL if your not registering the server with a satellite server", 
            "title": "Querying data from the RPM database"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/working_with_yum_and_rpm/#resigning-a-vendor-package-with-the-your-own-gpg-key", 
            "text": "[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 14214 License: GPLv2+\nSignature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.\n\n[root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nEnter pass phrase:\nPass phrase is good.\nspacewalk-selinux-1.4.1-1.el5-test.src.rpm:\ngpg: WARNING: standard input reopened\ngpg: WARNING: standard input reopened\n\n[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 14214 License: GPLv2+\nSignature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.", 
            "title": "Resigning a vendor package with the your own GPG key"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/yum_and_rpm/", 
            "text": "working with yum and rpm\n\n\n\n\ndownload the latest repodata\n\n\nyum clean\nyum repolist\n\n\n\nlist current repos\n\n\nyum repolist\n\n\n\n\n\nQuerying data from the RPM database\n\n\nList the available tags\n\n\nrpm --querytags\n\n\n\nyou can specify 1 or more tags plus what ever formatting you want /n /t etc..\n\n\nrpm -q -qf \"%{TAGNAME}\\n\" fake-rpm\nor\nrpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm\n\n\n\ndetermine which rpm has the largest file on the server\n\n\nrpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on third field\n\n\ndetermine when an rpm was installed, displayed in human readable time\n\n\nrpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm\n\n\ndetermine inter dependencies\n\n\nrpm -q --whatrequires /usr/lib/libstdc++.so.5\n\n\nrpm -q --whatprovides /usr/lib/libstdc++.so.5\n\n\n\ndisable yum plugins on RHEL if your not registering the server with a satellite server\n\n\n\n\nResigning a vendor package with the example GPG key\n\n\n[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 144 License: GPLv2+\nSignature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.\n\n[root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nEnter pass phrase:\nPass phrase is good.\nspacewalk-selinux-1.4.1-1.el5-test.src.rpm:\ngpg: WARNING: standard input reopened\ngpg: WARNING: standard input reopened\n\n[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 144 License: GPLv2+\nSignature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.\n\n\n\n\n\nRPM packaging and development\n\n\nBuilding a custom meta package that encompasses multiple customer child package\n\n\nsource files /usr/src/redhat/SOURCES/example-1.1.tar.gz\n\n\nmember packages\n\n\n\n\nexample-1.1-4.el5.i386.rpm (meta-package)\n\n\nexample-tools-1.1-4.el5.i386.rpm\n\n\nexample-devops-1.1-4.el5.i386.rpm\n\n\nexample-security-1.1-4.el5.i386.rpm\n\n\nexample-vmware-1.1-4.el5.i386.rpm\n\n\n\n\nRPM Package development server.\n\n\n\n\nThe RPM build directory is /usr/src/redhat/\n\n\npackages are built as the root user\n\n\nYou will need the pass-phrase for the example gpg signing key\n\n\nWhen building the example-tools there are distribution specific dependencies.\n\n\nexample meta package is built as 32bit. as it contains no binary executables it will work on both 32 \n 64bit.\n\n\n\n\nWalk though of updating/modifying the example package\n\n\nDelete the previously extracted package tarball, this is required in case someone has made changes to the files\n\n\nrm -R /usr/src/redhat/BUILD/example-1.1\n\n\n\nExtract the package tar ball that was created during the previous RPM creation\n\n\ncd /usr/src/redhat/BUILD/\ntar xvf /usr/src/redhat/SOURCES/example-1.1.tar.gz\n\n\n\nMake the desired change to the files in the rpm. eg...\n\n\nvi /usr/src/redhat/BUILD/example-1.1/etc/cron.daily/housekeeping-daily\n\n\n\nCreate a compressed tarball of the files that will be including in the RPM\n\n\ncd /usr/src/redhat/BUILD/\ntar zcvf /usr/src/redhat/SOURCES/example-1.1.tar.gz ./example-1.1\n\n\n\nUpdate the release version and change log in the spec file\n\n\nvi /usr/src/redhat/SPECS/example.spec\n\n\n\nBuild the rpms for the various distribution targets\n\n\nrpmbuild --define 'dist .el5' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec\nrpmbuild --define 'dist .el6' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec.el6\n\n\n\nRepo locations...\n\n\n/data/redhat/linux/enterprise/5Server/en/example/i386/\n/data/redhat/linux/enterprise/5Server/en/example/x86_64/\n/data/redhat/linux/enterprise/6Server/en/example/x86_64/\n/data/redhat/linux/enterprise/7Server/en/example/x86_64/\n\n\n\nCopy new RPMs to the repo directories\n\n\nNEWVERSION=\"1.1-4\" # set to the desired version \ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/i386/\ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el6.i386.rpm /data/redhat/linux/enterprise/6Server/en/example/x86_64/\ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/x86_64/\ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el7.i386.rpm /data/redhat/linux/enterprise/7Server/en/example/x86_64/\n\n\n\nUpdating the repos\n\n\ncd /data/redhat/linux/enterprise/5Server/en/example/i386/\ncreaterepo .\ncd /data/redhat/linux/enterprise/6Server/en/example/x86_64/\ncreaterepo .\ncd /data/redhat/linux/enterprise/5Server/en/example/x86_64/\ncreaterepo .\ncd /data/redhat/linux/enterprise/7Server/en/example/x86_64/\ncreaterepo .\n\n\n\nSyncing the YUM repos with the satellite server\n\n\nLogin to the REDHAT satellite server\n\nand sync the channels individually\n\n\nspacewalk-repo-sync --channel rhel6-x86_64-example --type yum\nspacewalk-repo-sync --channel rhel5-x86_64-example --type yum\nspacewalk-repo-sync --channel rhel5-i386-example   --type yum\nspacewalk-repo-sync --channel rhel7-x86_64-example --type yum\n\n\n\nor\n\n\nSync all the channels including to RedHat ( warning this will take a long time )\n\n\n[root@satellite1 ~]# /opt/example/bin/satellite-sync.sh\n\nNo new packages to sync.\nRepo http://server.example.local/redhat/linux/enterprise/5Server/en/rhel-example/x86_64/ has 0 errata.\nSync completed.\nTotal time: 0:00:00\nRepo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/\nPackages in repo: 46\nPackages already synced: 37\nPackages to sync: 9\n1/9 : kmod-vmware-tools-pvscsi-1.0.2.0-2.6.18.8.el5.3-0.x86_64\n2/9 : vmware-tools-xorg-drv-mouse-12.6.4.0-4-0.x86_64\n3/9 : kmod-vmware-tools-vsock-1.0.0.0-2.6.18.8.el5.3-0.x86_64\n4/9 : kmod-vmware-tools-vmsync-1.1.0.1-2.6.18.8.el5.3-0.x86_64\n5/9 : kmod-vmware-tools-vmci-9.0.1.1-2.6.18.8.el5.3-0.x86_64\n6/9 : kmod-vmware-tools-vmmemctl-1.2.1.2-2.6.18.8.el5.3-0.x86_64\n7/9 : kmod-vmware-tools-vmblock-1.1.2.0-2.6.18.8.el5.3-0.x86_64\n8/9 : kmod-vmware-tools-vmhgfs-1.4.1.1-2.6.18.8.el5.3-0.x86_64\n9/9 : vmware-tools-xorg-drv-display-11.0.2.10-4-0.x86_64\nRepo http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/ has 0 errata.\nSync completed.\nTotal time: 0:00:04\nRepo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/\nPackages in repo: 90\nPackages already synced: 86\nPackages to sync: 4\n1/4 : example-tools-1.1-4.el5-0.i386\n2/4 : example-tools-1.1-213.el5-0.i386\n3/4 : tidalagent-3.1.0.13-8-0.i386 \nRepo http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/ has 0 errata.\nSync completed.\n\n\n\n\n\nDealing with a package interdependency issue of my own creation....\n\n\nTo explain the issue, in the package dependencies we have:\n\n\n\n\nexample-tools requires example at the same version\n\n\nexample-devops requires example at the same version \n\n\nThere is no version lock between tools and devops\n\n\n\n\nThe upgrade appears to go as follows: (Assume installed version is \u20181\u2019, target version is \u20182\u2019 and latest in repo is \u20183\u2019\n\n\n\n\nAll three packages installed at version 1\n\n\nRequest example-tools upgrade to version 2, requires example also at version 2.\n\n\nConflict as example-devops is at version 1, and requires example at version 1\n\n\nyum attempts to resolve conflict, by trying to upgrade example-devops. (This is an upgrade that will just pick the latest available, so picks version 3\n\n\nexample-devops version 3 depends on example version 3\n\n\nConflict now between example and example-tools, so upgrade example-tools to resolve\n\n\nAll packages now at version 3.\n\n\n\n\nTwo things to try:\n\n\n\n\nVersion lock all of the packages together (This will work)\n\n\n\n\nCreate a RHN errata, and try applying it.\n\n\nyum install example-devops --exclude=\n1.1-2\n --exclude=\n1.1-3\n\n\nInstalled:\n  example-devops.i386 0:1.1-207.el5", 
            "title": "Yum and rpm"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/yum_and_rpm/#working-with-yum-and-rpm", 
            "text": "download the latest repodata  yum clean\nyum repolist  list current repos  yum repolist", 
            "title": "working with yum and rpm"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/yum_and_rpm/#querying-data-from-the-rpm-database", 
            "text": "List the available tags  rpm --querytags  you can specify 1 or more tags plus what ever formatting you want /n /t etc..  rpm -q -qf \"%{TAGNAME}\\n\" fake-rpm\nor\nrpm -q --qf \"%{INSTALLTID}\\n%{FILENAMES}\\n%{BUILDTIME}\\n\" fake-rpm  determine which rpm has the largest file on the server  rpm -qa --queryformat \"[%-15{NAME} %-50{FILENAMES} %{FILESIZES}\\n]\" | sort -n -k 3 | tail -20 # -k 3 = sort on third field  determine when an rpm was installed, displayed in human readable time  rpm -q --qf \"%{NAME}-%{VERSION}-%{RELEASE} %{INSTALLTIME:date}\\n\" fake-rpm  determine inter dependencies  rpm -q --whatrequires /usr/lib/libstdc++.so.5  rpm -q --whatprovides /usr/lib/libstdc++.so.5  disable yum plugins on RHEL if your not registering the server with a satellite server", 
            "title": "Querying data from the RPM database"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/yum_and_rpm/#resigning-a-vendor-package-with-the-example-gpg-key", 
            "text": "[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 144 License: GPLv2+\nSignature : DSA/SHA1, Tue 26 Apr 2011 03:57:18 AM EST, Key ID ed635379b3892132\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.\n\n[root@server ~]# rpm --resign spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nEnter pass phrase:\nPass phrase is good.\nspacewalk-selinux-1.4.1-1.el5-test.src.rpm:\ngpg: WARNING: standard input reopened\ngpg: WARNING: standard input reopened\n\n[root@server ~]# rpm -qip spacewalk-selinux-1.4.1-1.el5-test.src.rpm\nName : spacewalk-selinux Relocations: (not relocatable)\nVersion : 1.4.1 Vendor: Koji\nRelease : 1.el5 Build Date: Thu 03 Mar 2011 08:57:31 PM EST\nInstall Date: (not installed) Build Host: domU-12-31-38-00-09-D1\nGroup : System Environment/Base Source RPM: (none)\nSize : 144 License: GPLv2+\nSignature : DSA/SHA1, Mon 19 Aug 2013 12:21:12 PM EST, Key ID 6cf907deb8607f33 ## Key has changed\nPackager : Koji\nURL : http://fedorahosted.org/spacewalk\nSummary : SELinux policy module supporting Spacewalk Server\nDescription :\nSELinux policy module supporting Spacewalk Server.", 
            "title": "Resigning a vendor package with the example GPG key"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/yum_and_rpm/#rpm-packaging-and-development", 
            "text": "Building a custom meta package that encompasses multiple customer child package  source files /usr/src/redhat/SOURCES/example-1.1.tar.gz  member packages   example-1.1-4.el5.i386.rpm (meta-package)  example-tools-1.1-4.el5.i386.rpm  example-devops-1.1-4.el5.i386.rpm  example-security-1.1-4.el5.i386.rpm  example-vmware-1.1-4.el5.i386.rpm   RPM Package development server.   The RPM build directory is /usr/src/redhat/  packages are built as the root user  You will need the pass-phrase for the example gpg signing key  When building the example-tools there are distribution specific dependencies.  example meta package is built as 32bit. as it contains no binary executables it will work on both 32   64bit.", 
            "title": "RPM packaging and development"
        }, 
        {
            "location": "/linux/RedHat/yum_rpm/yum_and_rpm/#walk-though-of-updatingmodifying-the-example-package", 
            "text": "Delete the previously extracted package tarball, this is required in case someone has made changes to the files  rm -R /usr/src/redhat/BUILD/example-1.1  Extract the package tar ball that was created during the previous RPM creation  cd /usr/src/redhat/BUILD/\ntar xvf /usr/src/redhat/SOURCES/example-1.1.tar.gz  Make the desired change to the files in the rpm. eg...  vi /usr/src/redhat/BUILD/example-1.1/etc/cron.daily/housekeeping-daily  Create a compressed tarball of the files that will be including in the RPM  cd /usr/src/redhat/BUILD/\ntar zcvf /usr/src/redhat/SOURCES/example-1.1.tar.gz ./example-1.1  Update the release version and change log in the spec file  vi /usr/src/redhat/SPECS/example.spec  Build the rpms for the various distribution targets  rpmbuild --define 'dist .el5' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec\nrpmbuild --define 'dist .el6' --target=i386 -ba --sign /usr/src/redhat/SPECS/example.spec.el6  Repo locations...  /data/redhat/linux/enterprise/5Server/en/example/i386/\n/data/redhat/linux/enterprise/5Server/en/example/x86_64/\n/data/redhat/linux/enterprise/6Server/en/example/x86_64/\n/data/redhat/linux/enterprise/7Server/en/example/x86_64/  Copy new RPMs to the repo directories  NEWVERSION=\"1.1-4\" # set to the desired version \ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/i386/\ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el6.i386.rpm /data/redhat/linux/enterprise/6Server/en/example/x86_64/\ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el5.i386.rpm /data/redhat/linux/enterprise/5Server/en/example/x86_64/\ncp -v /usr/src/redhat/RPMS/i386/example*$NEWVERSION.el7.i386.rpm /data/redhat/linux/enterprise/7Server/en/example/x86_64/  Updating the repos  cd /data/redhat/linux/enterprise/5Server/en/example/i386/\ncreaterepo .\ncd /data/redhat/linux/enterprise/6Server/en/example/x86_64/\ncreaterepo .\ncd /data/redhat/linux/enterprise/5Server/en/example/x86_64/\ncreaterepo .\ncd /data/redhat/linux/enterprise/7Server/en/example/x86_64/\ncreaterepo .  Syncing the YUM repos with the satellite server  Login to the REDHAT satellite server \nand sync the channels individually  spacewalk-repo-sync --channel rhel6-x86_64-example --type yum\nspacewalk-repo-sync --channel rhel5-x86_64-example --type yum\nspacewalk-repo-sync --channel rhel5-i386-example   --type yum\nspacewalk-repo-sync --channel rhel7-x86_64-example --type yum  or  Sync all the channels including to RedHat ( warning this will take a long time )  [root@satellite1 ~]# /opt/example/bin/satellite-sync.sh\n\nNo new packages to sync.\nRepo http://server.example.local/redhat/linux/enterprise/5Server/en/rhel-example/x86_64/ has 0 errata.\nSync completed.\nTotal time: 0:00:00\nRepo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/\nPackages in repo: 46\nPackages already synced: 37\nPackages to sync: 9\n1/9 : kmod-vmware-tools-pvscsi-1.0.2.0-2.6.18.8.el5.3-0.x86_64\n2/9 : vmware-tools-xorg-drv-mouse-12.6.4.0-4-0.x86_64\n3/9 : kmod-vmware-tools-vsock-1.0.0.0-2.6.18.8.el5.3-0.x86_64\n4/9 : kmod-vmware-tools-vmsync-1.1.0.1-2.6.18.8.el5.3-0.x86_64\n5/9 : kmod-vmware-tools-vmci-9.0.1.1-2.6.18.8.el5.3-0.x86_64\n6/9 : kmod-vmware-tools-vmmemctl-1.2.1.2-2.6.18.8.el5.3-0.x86_64\n7/9 : kmod-vmware-tools-vmblock-1.1.2.0-2.6.18.8.el5.3-0.x86_64\n8/9 : kmod-vmware-tools-vmhgfs-1.4.1.1-2.6.18.8.el5.3-0.x86_64\n9/9 : vmware-tools-xorg-drv-display-11.0.2.10-4-0.x86_64\nRepo http://server.example.local/redhat/linux/enterprise/5Server/en/vmware-tools/latest/x86_64/ has 0 errata.\nSync completed.\nTotal time: 0:00:04\nRepo URL: http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/\nPackages in repo: 90\nPackages already synced: 86\nPackages to sync: 4\n1/4 : example-tools-1.1-4.el5-0.i386\n2/4 : example-tools-1.1-213.el5-0.i386\n3/4 : tidalagent-3.1.0.13-8-0.i386 \nRepo http://server.example.local/redhat/linux/enterprise/5Server/en/example/i386/ has 0 errata.\nSync completed.   Dealing with a package interdependency issue of my own creation....  To explain the issue, in the package dependencies we have:   example-tools requires example at the same version  example-devops requires example at the same version   There is no version lock between tools and devops   The upgrade appears to go as follows: (Assume installed version is \u20181\u2019, target version is \u20182\u2019 and latest in repo is \u20183\u2019   All three packages installed at version 1  Request example-tools upgrade to version 2, requires example also at version 2.  Conflict as example-devops is at version 1, and requires example at version 1  yum attempts to resolve conflict, by trying to upgrade example-devops. (This is an upgrade that will just pick the latest available, so picks version 3  example-devops version 3 depends on example version 3  Conflict now between example and example-tools, so upgrade example-tools to resolve  All packages now at version 3.   Two things to try:   Version lock all of the packages together (This will work)   Create a RHN errata, and try applying it.  yum install example-devops --exclude= 1.1-2  --exclude= 1.1-3  Installed:\n  example-devops.i386 0:1.1-207.el5", 
            "title": "Walk though of updating/modifying the example package"
        }, 
        {
            "location": "/linux/Commands/email_on_linux/", 
            "text": "change mail relay for postfix\n\n\nvi /etc/postfix/transport\n\npostmap /etc/postfix/transport\n\npostfix\n\n\n\ndelete all mail from queue\n\n\npostsuper -d ALL\n\npostqueue -p\n\n\n\nsend a text file as inline\n\n\nmailx -s \"test from new server\" seamus@example.com\n~r{filename}\n.", 
            "title": "Email on linux"
        }, 
        {
            "location": "/linux/Commands/email_on_linux/#change-mail-relay-for-postfix", 
            "text": "vi /etc/postfix/transport\n\npostmap /etc/postfix/transport\n\npostfix", 
            "title": "change mail relay for postfix"
        }, 
        {
            "location": "/linux/Commands/email_on_linux/#delete-all-mail-from-queue", 
            "text": "postsuper -d ALL\n\npostqueue -p", 
            "title": "delete all mail from queue"
        }, 
        {
            "location": "/linux/Commands/email_on_linux/#send-a-text-file-as-inline", 
            "text": "mailx -s \"test from new server\" seamus@example.com\n~r{filename}\n.", 
            "title": "send a text file as inline"
        }, 
        {
            "location": "/linux/Commands/hot_add_memory/", 
            "text": "How to enable Hot-add Memory in Linux RHEL 5 ? \nWhat if you have Hot-added memory in your RHEL 5 server but it's still displaying Last memory configuration ?\n\n\nStep : \n\n\nCheck if the acpi modules are loaded\n\n\nlsmod | grep acpi\n\n\n\nYou should see both ...\n\n\nacpiphp                43673  0\nacpi_memhotplug        42199  0\n\n\n\nIf not loaded, \n\n\n modprobe acpiphp\n modprobe acpi_memhotplug\n\n\n\n\n\nNow increase your server memory with Hot-add memory module. This will be done by Hardware engineer directly on the server, i am not talking about on OS level.\n\n\n\n\nto determine which modules are offline\n\n\n grep offline /sys/devices/system/memory/memory*/state\n\n\n\nexecute either of the following to set the modules to be online\n\n\n for offline_module in `grep offline /sys/devices/system/memory/memory*/state | sed s/\\:offline//g`\n do echo  \"online\" \n $offline_module\n done\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncat /sys/devices/system/memory/memory*/state\n\n\n\nSave it \n exit\n\n\nFor Ubuntu you have to try with another script as below,\n\n\n#!/bin/sh\ni=0\nwhile [ $i -lt 4500 ]\ndo\n   if [ -f /sys/devices/system/memory/memory$i/state ]\n   then\n      if grep \"offline\" /sys/devices/system/memory/memory$i/state\n      then\n         echo 'online' \n /sys/devices/system/memory/memory$i/state\n      fi\n   fi\n   i=`expr $i + 1`\ndone\n\n\n\n\n\n\n\nGive this script execution permission by,\n\n\nchmod 777 memory_online.sh\n\n\n\n\n\n\nNext execute it \n check memory in either \"/proc/meminfo\" file or by below command\n\n\n\n\n\n\n#free\n\n\nDone.", 
            "title": "Hot add memory"
        }, 
        {
            "location": "/linux/Commands/hot_add_memory/#chmod-777-memory_onlinesh", 
            "text": "Next execute it   check memory in either \"/proc/meminfo\" file or by below command    #free  Done.", 
            "title": "chmod 777 memory_online.sh"
        }, 
        {
            "location": "/linux/Monitoring/nagios_2.6_install/", 
            "text": "Seamus's notes on an Install Nagios 2.6 for Suse 10 Solaris 9 and HPUX\n\n\nNagios is a scheduling Engine and nothing more!\n\n\n1)  Description of Nagios \n\n\nThis is the way it was designed from the out set. The idea is to keep the core of Nagios as simple as possible, and therefore as flexible as possible. Nagios it self cannot probe, test, email or SMS by itself it relies on other programs/plugins for all of its functions. The 3 main category's of plugins are listed below.\n\n\nNRPE allows you to remotely execute Nagios plugins on other Linux/Unix machines. \n\nNSCA allows you to integrate passive alerts and checks from remote machines and applications with Nagios. \n*Nagios plugins, there are literally thousands of 3rd party Nagios plugins that cover everything from simple ping's to logging into oracle and performing database queries. The Nagios install usually comes with a default set of plugins. \n\n\n2)  Overview  of  Nagios' configuration\n\n\nNagios's keeps its configuration in flat text files. It is left up to you (the Admin)  to manage these file's by hand. You can have all the config data in a single file or you can split it up with files per host, per service etc..\n\n\nAfter you have a few hosts configured. It becomes tedious and time consuming to manually manage all the interdependencies of the various configuration values.  eg if you remove a host from one file you then must go through all the service and alert config files and remove it from there.\n\n\nTo see how complicated the configuration interdependencies are click the following link.\n\n\nhttp://nagios.sourceforge.net/download/contrib/documentation/misc/config_diagrams/nagios-config.png\n\n\n3)  Overview of NagiosQL\n\n\nDue to the complexity of the config files. I chose to use a php/MySQL frontend called NagiosQL to manage all the config files on my behalf. There are a number of current and orphaned Nagios front end projects I chose NagiosQL because it seemed to concentrate on just getting the job done.\n\n\nWith NagiosQL all the configuration information is stored in a MySQL database. All updates are performed via web forms. Java scripting is used to perform some form validation. Once you have made all your changes you click the write config files link and all the local config files are over written with the new config from the database. You will have to restart Nagios for the new config to take effect. You can use either of the 3 following methods....\n\n\ncommand line..... /etc/init.d/nagios reload\n\n\nnagios CGI.....  lefthand panel --\n process info --\n Restart the Nagios process ++\n\n\nnagiosQL.....   lefthand panel --\n  Tools --\n Nagios control --\n Restart Nagios ++\n\n\nEND of Nagios and NagiosQL Intro\n\n\nInstallation of Nagios and Nagios QL \n\n\nList of packages I installed on the nagios server example.com.\n\n\n\n\nnagios-nsca-2.6-7.6\n\n\nnagios-plugins-1.4.5-16.6\n\n\nnagios-2.6-13.7\n\n\nnagios-nsca-client-2.6-7.6\n\n\nnagios-plugins-extras-1.4.5-16.6\n\n\nnagios-www-2.6-13.7\n\n\nnagios-nrpe-2.12-1\n\n\nnagiosQL-2.0.2-1.1\n\n\n\n\nN.B. the reason why i chose nagios 2.x over nagios 3.x is because there are no Mysql frontends for 3.x yet.\n\n\nSteps I took to install Nagios server on SUSE 10..........\n\n\nInstall Nagios and associated dependencies via YAST \nInstall nrpe daemon \n\n\nSUSE doesn't come with the nrpe daemon. So you can either compile it yourself or find a package provided by a a 3rd party. I chose to use an rpm provided by.....\n\n\nhttp://blog.barfoo.org/2008/07/14/latest-sles10-rpm-additions\n\n\n  rpm -ivh nagios-nrpe-2.12-1.i586.rpm\n\n\n\nOnce nagios nrpe and all the plugins have been installed you have to setup the permissions for the web interface.\n\n\ncreate the /usr/lib/nagios/cgi/.htaccess file with the following \n\n\nAuthName \"Nagios Access\"\nAuthType Basic\nAuthUserFile /etc/nagios/.htpasswd.users\nrequire valid-user\n\n\n\ncreate the password file with \n\n\n/usr/sbin/htpasswd2 -c /etc/nagios/.htpasswd.users {user name}\n\n\n\nrestart Apache and login to the web page to test \nhttp://hostname/nagios\n\n\nOK Nagios has now been installed.\n\n\n\n\nInstall mysql from YAST\n\n\nstart mysql and setup a password for the root account...\n\n\n/usr/bin/mysqladmin -u root password 'new-password'\n/usr/bin/mysqladmin -u root -h example.com password 'new-password'\n\n\n\nBefore you install NagiosQL, make you you have meet the following  requirements.....\n\n\n\n\n\n\nWebserver (Apache 1.3.x/2.0.x)\n\n\n\n\n\n\nPHP Version 4.1 or higher / 5.0 or higher\n\n\n\n\n\n\nMySQL Version 4.1 or higher / 5.0 or higher\n\n\n\n\n\n\nPear Module HTML_Template_IT Version 1.1 (http://pear.php.net)\n\n\n\n\n\n\nNagios Version 2.x (1.x is not supported)\n\n\n\n\n\n\nJavascript supported by your browser\n\n\n\n\n\n\nCookies accepted by your browser \n\n\n\n\n\n\ndownload the nagiosql front end and php-pear-HTML_Template_IT from \n\n\nwget http://download.opensuse.org/repositories/home:/rhomann/SLE_10/noarch/nagiosQL-2.0.2-1.1.noarch.rpm\nwget http://download.opensuse.org/repositories/home:/rhomann/SLE_10/noarch/php5-pear-HTML_Template_IT-1.2.1-4.1.noarch.rpm\n\n\n\nInstall the rpms\n\n\nrpm -ivh  php5-pear-HTML_Template_IT-1.2.1-4.1.noarch.rpm\nrpm -ivh  nagiosQL-2.0.2-1.1.noarch.rpm\n\n\n\nStart the mysql database engine.\n\n\n/etc/init.d/mysqld start\n\n\n\npopulate MySQL with data for nagiosQL : \n\n\n mysql -u root -p \n /usr/share/nagiosQL/config/nagiosQL_v2_db_mysql.sql\n\n\n\nFor some reason the SQL script above didn't create the user account so I had to perform this step manually...\n\n\nmysql -u root -p\nmysql\n use mysql\nDatabase changed\nmysql\n GRANT USAGE ON *.* TO 'nagiosQLusr'@'localhost' IDENTIFIED BY 'nagiosQLpwd';\nQuery OK, 0 rows affected (0.01 sec)\nmysql\n GRANT SELECT,INSERT,UPDATE,DELETE ON `nagiosQL`.* TO'nagiosQLusr'@'localhost';\nQuery OK, 0 rows affected (0.00 sec)\n\n\n\nBy default Nagios stores all its configuration information is just 2 files, NagiosQL prefers that you spread the configuration info across multiple files..\n\n\nYou have to manually create directories so that NagiosQL can create the config files in the specified directories.\n\n\nYou will also have to make changes to the /etc/nagios/nagios.cfg file to reflect these changes.\n\n\n~ In my case apache ran as user wwwrun and group daemon ~\n\n\nmkdir /etc/nagios/hosts\nmkdir /etc/nagios/services\nmkdir /etc/nagios/backup\nmkdir /etc/nagios/backup/hosts\nmkdir /etc/nagios/backup/services \nchmod 6755 /etc/nagios\nchown wwwrun.daemon /etc/nagios\nchmod 6755 /etc/nagios/hosts\nchown wwwrun.daemon /etc/nagios/hosts\nchmod 6755 /etc/nagios/services\nchown wwwrun.daemon /etc/nagios/services\nchmod 6755 /etc/nagios/backup\nchown wwwrun.daemon /etc/nagios/backup\nchmod 6755 /etc/nagios/backup/hosts\nchown wwwrun.daemon /etc/nagios/backup/hosts\nchmod 6755 /etc/nagios/backup/services\nchown wwwrun.daemon /etc/nagios/backup/services\nchmod 644 /etc/nagios/*.cfg\nchown wwwrun.daemon /etc/nagios/*.cfg\n\n\n\ncommunication from the web interface to the nagios daemon is performed via a named pipe\n\n\nthis allows to to trigger nagios to reread its config files or disable alerts etc..\n\n\ncreate command file and set permissions, note the location of this file is specified in the /etc/nagios.cfg\n\n\ntouch /var/spool/nagios/nagios.cmd \nchown nagios.www /var/spool/nagios/nagios.cmd\nchmod 660  /var/spool/nagios/nagios.cmd\n\n\n\nedit the /etc/php5/apache2/php.ini config file to allow magicquotes\n\n\nmagic_quotes_gpc = On\n\n\n\nset language to english ....\n\n\nlang  = lang_en\n\n\n\nrun http://example.com/nagiosQL/testQL.php to test the configuration\n\n\nOnce you have all this working nagios will be able to perform external probes of its clients\n\n\ncheck_alive # ping\n\n\ncheck_http # check if a web server is up\n\n\nIf you only wish to perform external probes there is no need to install any extra software on the (monitored hosts) .\n\n\nIf you wish to check things such as disk usage number of processes users etc you will need to install an agent on the  (monitored hosts) below details the steps i took to get nrpe working on the (monitored hosts).\n\n\nEnd of Installation of Nagios and Nagios QL\n\n\nStart of Client Installation section \n\n\nCLIENTS\n\n\nSUSE RPM dependency order for the nrpe daemon and plugins.\n\n\nrpm -ivh perl-Crypt-DES-2.05-3.2.el4.rf.i386.rpm\nrpm -ivh perl-Socket6-0.20-1.el4.rf.i386.rpm\nrpm -ivh perl-Net-SNMP-5.2.0-1.2.el4.rf.noarch.rpm\nrpm -ivh fping-2.4-1.b2.2.el4.rf.i386.rpm\nrpm -ivh nagios-plugins-1.4.9-1.el4.rf.i386.rpm\nrpm -ivh nagios-nrpe-2.5.2-1.el4.rf.i386.rpm\n\n\n\nHP-UX\n\n\nNeither HP or the Nagios team provide  runtime binary's.\n\n\nGetting the nrpe agent to work on HPUX  requires you to either compile from source or use 3rd party binaries. \n\n\nUnfortunately I never managed to get ssl to work under HPUX so from the nagios server I have to use the -n (noSSL) option with nrpe. eg\n\n\nnrpe_check -n -H mailhost.example.com -c check_load\n\n\n\nWe had the nrpe daemon for hpux-Itanium on a cd, i don't know where it came from i think it was part of \"HP's Internet Express repository\" it may have come from here http://mayoxide.com/naghpux/\n\n\nThe PA_RISC nrep daemon can be downloaded from here\n\n\nhttp://www.bennyvision.com/projects/nagios/index.php\n\n\nSolaris 9\n\n\nNeither SUN or the Nagios team provide runtime binary's.\n\n\nInstead of compiling from source I chose to go the lazy route of using binary packages provided by www.blastwave.org/packages.php/nrpe . To install this package required a vast number of dependant packages.\n\n\nI used the automated dependency system to do the hard work for me. Fist this required the installation of.. pkg-get\n\n\nI first downloaded a self contained binary of wget\n\n\nhttp://blastwave.network.com/csw/wget-sparc.bin\n\n\nI then used this binary to (w)get the pkg-get\n\n\nwget http://blastwave.network.com/csw/pkg_get.pkg\npkgadd -d pkg_get.pkg  #install the package\nvi /opt/csw/etc/pkg-get.conf  #edit the http proxy value and enable the export proxy directive\npkg-get -U #download the latest list of packages and their interdependencies\npkg-get -i wget #install the full version of wget\n\n\n\nNow for the big install...\n\n\npkg-get -i nrpe # this will require many dependant packages, just say yes to all .\n\n\n\nonce this a has finished you will have an installation of nrpe and all the plugins.\n\n\nyou can test the plugins by running them directly\n\n\n/opt/csw/libexec/nagios-plugins/check_load -w 15,10,5 -c 30,25,20\n/opt/csw/libexec/nagios-plugins/check_dummy 0\n\n\n\nIf this works fine, you can then configure the nrpe daemon\n\n\nabout the only changes you will have to /opt/csw/etc/nrpe.cfg are to allow the nrpe daemon to accept connections from the nagios server\n\n\nallowed_hosts=127.0.0.1,192.168.125.42\n\n\n\nand to allow command argument processing\n\n\ndont_blame_nrpe=0 # set to 1 if you want to enable\n\n\n\nyou can now start the daemon manually by running\n\n\n/opt/csw/bin/nrpe -d -c /opt/csw/etc/nrpe.cfg\n\n\n\nthen from a remote host check the daemon by running one of the command that is enabled in the target hosts nrpe.cfg file eg....\n\n\n/usr/lib/nagios/plugins/check_nrpe -H {target host}  -c check_load -a -w  1,1,1,2,2,2\n\n\n\nOnce this is working kill the daemon and set it up via inet.d \n\n\nadd the following line to /etc/inetd.conf\n\n\nnrpe    stream  tcp     nowait  nagios  /opt/csw/bin/nrpe       /opt/csw/bin/nrpe -i -c /opt/csw/etc/nrpe.cfg\n\n\n\nadd the following line to /etc/services\n\n\nnrpe    5666/tcp\n\n\n\nforce the inet daemon to reread the config files\n\n\npkill -HUP inetd\n\n\n\nrun the remote check again\n\n\nEnd of Client install section", 
            "title": "Nagios 2.6 install"
        }, 
        {
            "location": "/linux/Monitoring/nagios_2.6_install/#hp-ux", 
            "text": "Neither HP or the Nagios team provide  runtime binary's.  Getting the nrpe agent to work on HPUX  requires you to either compile from source or use 3rd party binaries.   Unfortunately I never managed to get ssl to work under HPUX so from the nagios server I have to use the -n (noSSL) option with nrpe. eg  nrpe_check -n -H mailhost.example.com -c check_load  We had the nrpe daemon for hpux-Itanium on a cd, i don't know where it came from i think it was part of \"HP's Internet Express repository\" it may have come from here http://mayoxide.com/naghpux/  The PA_RISC nrep daemon can be downloaded from here  http://www.bennyvision.com/projects/nagios/index.php", 
            "title": "HP-UX"
        }, 
        {
            "location": "/linux/Monitoring/nagios_2.6_install/#solaris-9", 
            "text": "", 
            "title": "Solaris 9"
        }, 
        {
            "location": "/linux/Monitoring/nagios_2.6_install/#neither-sun-or-the-nagios-team-provide-runtime-binarys", 
            "text": "Instead of compiling from source I chose to go the lazy route of using binary packages provided by www.blastwave.org/packages.php/nrpe . To install this package required a vast number of dependant packages.  I used the automated dependency system to do the hard work for me. Fist this required the installation of.. pkg-get  I first downloaded a self contained binary of wget  http://blastwave.network.com/csw/wget-sparc.bin  I then used this binary to (w)get the pkg-get  wget http://blastwave.network.com/csw/pkg_get.pkg\npkgadd -d pkg_get.pkg  #install the package\nvi /opt/csw/etc/pkg-get.conf  #edit the http proxy value and enable the export proxy directive\npkg-get -U #download the latest list of packages and their interdependencies\npkg-get -i wget #install the full version of wget  Now for the big install...  pkg-get -i nrpe # this will require many dependant packages, just say yes to all .  once this a has finished you will have an installation of nrpe and all the plugins.  you can test the plugins by running them directly  /opt/csw/libexec/nagios-plugins/check_load -w 15,10,5 -c 30,25,20\n/opt/csw/libexec/nagios-plugins/check_dummy 0  If this works fine, you can then configure the nrpe daemon  about the only changes you will have to /opt/csw/etc/nrpe.cfg are to allow the nrpe daemon to accept connections from the nagios server  allowed_hosts=127.0.0.1,192.168.125.42  and to allow command argument processing  dont_blame_nrpe=0 # set to 1 if you want to enable  you can now start the daemon manually by running  /opt/csw/bin/nrpe -d -c /opt/csw/etc/nrpe.cfg  then from a remote host check the daemon by running one of the command that is enabled in the target hosts nrpe.cfg file eg....  /usr/lib/nagios/plugins/check_nrpe -H {target host}  -c check_load -a -w  1,1,1,2,2,2  Once this is working kill the daemon and set it up via inet.d   add the following line to /etc/inetd.conf  nrpe    stream  tcp     nowait  nagios  /opt/csw/bin/nrpe       /opt/csw/bin/nrpe -i -c /opt/csw/etc/nrpe.cfg  add the following line to /etc/services  nrpe    5666/tcp  force the inet daemon to reread the config files  pkill -HUP inetd  run the remote check again  End of Client install section", 
            "title": "Neither SUN or the Nagios team provide runtime binary's."
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Extend_lvm_filesystem/", 
            "text": "LVM tips and Hints\n\n\nResizing (extending) Logical Volumes and filesystems\n\n\nLive LogicalVolume and Filesystem  extension in a single step (-r)\n\n\nDetermine the free space/extents within the Volume Group\n\n\nvgdisplay {vgname} | grep Free\n\n\n\nDecide if you want to declare the size in relative or absolute terms\n\n\nlvextend -r /dev/vgroot/lvtmp -L 30G         # set size to 30GB\n      or\nlvextend -r /dev/vgroot/lvtmp -L +5G         # increase size by 5GB\n      or\nlvextend -r /dev/vgroot/lvtmp -l +100%FREE   # increase the LV to use up all free extents within the VG\n\n\n\nLive LogicalVolume extension with a separate step to extend the filesystem\n\n\nDetermine the free space/extents within the Volume Group\n\n\nvgdisplay  | grep Free\n\n\n\nExtend the Logical Volume\n\n\nlvextend -L +100MB /dev/vgsys/tmp          # Increase  size by 100MB\n\n\n\nExtend the Filesystem to fill the logical Volume\n\n\nresize2fs /dev/vgsys/tmp                   # optional size parameter\n\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 1\nPerforming an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks.\nThe filesystem on /dev/vgsys/tmp is now 557056 blocks long.\n\n\n\n\n\nReconfigure swap\n\n\nexpanding swap\n\n\nswapoff -a                                   # turn swap off\n\nlvextend  /dev/vgroot/lvswap -L 30G          # set size to 30GB\n      or\nlvextend  /dev/vgroot/lvswap -L +5G          # increase size by 5GB\n      or\nlvextend  /dev/vgroot/lvswap -l +100%FREE    # increase the LV to use up all free extents within the VG\n\nmkswap /dev/vgroot/lvswap                    # recreate the swap filesystem\n\nswapon -a                                    # turn swap on\n\n\n\nremoving swap (to freee up disk for filesystem expansion)\n\n\nswapoff -a\n\n\n\nEdit /etc/fstab \n remove the swap line, to stop it from activating upon next boot\n\n\n/dev/vgsys/swap         swap      swap    defaults        0 0\n\n\n\nRemove the swap logical volume\n\n\nlvremove /dev/vgroot/lvswap\n\n\n\nnow you can use the swap space for something else\n\n\nturn the swap back on\n\n\nswapon -a\n\n\n\nshow the swap\n\n\nswapon -s\n\nfree -g", 
            "title": "Extend lvm filesystem"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Extend_lvm_filesystem/#lvm-tips-and-hints", 
            "text": "Resizing (extending) Logical Volumes and filesystems  Live LogicalVolume and Filesystem  extension in a single step (-r)  Determine the free space/extents within the Volume Group  vgdisplay {vgname} | grep Free  Decide if you want to declare the size in relative or absolute terms  lvextend -r /dev/vgroot/lvtmp -L 30G         # set size to 30GB\n      or\nlvextend -r /dev/vgroot/lvtmp -L +5G         # increase size by 5GB\n      or\nlvextend -r /dev/vgroot/lvtmp -l +100%FREE   # increase the LV to use up all free extents within the VG  Live LogicalVolume extension with a separate step to extend the filesystem  Determine the free space/extents within the Volume Group  vgdisplay  | grep Free  Extend the Logical Volume  lvextend -L +100MB /dev/vgsys/tmp          # Increase  size by 100MB  Extend the Filesystem to fill the logical Volume  resize2fs /dev/vgsys/tmp                   # optional size parameter\n\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 1\nPerforming an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks.\nThe filesystem on /dev/vgsys/tmp is now 557056 blocks long.   Reconfigure swap  expanding swap  swapoff -a                                   # turn swap off\n\nlvextend  /dev/vgroot/lvswap -L 30G          # set size to 30GB\n      or\nlvextend  /dev/vgroot/lvswap -L +5G          # increase size by 5GB\n      or\nlvextend  /dev/vgroot/lvswap -l +100%FREE    # increase the LV to use up all free extents within the VG\n\nmkswap /dev/vgroot/lvswap                    # recreate the swap filesystem\n\nswapon -a                                    # turn swap on  removing swap (to freee up disk for filesystem expansion)  swapoff -a  Edit /etc/fstab   remove the swap line, to stop it from activating upon next boot  /dev/vgsys/swap         swap      swap    defaults        0 0  Remove the swap logical volume  lvremove /dev/vgroot/lvswap  now you can use the swap space for something else  turn the swap back on  swapon -a  show the swap  swapon -s\n\nfree -g", 
            "title": "LVM tips and Hints"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/", 
            "text": "Resizing (extending) Logical Volumes and filesystems\n\n\nLive LogicalVolume and Filesystem  extension in a single step (-r)\n\n\nDetermine the free space/extents withing the Volume Group\n\n\nvgdisplay {vgname} | grep Free\n\n\n\nDecide if you want to declare the size in relative or absolute terms\n\n\nlvextend -r /dev/vgroot/lvtmp -L 30G         # set size to 30GB\n      or\nlvextend -r /dev/vgroot/lvtmp -L +5G         # increase size by 5GB\n      or\nlvextend -r /dev/vgroot/lvtmp -l +100%FREE   # increase the LV to use up all free extents within the VG\n\n\n\nLive LogicalVolume extension with a separate step to extend the filesystem\n\n\nDetermine the free space/extents within the Volume Group\n\n\nvgdisplay  | grep Free\n\n\n\nExtend the Logical Volume\n\n\nlvextend -L +100MB /dev/vgsys/tmp          # Increase  size by 100MB\n\n\n\nExtend the Filesystem to fill the logical Volume\n\n\nresize2fs /dev/vgsys/tmp                   # optional size parameter\n\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 1\nPerforming an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks.\nThe filesystem on /dev/vgsys/tmp is now 557056 blocks long.\n\n\n\n\n\nReconfigure swap\n\n\nexpanding swap\n\n\nswapoff -a                                   # turn swap off\n\nlvextend  /dev/vgroot/lvswap -L 30G          # set size to 30GB\n      or\nlvextend  /dev/vgroot/lvswap -L +5G          # increase size by 5GB\n      or\nlvextend  /dev/vgroot/lvswap -l +100%FREE    # increase the LV to use up all free extents within the VG\n\nmkswap /dev/vgroot/lvswap                    # recreate the swap filesystem\n\nswapon -a                                    # turn swap on\n\n\n\nremoving swap (to free up disk for filesystem expansion)\n\n\nswapoff -a\n\n\n\nEdit /etc/fstab \n remove the swap line, to stop it from activating upon next boot\n\n\n/dev/vgsys/swap         swap      swap    defaults        0 0\n\n\n\nRemove the swap logical volume\n\n\nlvremove /dev/vgroot/lvswap\n\n\n\nnow you can use the swap space for something else\n\n\nturn the swap back on\n\n\nswapon -a\n\n\n\nshow the swap\n\n\nswapon -s\n\nfree -g\n\n\n\nShrinking a linux filesystem on LVM\n\n\n\n\n\n\n\n\nOnline reduce/shrink\n\n\n1. Unmount filesystem\n\n\nif you cant unmount the filesystem on the running system see the offline section below\n\n\numount /dev/vg01/lvs-02\n\n\n\n2. Perform a check of the filesystem...\n\n\ne2fsck -f /dev/vg01/lvs-02\n\n\n\n3. Resize the filesystem\n\n\nresize2fs /dev/vg01/lvs-02 15G\n\n\n\n2. Reduce the size of the logical volume.\n\n\nlvreduce -L -15G /dev/vg01/lvs-02\n\n\n\n# Optional: you can also shrink down the volume group if desired.\n\n\nvgreduce vg01 /dev/sdxy\n\n\n\n\n\n\n\n\n\nOffline reduce/shrink root filesystem\n\n\n1. boot from rescue media\n\n\nif san connected append \"linux mpath\" to boot options\n\n\nDo not mount filesystems when booting from rescue media\n\n\nselect the menu option that skips the mounting of your filesyetsms\n\n\n2. Identify the Volume group\n\n\n/usr/sbin/lvm vgscan\n/usr/sbin/lvm vgdisplay\n\n\n\nnote: you have to run lvm {operand}... instead of the aliases lvscan\n\n\n3. Activate the the volume group\n\n\n/usr/sbin/lvm vgchange {VG-name} -a y\n\n\n\n4. Perform a check of the filesystem...\n\n\n/usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02\n\n\n\n5. Resize the filesystem\n\n\nresize2fs /dev/vg01/lvs-02 15G\n\n\n\n6. Reduce the size of the logical volume.\n\n\n/usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02\n\n\n\n# Optional: you can also shrink down the volume group if desired.\n\n\n/usr/sbin/lvm vgreduce vg01 /dev/sdxy\n\n\n\nExample: reducing the size of a swap volume\n\n\n[root@server ~]# swapoff -a\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 20G\n  Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda\n  New size given (640 extents) not less than existing size (625 extents)\n  Run `lvreduce --help' for more information.\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 10G\n  Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda\n  WARNING: Reducing active logical volume to 10.00 GB\n  THIS MAY DESTROY YOUR DATA (filesystem etc.)\nDo you really want to reduce swap? [y/n]: y\n  Reducing logical volume swap to 10.00 GB\n  Logical volume swap successfully resized\n\n[root@server ~]# mkswap /dev/vgsys/swap\nSetting up swapspace version 1, size = 10737414 kB\n\n[root@server ~]# swapon -a\n\n[root@server ~]# free\n             total       used       free     shared    buffers     cached\nMem:      65931888     527968   65403920          0      32036     343660\n-/+ buffers/cache:     152272   65779616\nSwap:     10485752          0   10485752", 
            "title": "Linux LVM Logical Volume Manager"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#resizing-extending-logical-volumes-and-filesystems", 
            "text": "Live LogicalVolume and Filesystem  extension in a single step (-r)  Determine the free space/extents withing the Volume Group  vgdisplay {vgname} | grep Free  Decide if you want to declare the size in relative or absolute terms  lvextend -r /dev/vgroot/lvtmp -L 30G         # set size to 30GB\n      or\nlvextend -r /dev/vgroot/lvtmp -L +5G         # increase size by 5GB\n      or\nlvextend -r /dev/vgroot/lvtmp -l +100%FREE   # increase the LV to use up all free extents within the VG  Live LogicalVolume extension with a separate step to extend the filesystem  Determine the free space/extents within the Volume Group  vgdisplay  | grep Free  Extend the Logical Volume  lvextend -L +100MB /dev/vgsys/tmp          # Increase  size by 100MB  Extend the Filesystem to fill the logical Volume  resize2fs /dev/vgsys/tmp                   # optional size parameter\n\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vgsys/tmp is mounted on /tmp; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 1\nPerforming an on-line resize of /dev/vgsys/tmp to 557056 (4k) blocks.\nThe filesystem on /dev/vgsys/tmp is now 557056 blocks long.", 
            "title": "Resizing (extending) Logical Volumes and filesystems"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#reconfigure-swap", 
            "text": "expanding swap  swapoff -a                                   # turn swap off\n\nlvextend  /dev/vgroot/lvswap -L 30G          # set size to 30GB\n      or\nlvextend  /dev/vgroot/lvswap -L +5G          # increase size by 5GB\n      or\nlvextend  /dev/vgroot/lvswap -l +100%FREE    # increase the LV to use up all free extents within the VG\n\nmkswap /dev/vgroot/lvswap                    # recreate the swap filesystem\n\nswapon -a                                    # turn swap on", 
            "title": "Reconfigure swap"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#removing-swap-to-free-up-disk-for-filesystem-expansion", 
            "text": "swapoff -a  Edit /etc/fstab   remove the swap line, to stop it from activating upon next boot  /dev/vgsys/swap         swap      swap    defaults        0 0  Remove the swap logical volume  lvremove /dev/vgroot/lvswap  now you can use the swap space for something else  turn the swap back on  swapon -a  show the swap  swapon -s\n\nfree -g", 
            "title": "removing swap (to free up disk for filesystem expansion)"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#shrinking-a-linux-filesystem-on-lvm", 
            "text": "", 
            "title": "Shrinking a linux filesystem on LVM"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#online-reduceshrink", 
            "text": "1. Unmount filesystem  if you cant unmount the filesystem on the running system see the offline section below  umount /dev/vg01/lvs-02  2. Perform a check of the filesystem...  e2fsck -f /dev/vg01/lvs-02  3. Resize the filesystem  resize2fs /dev/vg01/lvs-02 15G  2. Reduce the size of the logical volume.  lvreduce -L -15G /dev/vg01/lvs-02  # Optional: you can also shrink down the volume group if desired.  vgreduce vg01 /dev/sdxy", 
            "title": "Online reduce/shrink"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#offline-reduceshrink-root-filesystem", 
            "text": "1. boot from rescue media  if san connected append \"linux mpath\" to boot options  Do not mount filesystems when booting from rescue media  select the menu option that skips the mounting of your filesyetsms  2. Identify the Volume group  /usr/sbin/lvm vgscan\n/usr/sbin/lvm vgdisplay  note: you have to run lvm {operand}... instead of the aliases lvscan  3. Activate the the volume group  /usr/sbin/lvm vgchange {VG-name} -a y  4. Perform a check of the filesystem...  /usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02  5. Resize the filesystem  resize2fs /dev/vg01/lvs-02 15G  6. Reduce the size of the logical volume.  /usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02  # Optional: you can also shrink down the volume group if desired.  /usr/sbin/lvm vgreduce vg01 /dev/sdxy", 
            "title": "Offline reduce/shrink root filesystem"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Linux_LVM_Logical_Volume_Manager/#example-reducing-the-size-of-a-swap-volume", 
            "text": "[root@server ~]# swapoff -a\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 20G\n  Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda\n  New size given (640 extents) not less than existing size (625 extents)\n  Run `lvreduce --help' for more information.\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 10G\n  Found duplicate PV asdfgpGQfwBCWprZvax4wy: using /dev/sdq not /dev/sda\n  WARNING: Reducing active logical volume to 10.00 GB\n  THIS MAY DESTROY YOUR DATA (filesystem etc.)\nDo you really want to reduce swap? [y/n]: y\n  Reducing logical volume swap to 10.00 GB\n  Logical volume swap successfully resized\n\n[root@server ~]# mkswap /dev/vgsys/swap\nSetting up swapspace version 1, size = 10737414 kB\n\n[root@server ~]# swapon -a\n\n[root@server ~]# free\n             total       used       free     shared    buffers     cached\nMem:      65931888     527968   65403920          0      32036     343660\n-/+ buffers/cache:     152272   65779616\nSwap:     10485752          0   10485752", 
            "title": "Example: reducing the size of a swap volume"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Listing_Oracle_ASM_disks/", 
            "text": "listing disks in oracle grid cluster\n\n\n[root@server ~]# ls -l /dev/oracleasm/disks/\ntotal 0\nbrw-rw---- 1 grid oinstall 253, 31 Jun 19 18:25 VOLNP09\nbrw-rw---- 1 grid oinstall 253, 18 Jun 19 18:25 VOLNP10\nbrw-rw---- 1 grid oinstall 253, 19 Jun 19 18:25 VOLNP11\nbrw-rw---- 1 grid oinstall 253, 22 Jun 19 18:25 VOLNP12\n\n[root@server ~]# oracleasm listdisks\nVOLNP09\nVOLNP10\nVOLNP11\nVOLNP12", 
            "title": "Listing Oracle ASM disks"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Listing_Oracle_ASM_disks/#listing-disks-in-oracle-grid-cluster", 
            "text": "[root@server ~]# ls -l /dev/oracleasm/disks/\ntotal 0\nbrw-rw---- 1 grid oinstall 253, 31 Jun 19 18:25 VOLNP09\nbrw-rw---- 1 grid oinstall 253, 18 Jun 19 18:25 VOLNP10\nbrw-rw---- 1 grid oinstall 253, 19 Jun 19 18:25 VOLNP11\nbrw-rw---- 1 grid oinstall 253, 22 Jun 19 18:25 VOLNP12\n\n[root@server ~]# oracleasm listdisks\nVOLNP09\nVOLNP10\nVOLNP11\nVOLNP12", 
            "title": "listing disks in oracle grid cluster"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Multipath_and_NetApp_fixes/", 
            "text": "Fixing the inconsistent path ordering when using friendly_names and a separate var partition on RHEL 5\n\n\ncause\n\n\n\n\nRHEL 5 multipath bindings file is located in var\n\n\n/var isn't mounted before multipath is configured\n\n\nredhat does not include /var/lib/ in the initial ram disk which breaks multipath on boot\n\n\n\n\nsolution (work around)\n\n\ndont forget to comment out \nvi /etc/multipath.conf\n\n\nadd the following lines to multipath config to set netapp specific settings\n\n\ndefaults {\n    user_friendly_names yes\n    bindings_file /etc/multipath/bindings ##changed \n    flush_on_last_del       yes\n    max_fds max\n    pg_prio_calc    avg\n    queue_without_daemon    no\n}\n\n\ndevices {\n\n    device {\n            vendor                  \"NETAPP\"\n            product                 \"LUN\"\n            path_checker            tur\n            path_selector           \"round-robin 0\"\n            getuid_callout          \"/sbin/scsi_id -g -u -s /block/%n\"\n#           prio_callout            \"/sbin/mpath_prio_ontap /dev/%n\"\n            prio_callout            \"/sbin/mpath_prio_alua /dev/%n\"\n#           features                \"1 queue_if_no_path\"\n            features                \"3 queue_if_no_path pg_init_retries 50\"\n#           hardware_handler        \"0\"\n            hardware_handler        \"1 alua\"\n            path_grouping_policy    group_by_prio\n            failback                immediate\n            rr_weight               uniform\n            rr_min_io               128\n    }\n\n}\n\n\n\n\nmkdir /etc/multipath\ncp /var/lib/multipath/bindings /etc/multipath/bindings\ncd /boot\ncp initrd*img initrd*img.multipath-bak\nmkinitrd -f initrd-`uname -r`.img `uname -r`\nls -ltr  #make sure the file is correct size there seems to be a bug where it intermittently creates a initrd of half the size", 
            "title": "Multipath and NetApp fixes"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/Online_lun_resize/", 
            "text": "Increase the Lun size on the SAN or VM Hypervisor\n\n\ndmesg | tail\n\n    sdb: Write Protect is off\n    sdb: Mode Sense: 61 00 00 00\n    sdb: cache data unavailable\n    sdb: assuming drive cache: write through\n    sdb: detected capacity change from 214748364800 to 429496729600\n\n[root@server ~]# pvscan\n      PV /dev/sdb    VG vgdata    lvm2 [199.97 GB / 0    free]\n      PV /dev/sda2   VG vgsys     lvm2 [49.75 GB / 5.81 GB free]\n      Total: 2 [249.72 GB] / in use: 2 [249.72 GB] / in no VG: 0 [0   ]\n\n[root@server ~]# pvresize  /dev/sdb\n      Physical volume \"/dev/sdb\" changed\n      1 physical volume(s) resized / 0 physical volume(s) not resized\n\n[root@server ~]# pvs\n      PV         VG      Fmt  Attr PSize   PFree\n      /dev/sda2  vgsys   lvm2 a--   49.75G   5.81G\n      /dev/sdb   vgdata0 lvm2 a--  399.97G 200.00G\n\n\n[root@server ~]# lvextend -r -l +100%FREE /dev/vgdata/lv-data\n      Extending logical volume opt_splunk_var_lib_splunk to 399.97 GB\n      Logical volume opt_splunk_var_lib_splunk successfully resized\n    resize4fs 1.41.12 (17-May-2010)\n    Filesystem at /dev/mapper/vgdata-lvdata is mounted on /mnt/data on-line resizing required\n    old desc_blocks = 13, new_desc_blocks = 25\n    Performing an on-line resize of /dev/mapper/vgdata-lvdata to 104849408 (4k) blocks.\n    The filesystem on /dev/mapper/vgdata-lvdata is now 104849408 blocks long.\n\n[root@server ~]# df -h\n    Filesystem            Size  Used Avail Use% Mounted on\n    /dev/mapper/vgsys-root\n                           20G  3.7G   15G  20% /\n    /dev/sda1             244M   49M  183M  21% /boot\n    tmpfs                 1.9G     0  1.9G   0% /dev/shm\n    /dev/mapper/vgdata/lvdata\n                          394G  143G  232G  39% /mnt/data", 
            "title": "Online lun resize"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/RHEL_floppy_kickstart/", 
            "text": "Working with Floppy disk images\n\n\nCreate the image:-\n\n\ndd if=/dev/zero of=floppy.img bs=1K count=1440 \nmkdosfs -F 12 floppy.img \n# or \n/sbin/mkfs.msdos -C  floppy.img 1440\n\n\n\nCheck the image:-\n\n\nmount -o loop -t msdos dosfloppy /mnt/floppy\ndf -h  /mnt/floppy\numount /mnt/floppy\n\n\n\nCopy files to the image:-\n\n\nmount -o loop -t msdos dosfloppy /mnt/floppy\ncp file.txt /mnt/floppy\numount /mnt/floppy\n\n\n\nKeep in mind file names are limited to 8.3 case insensitive chars\n\n\nhttp://en.wikipedia.org/wiki/8.3_filename\n\n\nThe Floppy image contains", 
            "title": "RHEL floppy kickstart"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/boot_cdrom_dvdrom/", 
            "text": "#!/bin/bash\n# This script will create a bootable ISO9660 image\n# This script has to be run one directory below the cdrom source directory\n#\n#you can  modify the kickstart config file to suit your preferences\n#\n\n# this is the name of the outputted ISO file\nOUT=\"RHEL4u3\"\n#TARGET=\"/home/seamus/$VER\"\nDATE=`date +%Y%m%d\"_\"%H%M`\n\n\n#-V \"$VER\" \\\n\nmkisofs -o $OUT.iso \\\n-b isolinux/isolinux.bin \\\n-c isolinux/boot.cat \\\n-no-emul-boot -boot-load-size 4 -boot-info-table \\\n-R -J -v -T ./bootdisk/", 
            "title": "Boot cdrom dvdrom"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/", 
            "text": "disk/partition/filesystem imaging with dd, squashfs, gzip, losetup and avfs\n\n\n--To increase the compression ratio of the image, zero the free space in the filesystems before creating the initial image.--\n\n\nPrepare the source filesystems.......\n\n\nWindows\n\n\n\n\ndelete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/\n\n\ndefrag to reorder the bits... I just use the microsoft built in tool\n\n\nzero the free space...I use sdelete -z from sysinternals  http://download.sysinternals.com/files/SDelete.zip\n\n\n\n\nLinux\n\n\n\n\ndelete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/\n\n\ndefrag if you think it is necessary... e4defrag\n\n\nzero the free space...\n    $ dd if=/dev/zero of=/{mounted filesystem}/tempfile bs=1M\n\n\nremove the temporary zero file\n\n\n\n\n\n\n\n\nMake block copies of the filesystems using dd\n\n\nIndividual Partition image\n\n\n$ dd if=/dev/sdb1 of=/backups/sdb1.dd.img\n\n\n\nWhole disk image\n\n\n$ dd if=/dev/sdb of=/backups/sdb.dd.img\n\n\n\nCompressing the image to save space\n       -- note if you never plan on mounting the image to retrieve individual files you can pipe the dd through gzip\n\n\n$ dd if=/dev/sdb1 bs=1M | gzip \n /backups/sdb1.dd.img.gz\n\n   or you can gzip later\n\n$ dd if=/dev/sdb1 bs=1M of=/backups/sdb1.dd.img \n gzip /backups/sdb1.dd.img\n\n\n\ndd does not give you any progress indication.\n\n\nyou can however send the dd process a kill -USR1  signal and it will output its progress to std out.\n\n\nNote: you will have to send this signal from another terminal\n\n\nps | grep dd  \n kill -USR1 {pid}\nor \nkill -USR1 `ps -aef | grep \"dd\\ if\" | awk '{print $2}'\n\n\n\nyou should see output similar to the following in the terminal that is running the dd command\n\n\n12917407744 bytes (13 GB) copied, 511.9 s, 25MB/s\n\n\n\nMake file system copy using squashfs.\n\n\nthe advantages of squashfs over dd are..\n\n\n\n\nthe compression is multithreaded so its faster on multicore machines\n\n\n\n\neasy to mount without having to decompress the whole image.\n\n\ndd if=/dev/sdb1 | gzip \n /backups/sdb1.dd.img\n\n\n\n\n\n\nThe disadvantage of squashfs over dd is you cant use it to archive a whole disk including the partition table and boot block etc..x\n\n\n\n\nMounting and restoring from the disk/filesytems images..\n\n\nmounting the mksquashfs image\n\n\n$ mount /backups/sdb1.dd.img.sqfs /mnt/sdb1-squashfs\n\n\n\nmounting the Partition image\n\n\n$ mount /backups/sdb1.dd.img.sqfs/sdb1.dd.img sdb1-img\n\n\n\nmounting a whole disk image you will have to use kpartx to create the the device nodes\n\n\n$ kpartx -a /backup/sdb1.dd.img.sqfs/sdb.dd.img\n   --then mount the individual file systems, the device node will be in /dev/mapper/loop\nmount /dev/mapper/loop1p1 /mnt/sdb1\n\n\n\nmounting a single partition from the disk image\n\n\n--use parted to display the partition table in Bytes--\n    $ parted /backups/sdb.dd.img unit B print\n    Model:  (file)\n    Disk /backups/sdb.dd.img: 268435456B\n    Sector size (logical/physical): 512B/512B\n    Partition Table: msdos\n\n\nNumber  Start      End         Size        Type      File system  Flags\n 1      1024B      99980287B   99979264B   primary   ext4         boot\n 2      99980288B  268435455B  168455168B  extended\n 5      99981312B  268435455B  168454144B  logical   ext4\n\n\n\n--specify and offset to the mount command--\n    $ mount -o loop,offset=99981312 /backups/sdb.dd.img /media/sdb5\n--if there are file systems issues you may need to mount with the options -o ro,noload--\n\n\nusing avf and losetup to mount a gzipped dd partition image\n\n\ninstall http://sourceforge.net/projects/avf/\n\n\ncreate a virtual filesystem\n\n\n$ mountavfs \nMounting AVFS on /root/.avfs...\n\n\n\nthere is now a pseudo-fs replica of the real file system in /root/.avfs\n\n\n$ ls /root/.avfs/\nbin/        etc/        lib64/      mnt/        root/       selinux/    tmp/\nboot/       home/       lost+found/ opt/        run/        srv/        usr/\ndev/        lib/        media/      proc/       sbin/       sys/        var/\n/backups/\n\n$ cd /root/.avfs/backups/\n256/      256sqash/ guess/    hdb1/     loop1p2/  loop2/    sdb5/\n256MBimg/ avfs/     gziped/   loop1p1/  loop1p5/  sdb1/     sqfs/\n\n$ ls\n256       gziped   loop2             sdb1.tgz          sdb5.sqfs                     sdb.dd.img\n256MBimg  hdb1     sdb1              sdb5              sdb5.tgz                      sdb.dd.img.sqfs\n256sqash  loop1p1  sdb1.dd.img       sdb5.dd.img       sdb5.zero-filled.dd.img       sdb.zero-filled.dd.img\navfs      loop1p2  sdb1.dd.img.sqfs  sdb5.dd.img.gz    sdb5.zero-filled.dd.img.gz    sdb.zero-filled.dd.img.sqfs\nguess     loop1p5  sdb1.sqfs         sdb5.dd.img.sqfs  sdb5.zero-filled.dd.img.sqfs  sqfs\n\n$ losetup /dev/loop2 sdb5.gz#\n--the hash at the end is needed, it is short hand for losetup /dev/loop2 sdb5.gz#gunzip\n\n$ mount /dev/loop2 gziped\nmount: block device /dev/loop2 is write-protected, mounting read-only\nmount: wrong fs type, bad option, bad superblock on /dev/loop2,\n       missing codepage or helper program, or other error\n       In some cases useful info is found in syslog - try\n       dmesg | tail  or so\n\n$ mount -o ro,noload /dev/loop2 gziped\n\n$ df\nFilesystem     1K-blocks    Used Available Use% Mounted on\n/dev/sda1       14317616 6159480   7424184  46% /\nnone                   4       0         4   0% /sys/fs/cgroup\nudev             1008392       4   1008388   1% /dev\ntmpfs             205120     936    204184   1% /run\nnone                5120       0      5120   0% /run/lock\nnone             1025600      76   1025524   1% /run/shm\nnone              102400      36    102364   1% /run/user\n/dev/loop2        159304    5646    145433   4% /root/.avfs/media/gziped", 
            "title": "Disk filesystem partition imaging using squashfs and dd"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/disk_filesystem_partition_imaging_using_squashfs_and_dd/#diskpartitionfilesystem-imaging-with-dd-squashfs-gzip-losetup-and-avfs", 
            "text": "--To increase the compression ratio of the image, zero the free space in the filesystems before creating the initial image.--  Prepare the source filesystems.......  Windows   delete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/  defrag to reorder the bits... I just use the microsoft built in tool  zero the free space...I use sdelete -z from sysinternals  http://download.sysinternals.com/files/SDelete.zip   Linux   delete any superfluous files... I use bleachbit http://bleachbit.sourceforge.net/  defrag if you think it is necessary... e4defrag  zero the free space...\n    $ dd if=/dev/zero of=/{mounted filesystem}/tempfile bs=1M  remove the temporary zero file     Make block copies of the filesystems using dd  Individual Partition image  $ dd if=/dev/sdb1 of=/backups/sdb1.dd.img  Whole disk image  $ dd if=/dev/sdb of=/backups/sdb.dd.img  Compressing the image to save space\n       -- note if you never plan on mounting the image to retrieve individual files you can pipe the dd through gzip  $ dd if=/dev/sdb1 bs=1M | gzip   /backups/sdb1.dd.img.gz\n\n   or you can gzip later\n\n$ dd if=/dev/sdb1 bs=1M of=/backups/sdb1.dd.img   gzip /backups/sdb1.dd.img  dd does not give you any progress indication.  you can however send the dd process a kill -USR1  signal and it will output its progress to std out.  Note: you will have to send this signal from another terminal  ps | grep dd    kill -USR1 {pid}\nor \nkill -USR1 `ps -aef | grep \"dd\\ if\" | awk '{print $2}'  you should see output similar to the following in the terminal that is running the dd command  12917407744 bytes (13 GB) copied, 511.9 s, 25MB/s  Make file system copy using squashfs.  the advantages of squashfs over dd are..   the compression is multithreaded so its faster on multicore machines   easy to mount without having to decompress the whole image.  dd if=/dev/sdb1 | gzip   /backups/sdb1.dd.img    The disadvantage of squashfs over dd is you cant use it to archive a whole disk including the partition table and boot block etc..x   Mounting and restoring from the disk/filesytems images..  mounting the mksquashfs image  $ mount /backups/sdb1.dd.img.sqfs /mnt/sdb1-squashfs  mounting the Partition image  $ mount /backups/sdb1.dd.img.sqfs/sdb1.dd.img sdb1-img  mounting a whole disk image you will have to use kpartx to create the the device nodes  $ kpartx -a /backup/sdb1.dd.img.sqfs/sdb.dd.img\n   --then mount the individual file systems, the device node will be in /dev/mapper/loop\nmount /dev/mapper/loop1p1 /mnt/sdb1  mounting a single partition from the disk image  --use parted to display the partition table in Bytes--\n    $ parted /backups/sdb.dd.img unit B print\n    Model:  (file)\n    Disk /backups/sdb.dd.img: 268435456B\n    Sector size (logical/physical): 512B/512B\n    Partition Table: msdos  Number  Start      End         Size        Type      File system  Flags\n 1      1024B      99980287B   99979264B   primary   ext4         boot\n 2      99980288B  268435455B  168455168B  extended\n 5      99981312B  268435455B  168454144B  logical   ext4  --specify and offset to the mount command--\n    $ mount -o loop,offset=99981312 /backups/sdb.dd.img /media/sdb5\n--if there are file systems issues you may need to mount with the options -o ro,noload--  using avf and losetup to mount a gzipped dd partition image  install http://sourceforge.net/projects/avf/  create a virtual filesystem  $ mountavfs \nMounting AVFS on /root/.avfs...  there is now a pseudo-fs replica of the real file system in /root/.avfs  $ ls /root/.avfs/\nbin/        etc/        lib64/      mnt/        root/       selinux/    tmp/\nboot/       home/       lost+found/ opt/        run/        srv/        usr/\ndev/        lib/        media/      proc/       sbin/       sys/        var/\n/backups/\n\n$ cd /root/.avfs/backups/\n256/      256sqash/ guess/    hdb1/     loop1p2/  loop2/    sdb5/\n256MBimg/ avfs/     gziped/   loop1p1/  loop1p5/  sdb1/     sqfs/\n\n$ ls\n256       gziped   loop2             sdb1.tgz          sdb5.sqfs                     sdb.dd.img\n256MBimg  hdb1     sdb1              sdb5              sdb5.tgz                      sdb.dd.img.sqfs\n256sqash  loop1p1  sdb1.dd.img       sdb5.dd.img       sdb5.zero-filled.dd.img       sdb.zero-filled.dd.img\navfs      loop1p2  sdb1.dd.img.sqfs  sdb5.dd.img.gz    sdb5.zero-filled.dd.img.gz    sdb.zero-filled.dd.img.sqfs\nguess     loop1p5  sdb1.sqfs         sdb5.dd.img.sqfs  sdb5.zero-filled.dd.img.sqfs  sqfs\n\n$ losetup /dev/loop2 sdb5.gz#\n--the hash at the end is needed, it is short hand for losetup /dev/loop2 sdb5.gz#gunzip\n\n$ mount /dev/loop2 gziped\nmount: block device /dev/loop2 is write-protected, mounting read-only\nmount: wrong fs type, bad option, bad superblock on /dev/loop2,\n       missing codepage or helper program, or other error\n       In some cases useful info is found in syslog - try\n       dmesg | tail  or so\n\n$ mount -o ro,noload /dev/loop2 gziped\n\n$ df\nFilesystem     1K-blocks    Used Available Use% Mounted on\n/dev/sda1       14317616 6159480   7424184  46% /\nnone                   4       0         4   0% /sys/fs/cgroup\nudev             1008392       4   1008388   1% /dev\ntmpfs             205120     936    204184   1% /run\nnone                5120       0      5120   0% /run/lock\nnone             1025600      76   1025524   1% /run/shm\nnone              102400      36    102364   1% /run/user\n/dev/loop2        159304    5646    145433   4% /root/.avfs/media/gziped", 
            "title": "disk/partition/filesystem imaging with dd, squashfs, gzip, losetup and avfs"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/filesystem_shuffle/", 
            "text": "save the script below to a file on the root filesystem\n\n\nreboot into bash    init=/bin/bash\n\n\nexecute the script\n\n\n\n\nThe script will exit if any of the steps fail\n\n\n#!/bin/bash\nfail_notify() {\n echo \"step \"$@\" on line $BASH_LINENO failed \n\"\n exit\n}\n\ngrep 'vgsys-var_log[[:blank:]]' /etc/fstab\nif [ $? == 0 ]\n   then echo \"/var/log filesystem already exists\"\nelse\n   start_udev || fail_notify \"start_udev\"\n   mount -o rw,remount / || fail_notify \"remount of root\"\n   vgchange -ay vgsys || fail_notify \"activate vgsys\"\n   lvchange -ay /dev/vgsys/var || fail_notify \" activate lv var\"\n   vgscan --mknodes -v || fail_notify \"creating device nodes\"\n   lvcreate vgsys -L 2G -n var_log || fail_notify \"lvcreate\"\n   mkfs.ext4 /dev/vgsys/var_log || fail_notify \"mkfs\"\n   mkdir /tmp/var_log # no test as the directory may already exist if rerunning script\n   mount /dev/vgsys/var_log /tmp/var_log || fail_notify \"mount\"\n   mount /var || fail_notify \"mount var\"\n   cd /var/log/ ||   fail_notify \"cd /var/log \"\n   echo \"copying files from /var/log/ to new file system /tmp/var_log/\"\n   find . -mount -print0 | cpio -0dump /tmp/var_log/ || fail_notofy \"find\"\n   # need to put this line before  /var/log/audit mount point otherwise the  the file systems will overlap\n   sed -i '/vgsys-var_log_audit/i \\/dev\\/mapper\\/vgsys\\-var\\_log\\ \\/var\\/log\\             \\ ext4  \\ nodev\\,nosuid\\,noexec      \\ 1\\ 2' /etc/fstab  || fail notify \" fstab update\"\n   df # display a df to the user so they can check what has happened\n   echo \"remounting filesystems as read only\"\n   mount -o ro,remount /tmp/var_log || fail_notify \"remount of var_log\"\n   mount -o ro,remount /var || fail_notify \"remount of var\"\n   mount -o ro,remount / || fail_notify \"remount of /\"\n   echo \"+--------------------------+\"\n   echo \"| power cycle vm to finish |\"\n   echo \"| or execute \"exec init 6\" |\"\n   echo \"+--------------------------+\"\nfi", 
            "title": "Filesystem shuffle"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/rsync/", 
            "text": "rsync coping files from a source directory that doesn't exits in the destination directory, note this does not perform checksums and does not compare date statmps etc. Its handy for copying original data like photos imported from a camera\n\n\nrsync -zhr --ignore-existing --progress  --dry-run /source/ /dest/ | more", 
            "title": "Rsync"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/shrink_lvm_filesystem/", 
            "text": "Shrinking a Linux filesystem on LVM\n\n\n\n\n\n\n\n\nOnline reduce/shrink\n\n\n1. Unmount filesystem\n\n\nif you cant unmount the filesystem on the running system see the offline section below\n\n\numount /dev/vg01/lvs-02\n\n\n\n2. Perform a check of the filesystem...\n\n\ne2fsck -f /dev/vg01/lvs-02\n\n\n\n3. Resize the filesystem\n\n\nresize2fs /dev/vg01/lvs-02 15G\n\n\n\n2. Reduce the size of the logical volume.\n\n\nlvreduce -L -15G /dev/vg01/lvs-02\n\n\n\n# Optional: you can also shrink down the volume group if desired.\n\n\nvgreduce vg01 /dev/sdxy\n\n\n\n\n\n\n\n\n\nOffline reduce/shrink root filesystem\n\n\n1. boot from rescue media\n\n\nif san connected append \"linux mpath\" to boot options\n\n\nDo not mount filesystems when booting from rescue media\n\n\nselect the menu option that skips the mounting of your filesyetsms\n\n\n2. Identify the Volume group\n\n\n/usr/sbin/lvm vgscan\n/usr/sbin/lvm vgdisplay\n\n\n\nnote: you have to run lvm {operand}... instead of the aliases lvscan\n\n\n3. Activate the the volume group\n\n\n/usr/sbin/lvm vgchange {VG-name} -a y\n\n\n\n4. Perform a check of the filesystem...\n\n\n/usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02\n\n\n\n5. Resize the filesystem\n\n\nresize2fs /dev/vg01/lvs-02 15G\n\n\n\n6. Reduce the size of the logical volume.\n\n\n/usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02\n\n\n\n# Optional: you can also shrink down the volume group if desired.\n\n\n/usr/sbin/lvm vgreduce vg01 /dev/sdxy\n\n\n\nExample: reducing the size of a swap volume\n\n\n[root@server ~]# swapoff -a\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 20G\n  Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda\n  New size given (640 extents) not less than existing size (625 extents)\n  Run `lvreduce --help' for more information.\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 10G\n  Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda\n  WARNING: Reducing active logical volume to 10.00 GB\n  THIS MAY DESTROY YOUR DATA (filesystem etc.)\nDo you really want to reduce swap? [y/n]: y\n  Reducing logical volume swap to 10.00 GB\n  Logical volume swap successfully resized\n\n[root@server ~]# mkswap /dev/vgsys/swap\nSetting up swapspace version 1, size = 10737414 kB\n\n[root@server ~]# swapon -a\n\n[root@server ~]# free\n             total       used       free     shared    buffers     cached\nMem:      65931888     527968   65403920          0      32036     343660\n-/+ buffers/cache:     152272   65779616\nSwap:     10485752          0   10485752", 
            "title": "Shrink lvm filesystem"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/shrink_lvm_filesystem/#shrinking-a-linux-filesystem-on-lvm", 
            "text": "", 
            "title": "Shrinking a Linux filesystem on LVM"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/shrink_lvm_filesystem/#online-reduceshrink", 
            "text": "1. Unmount filesystem  if you cant unmount the filesystem on the running system see the offline section below  umount /dev/vg01/lvs-02  2. Perform a check of the filesystem...  e2fsck -f /dev/vg01/lvs-02  3. Resize the filesystem  resize2fs /dev/vg01/lvs-02 15G  2. Reduce the size of the logical volume.  lvreduce -L -15G /dev/vg01/lvs-02  # Optional: you can also shrink down the volume group if desired.  vgreduce vg01 /dev/sdxy", 
            "title": "Online reduce/shrink"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/shrink_lvm_filesystem/#offline-reduceshrink-root-filesystem", 
            "text": "1. boot from rescue media  if san connected append \"linux mpath\" to boot options  Do not mount filesystems when booting from rescue media  select the menu option that skips the mounting of your filesyetsms  2. Identify the Volume group  /usr/sbin/lvm vgscan\n/usr/sbin/lvm vgdisplay  note: you have to run lvm {operand}... instead of the aliases lvscan  3. Activate the the volume group  /usr/sbin/lvm vgchange {VG-name} -a y  4. Perform a check of the filesystem...  /usr/sbin/fsck.ext2 -f /dev/vg01/lvs-02  5. Resize the filesystem  resize2fs /dev/vg01/lvs-02 15G  6. Reduce the size of the logical volume.  /usr/sbin/lvm lvreduce -L -15G /dev/vg01/lvs-02  # Optional: you can also shrink down the volume group if desired.  /usr/sbin/lvm vgreduce vg01 /dev/sdxy", 
            "title": "Offline reduce/shrink root filesystem"
        }, 
        {
            "location": "/linux/Disks_and_File_Systems/shrink_lvm_filesystem/#example-reducing-the-size-of-a-swap-volume", 
            "text": "[root@server ~]# swapoff -a\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 20G\n  Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda\n  New size given (640 extents) not less than existing size (625 extents)\n  Run `lvreduce --help' for more information.\n\n[root@server ~]# lvreduce /dev/vgsys/swap -L 10G\n  Found duplicate PV 8RD6mpGQfwBCWprZvaxabcdefg123: using /dev/sdq not /dev/sda\n  WARNING: Reducing active logical volume to 10.00 GB\n  THIS MAY DESTROY YOUR DATA (filesystem etc.)\nDo you really want to reduce swap? [y/n]: y\n  Reducing logical volume swap to 10.00 GB\n  Logical volume swap successfully resized\n\n[root@server ~]# mkswap /dev/vgsys/swap\nSetting up swapspace version 1, size = 10737414 kB\n\n[root@server ~]# swapon -a\n\n[root@server ~]# free\n             total       used       free     shared    buffers     cached\nMem:      65931888     527968   65403920          0      32036     343660\n-/+ buffers/cache:     152272   65779616\nSwap:     10485752          0   10485752", 
            "title": "Example: reducing the size of a swap volume"
        }
    ]
}